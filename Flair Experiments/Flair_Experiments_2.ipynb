{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flair Experiments 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNvDEB-4_Tjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c32eed52-ffd1-4920-e938-5c0f51510693"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Requirement already satisfied: transformers>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.3.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.7)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: ipython==7.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (7.6.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.9)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.7)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.40)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (0.0.35)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.17.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (42.0.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (4.3.0)\n",
            "Requirement already satisfied: natto-py; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.9.0)\n",
            "Requirement already satisfied: kytea; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.1.4)\n",
            "Requirement already satisfied: SudachiPy; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2019.11.28)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair) (0.46)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Requirement already satisfied: dartsclone~=0.6.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.6)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLLGNSYI51ZK",
        "colab_type": "text"
      },
      "source": [
        "# Flair Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZstJLVX55Kd",
        "colab_type": "text"
      },
      "source": [
        "## I. Ontonotes Named Entity Recognition (English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Vab3Aw9lRI",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sr88VQv9m3-",
        "colab_type": "text"
      },
      "source": [
        "The [Ontonotes corpus](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf) is one of the best resources for different types of NLP and contains rich NER annotation. Get the corpus and split it into train, test and dev splits using the scripts provided by the [CoNLL-12 shared task](http://conll.cemantix.org/2012/data.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wR5JET799Ed",
        "colab_type": "text"
      },
      "source": [
        "Place train, test and dev data in CoNLL-03 format in `resources/tasks/onto-ner/` as follows:\n",
        "\n",
        "`resources/tasks/onto-ner/eng.testa`\n",
        "\n",
        "`resources/tasks/onto-ner/eng.testb`\n",
        "\n",
        "`resources/tasks/onto-ner/eng.train`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vjg2Osl-XdA",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB6OLyHF-cjU",
        "colab_type": "text"
      },
      "source": [
        "Once you have the data, reproduce our experiments exactly like for CoNLL-03, just with a different dataset and with FastText embeddings (they work better on this dataset).\n",
        "\n",
        "You also need to provide a `column_format` for the `ColumnCorpus` object indicating which column in the training file is the 'ner' information. The full code then is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpuF7Bc8-QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "67c7ad94-831d-4f99-d20c-4473e4ca27fa"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, FlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axvwn-OQ_RQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = flair.datasets.ColumnCorpus('resources/tasks/onto-ner',\n",
        "                                             column_format = {0: 'text', 1: 'pos', 2: 'upos', 3: 'ner'},\n",
        "                                             tag_to_bioes = 'ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKXXsyQP_ycI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kShxJ11N_8Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                  WordEmbeddings('crawl'),\n",
        "                  FlairEmbeddings('news-forward'),\n",
        "                  FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYsod6K0AM51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLq2dQy6AbnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              learning_rate = 0.1,\n",
        "              train_with_dev = True,\n",
        "              # it's a big dataset so maybe set embeddings_storage_mode to 'none'\n",
        "              # (embeddings are not kept in memory\n",
        "              embeddings_storage_mode = 'none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP41wpM7A6Cp",
        "colab_type": "text"
      },
      "source": [
        "## II. Penn Treebank Part-of-Speech Tagging (English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5FM0Ol5w66S",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLhU1_x4w8Nb",
        "colab_type": "text"
      },
      "source": [
        "Get the [Penn treebank](https://catalog.ldc.upenn.edu/ldc99t42) and follow the guidelines in  [Collins (2002)](http://www.cs.columbia.edu/~mcollins/papers/tagperc.pdf) to produce train, dev and test splits. Convert splits into CoNLL-U format and place train, test and dev data in `/path/to/penn/` as follows:\n",
        "\n",
        "`/path/to/penn/test.conll`\n",
        "\n",
        "`/path/to/penn/train.conll`\n",
        "\n",
        "`/path/to/penn/valid.conll`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pajiWfzxgVK",
        "colab_type": "text"
      },
      "source": [
        "Then, run the experiments with extvec embeddings and contextual string embeddings. Also, select 'pos' as `tag_type`, so the algorithm knows that POS tags and not NER are to be predicted from this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK7rrNzLxuY_",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOPb2xGpxwsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import UniversalDependenciesCorpus\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, FlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssZWLtm1zXRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = UniversalDependenciesCorpus(base_path = '/path/to/penn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFS1v0Y8y4JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'pos'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kmYfTE7zn4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "            WordEmbeddings('extvec'),\n",
        "            FlairEmbeddings('news-forward'),\n",
        "            FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUkO2mHZz5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1cqPYqQ0Lhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-pos',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9QGxPTD0Z31",
        "colab_type": "text"
      },
      "source": [
        "## III. CoNLL-2000 Noun Phrase Chunking (English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YDJYrQs0gOT",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1eP6uo0iDd",
        "colab_type": "text"
      },
      "source": [
        "Data is included in Flair and will get automatically downloaded when you run the script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrB3rIoY0nOu",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3mXrQhp0pgs",
        "colab_type": "text"
      },
      "source": [
        "Run the code with extvec embeddings and our proposed contextual string embeddings. Use 'np' as `tag_type`, so the algorithm knows that chunking tags and not NER are to be predicted from this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgs0ivlS01jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_2000\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, FlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCdlPKf1DdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "47d0cf20-7890-4c60-ec2e-832dc6d832f0"
      },
      "source": [
        "# 1. get the corpus\n",
        "corpus: Corpus = CONLL_2000()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:16:45,892 https://www.clips.uantwerpen.be/conll2000/chunking/train.txt.gz not found in cache, downloading to /tmp/tmp7suuhq6o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 611540/611540 [00:00<00:00, 845891.13B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:16:47,204 copying /tmp/tmp7suuhq6o to cache at /root/.flair/datasets/conll_2000/train.txt.gz\n",
            "2019-12-23 08:16:47,206 removing temp file /tmp/tmp7suuhq6o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:16:47,796 https://www.clips.uantwerpen.be/conll2000/chunking/test.txt.gz not found in cache, downloading to /tmp/tmp04czw6ua\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 139551/139551 [00:00<00:00, 332076.10B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:16:48,792 copying /tmp/tmp04czw6ua to cache at /root/.flair/datasets/conll_2000/test.txt.gz\n",
            "2019-12-23 08:16:48,794 removing temp file /tmp/tmp04czw6ua\n",
            "2019-12-23 08:16:48,816 Reading data from /root/.flair/datasets/conll_2000\n",
            "2019-12-23 08:16:48,817 Train: /root/.flair/datasets/conll_2000/train.txt\n",
            "2019-12-23 08:16:48,818 Dev: None\n",
            "2019-12-23 08:16:48,818 Test: /root/.flair/datasets/conll_2000/test.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXoj1mJw1HJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'np'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43sWdyvy1SL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2a54c792-8301-4684-e511-e8084f5acb91"
      },
      "source": [
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "              WordEmbeddings('extvec'),\n",
        "              FlairEmbeddings('news-forward'),\n",
        "              FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:18:44,450 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/extvec.gensim.vectors.npy not found in cache, downloading to /tmp/tmp4v5goqyt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1771225328/1771225328 [01:49<00:00, 16135927.20B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:20:34,904 copying /tmp/tmp4v5goqyt to cache at /root/.flair/embeddings/extvec.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:20:48,301 removing temp file /tmp/tmp4v5goqyt\n",
            "2019-12-23 08:20:49,090 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/extvec.gensim not found in cache, downloading to /tmp/tmp44butzb2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 91665398/91665398 [00:06<00:00, 14581110.36B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:20:56,076 copying /tmp/tmp44butzb2 to cache at /root/.flair/embeddings/extvec.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:20:56,191 removing temp file /tmp/tmp44butzb2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:21:03,620 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpsuglll0g\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034624/73034624 [00:05<00:00, 12805031.48B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:21:10,006 copying /tmp/tmpsuglll0g to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:21:10,102 removing temp file /tmp/tmpsuglll0g\n",
            "2019-12-23 08:21:22,190 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmpqafqyoez\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034575/73034575 [00:05<00:00, 14136615.07B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:21:28,048 copying /tmp/tmpqafqyoez to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:21:28,190 removing temp file /tmp/tmpqafqyoez\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D4kQUQo1kXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_8A5WsA2aiV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fcc886a-5cf4-44c3-a1e4-264256da98b3"
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-chunk',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-23 08:23:18,621 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,624 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('extvec')\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4396, out_features=4396, bias=True)\n",
            "  (rnn): LSTM(4396, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=45, bias=True)\n",
            ")\"\n",
            "2019-12-23 08:23:18,626 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,627 Corpus: \"Corpus: 8042 train + 894 dev + 2012 test sentences\"\n",
            "2019-12-23 08:23:18,628 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,630 Parameters:\n",
            "2019-12-23 08:23:18,631  - learning_rate: \"0.1\"\n",
            "2019-12-23 08:23:18,632  - mini_batch_size: \"32\"\n",
            "2019-12-23 08:23:18,639  - patience: \"3\"\n",
            "2019-12-23 08:23:18,641  - anneal_factor: \"0.5\"\n",
            "2019-12-23 08:23:18,642  - max_epochs: \"150\"\n",
            "2019-12-23 08:23:18,643  - shuffle: \"True\"\n",
            "2019-12-23 08:23:18,645  - train_with_dev: \"True\"\n",
            "2019-12-23 08:23:18,646  - batch_growth_annealing: \"False\"\n",
            "2019-12-23 08:23:18,647 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,648 Model training base path: \"resources/taggers/example-chunk\"\n",
            "2019-12-23 08:23:18,650 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,651 Device: cuda:0\n",
            "2019-12-23 08:23:18,652 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:18,653 Embeddings storage mode: cpu\n",
            "2019-12-23 08:23:18,655 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:23:19,225 epoch 1 - iter 0/280 - loss 79.78240967 - samples/sec: 1589.63\n",
            "2019-12-23 08:23:30,423 epoch 1 - iter 28/280 - loss 45.89483386 - samples/sec: 80.16\n",
            "2019-12-23 08:23:41,143 epoch 1 - iter 56/280 - loss 33.35441645 - samples/sec: 83.74\n",
            "2019-12-23 08:23:53,305 epoch 1 - iter 84/280 - loss 26.84607890 - samples/sec: 73.79\n",
            "2019-12-23 08:24:04,238 epoch 1 - iter 112/280 - loss 22.79725260 - samples/sec: 82.15\n",
            "2019-12-23 08:24:15,795 epoch 1 - iter 140/280 - loss 20.11263486 - samples/sec: 77.66\n",
            "2019-12-23 08:24:27,005 epoch 1 - iter 168/280 - loss 18.03266062 - samples/sec: 80.08\n",
            "2019-12-23 08:24:38,177 epoch 1 - iter 196/280 - loss 16.47128095 - samples/sec: 80.36\n",
            "2019-12-23 08:24:49,372 epoch 1 - iter 224/280 - loss 15.24945868 - samples/sec: 80.19\n",
            "2019-12-23 08:25:00,625 epoch 1 - iter 252/280 - loss 14.22565041 - samples/sec: 79.76\n",
            "2019-12-23 08:25:11,470 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:25:11,471 EPOCH 1 done: loss 13.4057 - lr 0.1000\n",
            "2019-12-23 08:25:11,475 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:25:11,481 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:25:11,693 epoch 2 - iter 0/280 - loss 5.68288231 - samples/sec: 4249.86\n",
            "2019-12-23 08:25:17,843 epoch 2 - iter 28/280 - loss 5.37179372 - samples/sec: 146.13\n",
            "2019-12-23 08:25:23,710 epoch 2 - iter 56/280 - loss 5.27414099 - samples/sec: 153.30\n",
            "2019-12-23 08:25:29,706 epoch 2 - iter 84/280 - loss 5.18962033 - samples/sec: 149.99\n",
            "2019-12-23 08:25:35,818 epoch 2 - iter 112/280 - loss 5.14564164 - samples/sec: 147.18\n",
            "2019-12-23 08:25:41,821 epoch 2 - iter 140/280 - loss 5.01043271 - samples/sec: 149.76\n",
            "2019-12-23 08:25:47,841 epoch 2 - iter 168/280 - loss 4.89853637 - samples/sec: 149.31\n",
            "2019-12-23 08:25:53,787 epoch 2 - iter 196/280 - loss 4.83844841 - samples/sec: 151.20\n",
            "2019-12-23 08:25:59,873 epoch 2 - iter 224/280 - loss 4.77884934 - samples/sec: 147.73\n",
            "2019-12-23 08:26:06,203 epoch 2 - iter 252/280 - loss 4.75743835 - samples/sec: 142.04\n",
            "2019-12-23 08:26:11,933 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:26:11,934 EPOCH 2 done: loss 4.7114 - lr 0.1000\n",
            "2019-12-23 08:26:11,935 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:26:11,936 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:26:12,155 epoch 3 - iter 0/280 - loss 3.91513228 - samples/sec: 4159.88\n",
            "2019-12-23 08:26:18,206 epoch 3 - iter 28/280 - loss 3.80689077 - samples/sec: 148.51\n",
            "2019-12-23 08:26:24,289 epoch 3 - iter 56/280 - loss 3.95369577 - samples/sec: 147.83\n",
            "2019-12-23 08:26:30,399 epoch 3 - iter 84/280 - loss 3.90485792 - samples/sec: 147.14\n",
            "2019-12-23 08:26:36,574 epoch 3 - iter 112/280 - loss 3.87463760 - samples/sec: 145.57\n",
            "2019-12-23 08:26:42,674 epoch 3 - iter 140/280 - loss 3.81879903 - samples/sec: 147.38\n",
            "2019-12-23 08:26:48,725 epoch 3 - iter 168/280 - loss 3.75883979 - samples/sec: 148.59\n",
            "2019-12-23 08:26:56,086 epoch 3 - iter 196/280 - loss 3.71653679 - samples/sec: 122.07\n",
            "2019-12-23 08:27:02,478 epoch 3 - iter 224/280 - loss 3.68120597 - samples/sec: 140.62\n",
            "2019-12-23 08:27:08,628 epoch 3 - iter 252/280 - loss 3.65762025 - samples/sec: 146.26\n",
            "2019-12-23 08:27:14,340 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:27:14,342 EPOCH 3 done: loss 3.6306 - lr 0.1000\n",
            "2019-12-23 08:27:14,345 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:27:14,348 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:27:14,563 epoch 4 - iter 0/280 - loss 2.57730675 - samples/sec: 4225.63\n",
            "2019-12-23 08:27:20,483 epoch 4 - iter 28/280 - loss 3.27361600 - samples/sec: 151.81\n",
            "2019-12-23 08:27:26,504 epoch 4 - iter 56/280 - loss 3.38999172 - samples/sec: 149.36\n",
            "2019-12-23 08:27:32,557 epoch 4 - iter 84/280 - loss 3.26795433 - samples/sec: 148.53\n",
            "2019-12-23 08:27:38,582 epoch 4 - iter 112/280 - loss 3.24717139 - samples/sec: 149.26\n",
            "2019-12-23 08:27:44,606 epoch 4 - iter 140/280 - loss 3.24973028 - samples/sec: 149.25\n",
            "2019-12-23 08:27:50,459 epoch 4 - iter 168/280 - loss 3.22114213 - samples/sec: 153.56\n",
            "2019-12-23 08:27:56,851 epoch 4 - iter 196/280 - loss 3.20391212 - samples/sec: 140.57\n",
            "2019-12-23 08:28:02,609 epoch 4 - iter 224/280 - loss 3.21990057 - samples/sec: 156.18\n",
            "2019-12-23 08:28:08,767 epoch 4 - iter 252/280 - loss 3.19904180 - samples/sec: 146.01\n",
            "2019-12-23 08:28:14,389 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:28:14,390 EPOCH 4 done: loss 3.1623 - lr 0.1000\n",
            "2019-12-23 08:28:14,392 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:28:14,394 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:28:14,593 epoch 5 - iter 0/280 - loss 1.94482660 - samples/sec: 4566.47\n",
            "2019-12-23 08:28:20,615 epoch 5 - iter 28/280 - loss 2.78378978 - samples/sec: 149.29\n",
            "2019-12-23 08:28:26,661 epoch 5 - iter 56/280 - loss 2.82943480 - samples/sec: 148.70\n",
            "2019-12-23 08:28:32,747 epoch 5 - iter 84/280 - loss 2.83591279 - samples/sec: 147.69\n",
            "2019-12-23 08:28:38,947 epoch 5 - iter 112/280 - loss 2.82580796 - samples/sec: 145.06\n",
            "2019-12-23 08:28:45,044 epoch 5 - iter 140/280 - loss 2.81044940 - samples/sec: 147.49\n",
            "2019-12-23 08:28:51,288 epoch 5 - iter 168/280 - loss 2.78207534 - samples/sec: 143.94\n",
            "2019-12-23 08:28:57,296 epoch 5 - iter 196/280 - loss 2.77779816 - samples/sec: 149.60\n",
            "2019-12-23 08:29:03,067 epoch 5 - iter 224/280 - loss 2.77503077 - samples/sec: 155.78\n",
            "2019-12-23 08:29:09,260 epoch 5 - iter 252/280 - loss 2.77268375 - samples/sec: 145.20\n",
            "2019-12-23 08:29:14,936 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:29:14,937 EPOCH 5 done: loss 2.7519 - lr 0.1000\n",
            "2019-12-23 08:29:14,938 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:29:14,940 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:29:15,184 epoch 6 - iter 0/280 - loss 3.64802170 - samples/sec: 3699.98\n",
            "2019-12-23 08:29:21,302 epoch 6 - iter 28/280 - loss 2.44065785 - samples/sec: 146.89\n",
            "2019-12-23 08:29:27,440 epoch 6 - iter 56/280 - loss 2.50028782 - samples/sec: 146.44\n",
            "2019-12-23 08:29:33,461 epoch 6 - iter 84/280 - loss 2.49959078 - samples/sec: 149.36\n",
            "2019-12-23 08:29:39,463 epoch 6 - iter 112/280 - loss 2.50953879 - samples/sec: 149.82\n",
            "2019-12-23 08:29:45,495 epoch 6 - iter 140/280 - loss 2.52276723 - samples/sec: 149.08\n",
            "2019-12-23 08:29:51,610 epoch 6 - iter 168/280 - loss 2.50641602 - samples/sec: 147.02\n",
            "2019-12-23 08:29:57,418 epoch 6 - iter 196/280 - loss 2.49301208 - samples/sec: 154.81\n",
            "2019-12-23 08:30:03,346 epoch 6 - iter 224/280 - loss 2.46304965 - samples/sec: 151.68\n",
            "2019-12-23 08:30:09,518 epoch 6 - iter 252/280 - loss 2.47215498 - samples/sec: 145.60\n",
            "2019-12-23 08:30:15,409 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:30:15,410 EPOCH 6 done: loss 2.4761 - lr 0.1000\n",
            "2019-12-23 08:30:15,411 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:30:15,416 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:30:15,638 epoch 7 - iter 0/280 - loss 1.91221821 - samples/sec: 4123.41\n",
            "2019-12-23 08:30:22,058 epoch 7 - iter 28/280 - loss 2.43780503 - samples/sec: 139.99\n",
            "2019-12-23 08:30:27,853 epoch 7 - iter 56/280 - loss 2.34359899 - samples/sec: 155.13\n",
            "2019-12-23 08:30:33,890 epoch 7 - iter 84/280 - loss 2.33485263 - samples/sec: 148.90\n",
            "2019-12-23 08:30:40,167 epoch 7 - iter 112/280 - loss 2.34860920 - samples/sec: 143.19\n",
            "2019-12-23 08:30:46,467 epoch 7 - iter 140/280 - loss 2.31690883 - samples/sec: 142.72\n",
            "2019-12-23 08:30:52,476 epoch 7 - iter 168/280 - loss 2.35159357 - samples/sec: 149.60\n",
            "2019-12-23 08:30:58,652 epoch 7 - iter 196/280 - loss 2.31251573 - samples/sec: 145.52\n",
            "2019-12-23 08:31:04,621 epoch 7 - iter 224/280 - loss 2.32218970 - samples/sec: 150.72\n",
            "2019-12-23 08:31:10,586 epoch 7 - iter 252/280 - loss 2.32544193 - samples/sec: 150.70\n",
            "2019-12-23 08:31:16,431 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:31:16,432 EPOCH 7 done: loss 2.3182 - lr 0.1000\n",
            "2019-12-23 08:31:16,434 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:31:16,435 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:31:16,663 epoch 8 - iter 0/280 - loss 1.56961250 - samples/sec: 3985.54\n",
            "2019-12-23 08:31:23,025 epoch 8 - iter 28/280 - loss 2.21749218 - samples/sec: 141.26\n",
            "2019-12-23 08:31:29,222 epoch 8 - iter 56/280 - loss 2.26294616 - samples/sec: 145.10\n",
            "2019-12-23 08:31:35,195 epoch 8 - iter 84/280 - loss 2.22986127 - samples/sec: 150.53\n",
            "2019-12-23 08:31:41,498 epoch 8 - iter 112/280 - loss 2.21510453 - samples/sec: 142.62\n",
            "2019-12-23 08:31:47,395 epoch 8 - iter 140/280 - loss 2.18386533 - samples/sec: 152.49\n",
            "2019-12-23 08:31:53,151 epoch 8 - iter 168/280 - loss 2.14068801 - samples/sec: 156.18\n",
            "2019-12-23 08:31:59,162 epoch 8 - iter 196/280 - loss 2.15566919 - samples/sec: 149.58\n",
            "2019-12-23 08:32:05,480 epoch 8 - iter 224/280 - loss 2.16948627 - samples/sec: 142.27\n",
            "2019-12-23 08:32:11,499 epoch 8 - iter 252/280 - loss 2.17665981 - samples/sec: 149.33\n",
            "2019-12-23 08:32:17,248 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:32:17,251 EPOCH 8 done: loss 2.1667 - lr 0.1000\n",
            "2019-12-23 08:32:17,252 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:32:17,255 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:32:17,479 epoch 9 - iter 0/280 - loss 2.11873341 - samples/sec: 4101.94\n",
            "2019-12-23 08:32:23,626 epoch 9 - iter 28/280 - loss 1.94828805 - samples/sec: 146.24\n",
            "2019-12-23 08:32:29,566 epoch 9 - iter 56/280 - loss 1.97748088 - samples/sec: 151.34\n",
            "2019-12-23 08:32:35,627 epoch 9 - iter 84/280 - loss 2.00367789 - samples/sec: 148.34\n",
            "2019-12-23 08:32:41,863 epoch 9 - iter 112/280 - loss 2.01512499 - samples/sec: 144.17\n",
            "2019-12-23 08:32:47,934 epoch 9 - iter 140/280 - loss 2.02857776 - samples/sec: 148.09\n",
            "2019-12-23 08:32:54,072 epoch 9 - iter 168/280 - loss 2.01988300 - samples/sec: 146.49\n",
            "2019-12-23 08:33:00,127 epoch 9 - iter 196/280 - loss 2.02008145 - samples/sec: 148.46\n",
            "2019-12-23 08:33:06,295 epoch 9 - iter 224/280 - loss 2.01328892 - samples/sec: 145.80\n",
            "2019-12-23 08:33:12,525 epoch 9 - iter 252/280 - loss 2.02064435 - samples/sec: 144.25\n",
            "2019-12-23 08:33:18,327 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:33:18,328 EPOCH 9 done: loss 2.0188 - lr 0.1000\n",
            "2019-12-23 08:33:18,330 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:33:18,333 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:33:18,519 epoch 10 - iter 0/280 - loss 2.53241992 - samples/sec: 4939.28\n",
            "2019-12-23 08:33:24,468 epoch 10 - iter 28/280 - loss 2.03574890 - samples/sec: 151.10\n",
            "2019-12-23 08:33:30,638 epoch 10 - iter 56/280 - loss 1.90279705 - samples/sec: 145.78\n",
            "2019-12-23 08:33:36,826 epoch 10 - iter 84/280 - loss 1.92595983 - samples/sec: 145.29\n",
            "2019-12-23 08:33:42,860 epoch 10 - iter 112/280 - loss 1.96174080 - samples/sec: 148.97\n",
            "2019-12-23 08:33:48,989 epoch 10 - iter 140/280 - loss 1.95918632 - samples/sec: 146.71\n",
            "2019-12-23 08:33:55,235 epoch 10 - iter 168/280 - loss 1.95385794 - samples/sec: 143.92\n",
            "2019-12-23 08:34:01,302 epoch 10 - iter 196/280 - loss 1.94119581 - samples/sec: 148.20\n",
            "2019-12-23 08:34:07,029 epoch 10 - iter 224/280 - loss 1.92873642 - samples/sec: 157.03\n",
            "2019-12-23 08:34:13,255 epoch 10 - iter 252/280 - loss 1.93241243 - samples/sec: 144.38\n",
            "2019-12-23 08:34:19,060 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:34:19,062 EPOCH 10 done: loss 1.9232 - lr 0.1000\n",
            "2019-12-23 08:34:19,063 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:34:19,064 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:34:19,252 epoch 11 - iter 0/280 - loss 2.34169769 - samples/sec: 4802.66\n",
            "2019-12-23 08:34:25,478 epoch 11 - iter 28/280 - loss 1.86579964 - samples/sec: 144.33\n",
            "2019-12-23 08:34:31,791 epoch 11 - iter 56/280 - loss 1.84585626 - samples/sec: 142.44\n",
            "2019-12-23 08:34:37,722 epoch 11 - iter 84/280 - loss 1.86222546 - samples/sec: 151.54\n",
            "2019-12-23 08:34:43,649 epoch 11 - iter 112/280 - loss 1.84556298 - samples/sec: 151.66\n",
            "2019-12-23 08:34:49,859 epoch 11 - iter 140/280 - loss 1.82345233 - samples/sec: 144.85\n",
            "2019-12-23 08:34:56,116 epoch 11 - iter 168/280 - loss 1.82927777 - samples/sec: 143.70\n",
            "2019-12-23 08:35:02,288 epoch 11 - iter 196/280 - loss 1.83996637 - samples/sec: 145.65\n",
            "2019-12-23 08:35:08,381 epoch 11 - iter 224/280 - loss 1.83852701 - samples/sec: 147.55\n",
            "2019-12-23 08:35:14,580 epoch 11 - iter 252/280 - loss 1.83128081 - samples/sec: 145.03\n",
            "2019-12-23 08:35:20,136 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:35:20,138 EPOCH 11 done: loss 1.8286 - lr 0.1000\n",
            "2019-12-23 08:35:20,140 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:35:20,142 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:35:20,349 epoch 12 - iter 0/280 - loss 2.12598968 - samples/sec: 4402.27\n",
            "2019-12-23 08:35:26,420 epoch 12 - iter 28/280 - loss 1.75332634 - samples/sec: 148.08\n",
            "2019-12-23 08:35:32,727 epoch 12 - iter 56/280 - loss 1.68771643 - samples/sec: 142.55\n",
            "2019-12-23 08:35:38,948 epoch 12 - iter 84/280 - loss 1.71877899 - samples/sec: 144.58\n",
            "2019-12-23 08:35:45,031 epoch 12 - iter 112/280 - loss 1.70193370 - samples/sec: 147.81\n",
            "2019-12-23 08:35:51,225 epoch 12 - iter 140/280 - loss 1.70204312 - samples/sec: 145.10\n",
            "2019-12-23 08:35:57,246 epoch 12 - iter 168/280 - loss 1.72289647 - samples/sec: 149.32\n",
            "2019-12-23 08:36:03,055 epoch 12 - iter 196/280 - loss 1.71808789 - samples/sec: 154.84\n",
            "2019-12-23 08:36:08,971 epoch 12 - iter 224/280 - loss 1.71279155 - samples/sec: 152.05\n",
            "2019-12-23 08:36:15,171 epoch 12 - iter 252/280 - loss 1.71613760 - samples/sec: 144.97\n",
            "2019-12-23 08:36:21,097 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:36:21,098 EPOCH 12 done: loss 1.7335 - lr 0.1000\n",
            "2019-12-23 08:36:21,102 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:36:21,103 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:36:21,293 epoch 13 - iter 0/280 - loss 1.01322854 - samples/sec: 4771.06\n",
            "2019-12-23 08:36:27,348 epoch 13 - iter 28/280 - loss 1.49305985 - samples/sec: 148.44\n",
            "2019-12-23 08:36:33,541 epoch 13 - iter 56/280 - loss 1.53795587 - samples/sec: 145.22\n",
            "2019-12-23 08:36:39,757 epoch 13 - iter 84/280 - loss 1.61437176 - samples/sec: 144.64\n",
            "2019-12-23 08:36:45,964 epoch 13 - iter 112/280 - loss 1.61253373 - samples/sec: 144.82\n",
            "2019-12-23 08:36:52,102 epoch 13 - iter 140/280 - loss 1.64763724 - samples/sec: 146.49\n",
            "2019-12-23 08:36:58,164 epoch 13 - iter 168/280 - loss 1.65697299 - samples/sec: 148.33\n",
            "2019-12-23 08:37:04,460 epoch 13 - iter 196/280 - loss 1.65553676 - samples/sec: 142.86\n",
            "2019-12-23 08:37:10,520 epoch 13 - iter 224/280 - loss 1.63550154 - samples/sec: 148.37\n",
            "2019-12-23 08:37:16,753 epoch 13 - iter 252/280 - loss 1.62986286 - samples/sec: 144.19\n",
            "2019-12-23 08:37:22,560 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:37:22,561 EPOCH 13 done: loss 1.6514 - lr 0.1000\n",
            "2019-12-23 08:37:22,564 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:37:22,565 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:37:22,785 epoch 14 - iter 0/280 - loss 1.66060400 - samples/sec: 4147.25\n",
            "2019-12-23 08:37:28,825 epoch 14 - iter 28/280 - loss 1.60359570 - samples/sec: 148.79\n",
            "2019-12-23 08:37:35,079 epoch 14 - iter 56/280 - loss 1.57837969 - samples/sec: 143.78\n",
            "2019-12-23 08:37:41,188 epoch 14 - iter 84/280 - loss 1.54274240 - samples/sec: 147.15\n",
            "2019-12-23 08:37:47,274 epoch 14 - iter 112/280 - loss 1.56377249 - samples/sec: 147.71\n",
            "2019-12-23 08:37:53,327 epoch 14 - iter 140/280 - loss 1.56493661 - samples/sec: 148.61\n",
            "2019-12-23 08:37:59,392 epoch 14 - iter 168/280 - loss 1.57855369 - samples/sec: 148.26\n",
            "2019-12-23 08:38:06,932 epoch 14 - iter 196/280 - loss 1.58867664 - samples/sec: 119.16\n",
            "2019-12-23 08:38:12,839 epoch 14 - iter 224/280 - loss 1.58776254 - samples/sec: 152.22\n",
            "2019-12-23 08:38:18,946 epoch 14 - iter 252/280 - loss 1.59527805 - samples/sec: 147.16\n",
            "2019-12-23 08:38:24,994 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:38:24,995 EPOCH 14 done: loss 1.5943 - lr 0.1000\n",
            "2019-12-23 08:38:24,997 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:38:24,998 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:38:25,206 epoch 15 - iter 0/280 - loss 0.85855806 - samples/sec: 4353.20\n",
            "2019-12-23 08:38:31,293 epoch 15 - iter 28/280 - loss 1.46783109 - samples/sec: 147.64\n",
            "2019-12-23 08:38:37,495 epoch 15 - iter 56/280 - loss 1.40409724 - samples/sec: 144.93\n",
            "2019-12-23 08:38:43,805 epoch 15 - iter 84/280 - loss 1.41971437 - samples/sec: 142.50\n",
            "2019-12-23 08:38:50,047 epoch 15 - iter 112/280 - loss 1.45946653 - samples/sec: 144.09\n",
            "2019-12-23 08:38:56,013 epoch 15 - iter 140/280 - loss 1.45362561 - samples/sec: 150.75\n",
            "2019-12-23 08:39:02,125 epoch 15 - iter 168/280 - loss 1.47753480 - samples/sec: 147.05\n",
            "2019-12-23 08:39:08,299 epoch 15 - iter 196/280 - loss 1.48937599 - samples/sec: 145.61\n",
            "2019-12-23 08:39:14,108 epoch 15 - iter 224/280 - loss 1.50115641 - samples/sec: 154.78\n",
            "2019-12-23 08:39:20,295 epoch 15 - iter 252/280 - loss 1.50577225 - samples/sec: 145.29\n",
            "2019-12-23 08:39:26,072 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:39:26,073 EPOCH 15 done: loss 1.5028 - lr 0.1000\n",
            "2019-12-23 08:39:26,074 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:39:26,079 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:39:26,277 epoch 16 - iter 0/280 - loss 0.97359765 - samples/sec: 4610.47\n",
            "2019-12-23 08:39:32,192 epoch 16 - iter 28/280 - loss 1.31515068 - samples/sec: 151.93\n",
            "2019-12-23 08:39:38,496 epoch 16 - iter 56/280 - loss 1.42196560 - samples/sec: 142.56\n",
            "2019-12-23 08:39:44,722 epoch 16 - iter 84/280 - loss 1.38928286 - samples/sec: 144.38\n",
            "2019-12-23 08:39:50,874 epoch 16 - iter 112/280 - loss 1.42509770 - samples/sec: 146.15\n",
            "2019-12-23 08:39:57,110 epoch 16 - iter 140/280 - loss 1.45283583 - samples/sec: 144.22\n",
            "2019-12-23 08:40:03,402 epoch 16 - iter 168/280 - loss 1.46165221 - samples/sec: 142.89\n",
            "2019-12-23 08:40:09,572 epoch 16 - iter 196/280 - loss 1.46662795 - samples/sec: 145.70\n",
            "2019-12-23 08:40:15,476 epoch 16 - iter 224/280 - loss 1.45727743 - samples/sec: 152.30\n",
            "2019-12-23 08:40:21,557 epoch 16 - iter 252/280 - loss 1.45851907 - samples/sec: 147.81\n",
            "2019-12-23 08:40:27,241 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:40:27,242 EPOCH 16 done: loss 1.4483 - lr 0.1000\n",
            "2019-12-23 08:40:27,247 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:40:27,250 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:40:27,464 epoch 17 - iter 0/280 - loss 1.40757060 - samples/sec: 4283.58\n",
            "2019-12-23 08:40:33,751 epoch 17 - iter 28/280 - loss 1.28742398 - samples/sec: 142.94\n",
            "2019-12-23 08:40:39,813 epoch 17 - iter 56/280 - loss 1.33266165 - samples/sec: 148.27\n",
            "2019-12-23 08:40:46,098 epoch 17 - iter 84/280 - loss 1.32718136 - samples/sec: 143.04\n",
            "2019-12-23 08:40:52,099 epoch 17 - iter 112/280 - loss 1.35887529 - samples/sec: 149.79\n",
            "2019-12-23 08:40:58,269 epoch 17 - iter 140/280 - loss 1.38399693 - samples/sec: 145.72\n",
            "2019-12-23 08:41:04,216 epoch 17 - iter 168/280 - loss 1.39960116 - samples/sec: 151.16\n",
            "2019-12-23 08:41:10,444 epoch 17 - iter 196/280 - loss 1.40374766 - samples/sec: 144.33\n",
            "2019-12-23 08:41:16,647 epoch 17 - iter 224/280 - loss 1.38536705 - samples/sec: 144.91\n",
            "2019-12-23 08:41:22,895 epoch 17 - iter 252/280 - loss 1.40823996 - samples/sec: 143.86\n",
            "2019-12-23 08:41:28,843 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:41:28,844 EPOCH 17 done: loss 1.4157 - lr 0.1000\n",
            "2019-12-23 08:41:28,846 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:41:28,847 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:41:29,080 epoch 18 - iter 0/280 - loss 2.19179201 - samples/sec: 3889.58\n",
            "2019-12-23 08:41:35,130 epoch 18 - iter 28/280 - loss 1.26718106 - samples/sec: 148.53\n",
            "2019-12-23 08:41:41,443 epoch 18 - iter 56/280 - loss 1.30624043 - samples/sec: 142.44\n",
            "2019-12-23 08:41:47,903 epoch 18 - iter 84/280 - loss 1.35742767 - samples/sec: 139.19\n",
            "2019-12-23 08:41:53,994 epoch 18 - iter 112/280 - loss 1.36775947 - samples/sec: 147.60\n",
            "2019-12-23 08:42:00,076 epoch 18 - iter 140/280 - loss 1.36867106 - samples/sec: 147.83\n",
            "2019-12-23 08:42:06,311 epoch 18 - iter 168/280 - loss 1.38317336 - samples/sec: 144.20\n",
            "2019-12-23 08:42:12,512 epoch 18 - iter 196/280 - loss 1.40439496 - samples/sec: 144.97\n",
            "2019-12-23 08:42:18,681 epoch 18 - iter 224/280 - loss 1.40950549 - samples/sec: 145.76\n",
            "2019-12-23 08:42:24,735 epoch 18 - iter 252/280 - loss 1.40719219 - samples/sec: 148.50\n",
            "2019-12-23 08:42:30,378 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:42:30,379 EPOCH 18 done: loss 1.4047 - lr 0.1000\n",
            "2019-12-23 08:42:30,382 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:42:30,383 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:42:30,611 epoch 19 - iter 0/280 - loss 1.00560021 - samples/sec: 3981.95\n",
            "2019-12-23 08:42:36,968 epoch 19 - iter 28/280 - loss 1.32320170 - samples/sec: 141.37\n",
            "2019-12-23 08:42:43,011 epoch 19 - iter 56/280 - loss 1.31224827 - samples/sec: 148.91\n",
            "2019-12-23 08:42:49,091 epoch 19 - iter 84/280 - loss 1.27175849 - samples/sec: 147.85\n",
            "2019-12-23 08:42:55,271 epoch 19 - iter 112/280 - loss 1.29967264 - samples/sec: 145.46\n",
            "2019-12-23 08:43:01,377 epoch 19 - iter 140/280 - loss 1.27583429 - samples/sec: 147.24\n",
            "2019-12-23 08:43:07,262 epoch 19 - iter 168/280 - loss 1.29535452 - samples/sec: 152.81\n",
            "2019-12-23 08:43:13,446 epoch 19 - iter 196/280 - loss 1.30781143 - samples/sec: 145.47\n",
            "2019-12-23 08:43:19,629 epoch 19 - iter 224/280 - loss 1.31749908 - samples/sec: 145.34\n",
            "2019-12-23 08:43:25,615 epoch 19 - iter 252/280 - loss 1.30489746 - samples/sec: 150.21\n",
            "2019-12-23 08:43:31,418 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:43:31,419 EPOCH 19 done: loss 1.3069 - lr 0.1000\n",
            "2019-12-23 08:43:31,420 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:43:31,421 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:43:31,638 epoch 20 - iter 0/280 - loss 1.47186136 - samples/sec: 4201.66\n",
            "2019-12-23 08:43:37,741 epoch 20 - iter 28/280 - loss 1.13839927 - samples/sec: 147.26\n",
            "2019-12-23 08:43:43,952 epoch 20 - iter 56/280 - loss 1.24423811 - samples/sec: 144.74\n",
            "2019-12-23 08:43:50,056 epoch 20 - iter 84/280 - loss 1.22772867 - samples/sec: 147.33\n",
            "2019-12-23 08:43:56,249 epoch 20 - iter 112/280 - loss 1.24641346 - samples/sec: 145.15\n",
            "2019-12-23 08:44:02,345 epoch 20 - iter 140/280 - loss 1.25021983 - samples/sec: 147.47\n",
            "2019-12-23 08:44:08,617 epoch 20 - iter 168/280 - loss 1.25131851 - samples/sec: 143.41\n",
            "2019-12-23 08:44:14,649 epoch 20 - iter 196/280 - loss 1.27480128 - samples/sec: 149.08\n",
            "2019-12-23 08:44:20,559 epoch 20 - iter 224/280 - loss 1.28502476 - samples/sec: 152.18\n",
            "2019-12-23 08:44:26,524 epoch 20 - iter 252/280 - loss 1.27560633 - samples/sec: 150.85\n",
            "2019-12-23 08:44:32,451 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:44:32,455 EPOCH 20 done: loss 1.2773 - lr 0.1000\n",
            "2019-12-23 08:44:32,456 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:44:32,458 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:44:32,701 epoch 21 - iter 0/280 - loss 1.21117544 - samples/sec: 3707.01\n",
            "2019-12-23 08:44:38,814 epoch 21 - iter 28/280 - loss 1.15876792 - samples/sec: 147.04\n",
            "2019-12-23 08:44:44,722 epoch 21 - iter 56/280 - loss 1.13619200 - samples/sec: 152.21\n",
            "2019-12-23 08:44:50,827 epoch 21 - iter 84/280 - loss 1.15522507 - samples/sec: 147.29\n",
            "2019-12-23 08:44:56,856 epoch 21 - iter 112/280 - loss 1.21080232 - samples/sec: 149.14\n",
            "2019-12-23 08:45:02,976 epoch 21 - iter 140/280 - loss 1.21325732 - samples/sec: 146.90\n",
            "2019-12-23 08:45:08,988 epoch 21 - iter 168/280 - loss 1.22834483 - samples/sec: 149.54\n",
            "2019-12-23 08:45:15,383 epoch 21 - iter 196/280 - loss 1.22612555 - samples/sec: 140.56\n",
            "2019-12-23 08:45:21,648 epoch 21 - iter 224/280 - loss 1.24334049 - samples/sec: 143.47\n",
            "2019-12-23 08:45:27,758 epoch 21 - iter 252/280 - loss 1.24192864 - samples/sec: 147.14\n",
            "2019-12-23 08:45:33,700 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:45:33,701 EPOCH 21 done: loss 1.2497 - lr 0.1000\n",
            "2019-12-23 08:45:33,702 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:45:33,704 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:45:33,906 epoch 22 - iter 0/280 - loss 1.01997101 - samples/sec: 4468.82\n",
            "2019-12-23 08:45:40,138 epoch 22 - iter 28/280 - loss 1.08392801 - samples/sec: 144.21\n",
            "2019-12-23 08:45:46,107 epoch 22 - iter 56/280 - loss 1.13138694 - samples/sec: 150.62\n",
            "2019-12-23 08:45:52,042 epoch 22 - iter 84/280 - loss 1.11894890 - samples/sec: 151.44\n",
            "2019-12-23 08:45:58,265 epoch 22 - iter 112/280 - loss 1.14400366 - samples/sec: 144.49\n",
            "2019-12-23 08:46:04,309 epoch 22 - iter 140/280 - loss 1.16733246 - samples/sec: 148.74\n",
            "2019-12-23 08:46:10,373 epoch 22 - iter 168/280 - loss 1.17948527 - samples/sec: 148.28\n",
            "2019-12-23 08:46:16,490 epoch 22 - iter 196/280 - loss 1.17943997 - samples/sec: 146.95\n",
            "2019-12-23 08:46:22,505 epoch 22 - iter 224/280 - loss 1.20126252 - samples/sec: 149.44\n",
            "2019-12-23 08:46:28,657 epoch 22 - iter 252/280 - loss 1.20480494 - samples/sec: 146.16\n",
            "2019-12-23 08:46:34,666 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:46:34,667 EPOCH 22 done: loss 1.2044 - lr 0.1000\n",
            "2019-12-23 08:46:34,671 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:46:34,674 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:46:34,904 epoch 23 - iter 0/280 - loss 0.98118198 - samples/sec: 4003.04\n",
            "2019-12-23 08:46:41,134 epoch 23 - iter 28/280 - loss 1.02246623 - samples/sec: 144.29\n",
            "2019-12-23 08:46:47,247 epoch 23 - iter 56/280 - loss 1.11186783 - samples/sec: 147.14\n",
            "2019-12-23 08:46:53,507 epoch 23 - iter 84/280 - loss 1.12347321 - samples/sec: 143.61\n",
            "2019-12-23 08:46:59,910 epoch 23 - iter 112/280 - loss 1.12735651 - samples/sec: 140.38\n",
            "2019-12-23 08:47:06,322 epoch 23 - iter 140/280 - loss 1.17233220 - samples/sec: 140.21\n",
            "2019-12-23 08:47:12,323 epoch 23 - iter 168/280 - loss 1.17811661 - samples/sec: 149.83\n",
            "2019-12-23 08:47:18,554 epoch 23 - iter 196/280 - loss 1.18029560 - samples/sec: 144.31\n",
            "2019-12-23 08:47:24,726 epoch 23 - iter 224/280 - loss 1.16903765 - samples/sec: 145.70\n",
            "2019-12-23 08:47:30,734 epoch 23 - iter 252/280 - loss 1.17394519 - samples/sec: 149.60\n",
            "2019-12-23 08:47:36,553 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:47:36,554 EPOCH 23 done: loss 1.1625 - lr 0.1000\n",
            "2019-12-23 08:47:36,559 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:47:36,560 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:47:36,784 epoch 24 - iter 0/280 - loss 1.46412492 - samples/sec: 4023.49\n",
            "2019-12-23 08:47:43,127 epoch 24 - iter 28/280 - loss 1.03587307 - samples/sec: 141.73\n",
            "2019-12-23 08:47:49,357 epoch 24 - iter 56/280 - loss 1.05281239 - samples/sec: 144.26\n",
            "2019-12-23 08:47:55,654 epoch 24 - iter 84/280 - loss 1.08619061 - samples/sec: 142.82\n",
            "2019-12-23 08:48:01,795 epoch 24 - iter 112/280 - loss 1.10873311 - samples/sec: 146.36\n",
            "2019-12-23 08:48:08,077 epoch 24 - iter 140/280 - loss 1.14219243 - samples/sec: 143.09\n",
            "2019-12-23 08:48:14,078 epoch 24 - iter 168/280 - loss 1.14743223 - samples/sec: 149.81\n",
            "2019-12-23 08:48:20,211 epoch 24 - iter 196/280 - loss 1.15256859 - samples/sec: 146.61\n",
            "2019-12-23 08:48:26,250 epoch 24 - iter 224/280 - loss 1.16333143 - samples/sec: 148.91\n",
            "2019-12-23 08:48:32,131 epoch 24 - iter 252/280 - loss 1.16181811 - samples/sec: 152.84\n",
            "2019-12-23 08:48:38,123 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:48:38,124 EPOCH 24 done: loss 1.1639 - lr 0.1000\n",
            "2019-12-23 08:48:38,129 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 08:48:38,131 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:48:38,325 epoch 25 - iter 0/280 - loss 1.14216530 - samples/sec: 4716.62\n",
            "2019-12-23 08:48:44,522 epoch 25 - iter 28/280 - loss 1.00634825 - samples/sec: 145.02\n",
            "2019-12-23 08:48:50,719 epoch 25 - iter 56/280 - loss 1.03859891 - samples/sec: 145.08\n",
            "2019-12-23 08:48:56,827 epoch 25 - iter 84/280 - loss 1.07260227 - samples/sec: 147.21\n",
            "2019-12-23 08:49:02,852 epoch 25 - iter 112/280 - loss 1.07438994 - samples/sec: 149.22\n",
            "2019-12-23 08:49:08,882 epoch 25 - iter 140/280 - loss 1.07403959 - samples/sec: 149.09\n",
            "2019-12-23 08:49:16,414 epoch 25 - iter 168/280 - loss 1.07374156 - samples/sec: 119.28\n",
            "2019-12-23 08:49:22,485 epoch 25 - iter 196/280 - loss 1.07836217 - samples/sec: 148.09\n",
            "2019-12-23 08:49:28,715 epoch 25 - iter 224/280 - loss 1.08461075 - samples/sec: 144.36\n",
            "2019-12-23 08:49:34,865 epoch 25 - iter 252/280 - loss 1.09862710 - samples/sec: 146.20\n",
            "2019-12-23 08:49:41,003 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:49:41,005 EPOCH 25 done: loss 1.1043 - lr 0.1000\n",
            "2019-12-23 08:49:41,007 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:49:41,009 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:49:41,249 epoch 26 - iter 0/280 - loss 0.64777637 - samples/sec: 3791.73\n",
            "2019-12-23 08:49:47,405 epoch 26 - iter 28/280 - loss 0.99916601 - samples/sec: 146.17\n",
            "2019-12-23 08:49:53,592 epoch 26 - iter 56/280 - loss 1.00992456 - samples/sec: 145.32\n",
            "2019-12-23 08:49:59,621 epoch 26 - iter 84/280 - loss 1.03101855 - samples/sec: 149.11\n",
            "2019-12-23 08:50:05,834 epoch 26 - iter 112/280 - loss 1.04231473 - samples/sec: 144.88\n",
            "2019-12-23 08:50:12,030 epoch 26 - iter 140/280 - loss 1.07534303 - samples/sec: 145.12\n",
            "2019-12-23 08:50:18,280 epoch 26 - iter 168/280 - loss 1.08327134 - samples/sec: 143.87\n",
            "2019-12-23 08:50:24,479 epoch 26 - iter 196/280 - loss 1.10337235 - samples/sec: 145.05\n",
            "2019-12-23 08:50:30,642 epoch 26 - iter 224/280 - loss 1.09335075 - samples/sec: 145.85\n",
            "2019-12-23 08:50:36,980 epoch 26 - iter 252/280 - loss 1.09566863 - samples/sec: 141.81\n",
            "2019-12-23 08:50:42,645 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:50:42,647 EPOCH 26 done: loss 1.0908 - lr 0.1000\n",
            "2019-12-23 08:50:42,652 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:50:42,654 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:50:42,919 epoch 27 - iter 0/280 - loss 0.64916396 - samples/sec: 3443.38\n",
            "2019-12-23 08:50:49,187 epoch 27 - iter 28/280 - loss 1.04610914 - samples/sec: 143.36\n",
            "2019-12-23 08:50:55,366 epoch 27 - iter 56/280 - loss 1.09589539 - samples/sec: 145.48\n",
            "2019-12-23 08:51:01,565 epoch 27 - iter 84/280 - loss 1.10137568 - samples/sec: 145.04\n",
            "2019-12-23 08:51:07,679 epoch 27 - iter 112/280 - loss 1.07481747 - samples/sec: 146.98\n",
            "2019-12-23 08:51:13,828 epoch 27 - iter 140/280 - loss 1.06131492 - samples/sec: 146.20\n",
            "2019-12-23 08:51:20,185 epoch 27 - iter 168/280 - loss 1.06979472 - samples/sec: 141.45\n",
            "2019-12-23 08:51:26,037 epoch 27 - iter 196/280 - loss 1.07425115 - samples/sec: 153.66\n",
            "2019-12-23 08:51:32,227 epoch 27 - iter 224/280 - loss 1.06950667 - samples/sec: 145.26\n",
            "2019-12-23 08:51:38,288 epoch 27 - iter 252/280 - loss 1.07518705 - samples/sec: 148.32\n",
            "2019-12-23 08:51:44,141 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:51:44,142 EPOCH 27 done: loss 1.0788 - lr 0.1000\n",
            "2019-12-23 08:51:44,145 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:51:44,146 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:51:44,382 epoch 28 - iter 0/280 - loss 1.02260709 - samples/sec: 3829.96\n",
            "2019-12-23 08:51:50,728 epoch 28 - iter 28/280 - loss 0.99000508 - samples/sec: 141.68\n",
            "2019-12-23 08:51:56,989 epoch 28 - iter 56/280 - loss 0.98708702 - samples/sec: 143.58\n",
            "2019-12-23 08:52:03,012 epoch 28 - iter 84/280 - loss 0.99501665 - samples/sec: 149.26\n",
            "2019-12-23 08:52:09,220 epoch 28 - iter 112/280 - loss 1.00553720 - samples/sec: 144.84\n",
            "2019-12-23 08:52:15,232 epoch 28 - iter 140/280 - loss 0.99942181 - samples/sec: 149.57\n",
            "2019-12-23 08:52:21,546 epoch 28 - iter 168/280 - loss 1.00390181 - samples/sec: 142.42\n",
            "2019-12-23 08:52:27,634 epoch 28 - iter 196/280 - loss 1.00813775 - samples/sec: 148.07\n",
            "2019-12-23 08:52:33,659 epoch 28 - iter 224/280 - loss 1.01622279 - samples/sec: 149.21\n",
            "2019-12-23 08:52:40,246 epoch 28 - iter 252/280 - loss 1.01997029 - samples/sec: 136.69\n",
            "2019-12-23 08:52:45,969 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:52:45,970 EPOCH 28 done: loss 1.0201 - lr 0.1000\n",
            "2019-12-23 08:52:45,972 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:52:45,973 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:52:46,180 epoch 29 - iter 0/280 - loss 0.78744477 - samples/sec: 4381.32\n",
            "2019-12-23 08:52:52,356 epoch 29 - iter 28/280 - loss 0.96191176 - samples/sec: 145.48\n",
            "2019-12-23 08:52:58,686 epoch 29 - iter 56/280 - loss 1.02527609 - samples/sec: 142.05\n",
            "2019-12-23 08:53:04,836 epoch 29 - iter 84/280 - loss 1.03746687 - samples/sec: 146.20\n",
            "2019-12-23 08:53:11,021 epoch 29 - iter 112/280 - loss 1.01798689 - samples/sec: 145.44\n",
            "2019-12-23 08:53:17,296 epoch 29 - iter 140/280 - loss 1.02182572 - samples/sec: 143.23\n",
            "2019-12-23 08:53:23,523 epoch 29 - iter 168/280 - loss 1.01760298 - samples/sec: 144.35\n",
            "2019-12-23 08:53:29,747 epoch 29 - iter 196/280 - loss 1.01264007 - samples/sec: 144.44\n",
            "2019-12-23 08:53:36,043 epoch 29 - iter 224/280 - loss 1.02103649 - samples/sec: 142.75\n",
            "2019-12-23 08:53:42,139 epoch 29 - iter 252/280 - loss 1.01334539 - samples/sec: 147.52\n",
            "2019-12-23 08:53:47,999 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:53:48,001 EPOCH 29 done: loss 1.0121 - lr 0.1000\n",
            "2019-12-23 08:53:48,004 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:53:48,005 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:53:48,253 epoch 30 - iter 0/280 - loss 1.20650434 - samples/sec: 3709.48\n",
            "2019-12-23 08:53:54,429 epoch 30 - iter 28/280 - loss 0.94791323 - samples/sec: 145.53\n",
            "2019-12-23 08:54:00,573 epoch 30 - iter 56/280 - loss 0.97612946 - samples/sec: 146.36\n",
            "2019-12-23 08:54:06,943 epoch 30 - iter 84/280 - loss 1.00362005 - samples/sec: 141.14\n",
            "2019-12-23 08:54:12,819 epoch 30 - iter 112/280 - loss 0.96589365 - samples/sec: 152.98\n",
            "2019-12-23 08:54:18,952 epoch 30 - iter 140/280 - loss 0.96925147 - samples/sec: 146.53\n",
            "2019-12-23 08:54:25,292 epoch 30 - iter 168/280 - loss 0.97597635 - samples/sec: 141.80\n",
            "2019-12-23 08:54:31,525 epoch 30 - iter 196/280 - loss 0.98141752 - samples/sec: 144.25\n",
            "2019-12-23 08:54:37,736 epoch 30 - iter 224/280 - loss 0.98289266 - samples/sec: 144.79\n",
            "2019-12-23 08:54:43,901 epoch 30 - iter 252/280 - loss 0.98717162 - samples/sec: 145.80\n",
            "2019-12-23 08:54:49,581 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:54:49,582 EPOCH 30 done: loss 0.9802 - lr 0.1000\n",
            "2019-12-23 08:54:49,583 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:54:49,585 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:54:49,797 epoch 31 - iter 0/280 - loss 0.81919897 - samples/sec: 4322.61\n",
            "2019-12-23 08:54:56,104 epoch 31 - iter 28/280 - loss 1.11307323 - samples/sec: 142.49\n",
            "2019-12-23 08:55:02,196 epoch 31 - iter 56/280 - loss 1.02733338 - samples/sec: 147.59\n",
            "2019-12-23 08:55:08,298 epoch 31 - iter 84/280 - loss 0.99592009 - samples/sec: 147.41\n",
            "2019-12-23 08:55:14,353 epoch 31 - iter 112/280 - loss 0.98820037 - samples/sec: 148.46\n",
            "2019-12-23 08:55:20,561 epoch 31 - iter 140/280 - loss 0.97329226 - samples/sec: 144.89\n",
            "2019-12-23 08:55:26,792 epoch 31 - iter 168/280 - loss 0.97965602 - samples/sec: 144.23\n",
            "2019-12-23 08:55:32,837 epoch 31 - iter 196/280 - loss 0.98984406 - samples/sec: 148.77\n",
            "2019-12-23 08:55:38,959 epoch 31 - iter 224/280 - loss 0.98662358 - samples/sec: 146.85\n",
            "2019-12-23 08:55:45,294 epoch 31 - iter 252/280 - loss 0.99306782 - samples/sec: 141.88\n",
            "2019-12-23 08:55:51,217 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:55:51,218 EPOCH 31 done: loss 0.9886 - lr 0.1000\n",
            "2019-12-23 08:55:51,223 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 08:55:51,224 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:55:51,454 epoch 32 - iter 0/280 - loss 0.53945571 - samples/sec: 3957.49\n",
            "2019-12-23 08:55:57,517 epoch 32 - iter 28/280 - loss 0.85648485 - samples/sec: 148.22\n",
            "2019-12-23 08:56:03,885 epoch 32 - iter 56/280 - loss 0.89381618 - samples/sec: 141.18\n",
            "2019-12-23 08:56:10,001 epoch 32 - iter 84/280 - loss 0.90573307 - samples/sec: 146.99\n",
            "2019-12-23 08:56:15,982 epoch 32 - iter 112/280 - loss 0.92782901 - samples/sec: 150.30\n",
            "2019-12-23 08:56:22,291 epoch 32 - iter 140/280 - loss 0.94808001 - samples/sec: 142.52\n",
            "2019-12-23 08:56:28,388 epoch 32 - iter 168/280 - loss 0.93626186 - samples/sec: 147.47\n",
            "2019-12-23 08:56:34,568 epoch 32 - iter 196/280 - loss 0.92766291 - samples/sec: 145.49\n",
            "2019-12-23 08:56:40,797 epoch 32 - iter 224/280 - loss 0.92374796 - samples/sec: 144.31\n",
            "2019-12-23 08:56:46,786 epoch 32 - iter 252/280 - loss 0.93602427 - samples/sec: 150.16\n",
            "2019-12-23 08:56:52,865 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:56:52,869 EPOCH 32 done: loss 0.9432 - lr 0.1000\n",
            "2019-12-23 08:56:52,870 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:56:52,872 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:56:53,107 epoch 33 - iter 0/280 - loss 1.22059500 - samples/sec: 3841.97\n",
            "2019-12-23 08:56:59,185 epoch 33 - iter 28/280 - loss 1.02078114 - samples/sec: 147.87\n",
            "2019-12-23 08:57:05,436 epoch 33 - iter 56/280 - loss 0.95633490 - samples/sec: 143.81\n",
            "2019-12-23 08:57:11,598 epoch 33 - iter 84/280 - loss 0.95639087 - samples/sec: 145.92\n",
            "2019-12-23 08:57:17,842 epoch 33 - iter 112/280 - loss 0.92795850 - samples/sec: 144.07\n",
            "2019-12-23 08:57:24,076 epoch 33 - iter 140/280 - loss 0.92052246 - samples/sec: 144.18\n",
            "2019-12-23 08:57:30,316 epoch 33 - iter 168/280 - loss 0.91208043 - samples/sec: 144.09\n",
            "2019-12-23 08:57:36,485 epoch 33 - iter 196/280 - loss 0.89486555 - samples/sec: 145.75\n",
            "2019-12-23 08:57:42,717 epoch 33 - iter 224/280 - loss 0.90954103 - samples/sec: 144.24\n",
            "2019-12-23 08:57:48,838 epoch 33 - iter 252/280 - loss 0.91707927 - samples/sec: 146.85\n",
            "2019-12-23 08:57:54,695 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:57:54,696 EPOCH 33 done: loss 0.9304 - lr 0.1000\n",
            "2019-12-23 08:57:54,697 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:57:54,699 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:57:54,911 epoch 34 - iter 0/280 - loss 0.95418131 - samples/sec: 4316.22\n",
            "2019-12-23 08:58:01,066 epoch 34 - iter 28/280 - loss 0.88545211 - samples/sec: 146.05\n",
            "2019-12-23 08:58:07,356 epoch 34 - iter 56/280 - loss 0.91739531 - samples/sec: 143.03\n",
            "2019-12-23 08:58:13,643 epoch 34 - iter 84/280 - loss 0.92571931 - samples/sec: 143.03\n",
            "2019-12-23 08:58:19,679 epoch 34 - iter 112/280 - loss 0.90746896 - samples/sec: 148.99\n",
            "2019-12-23 08:58:25,828 epoch 34 - iter 140/280 - loss 0.91627560 - samples/sec: 146.21\n",
            "2019-12-23 08:58:32,129 epoch 34 - iter 168/280 - loss 0.92373889 - samples/sec: 142.74\n",
            "2019-12-23 08:58:38,545 epoch 34 - iter 196/280 - loss 0.91500394 - samples/sec: 140.15\n",
            "2019-12-23 08:58:44,583 epoch 34 - iter 224/280 - loss 0.91543953 - samples/sec: 148.92\n",
            "2019-12-23 08:58:50,679 epoch 34 - iter 252/280 - loss 0.91464012 - samples/sec: 147.51\n",
            "2019-12-23 08:58:56,426 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:58:56,427 EPOCH 34 done: loss 0.9087 - lr 0.1000\n",
            "2019-12-23 08:58:56,429 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:58:56,433 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:58:56,616 epoch 35 - iter 0/280 - loss 0.46293783 - samples/sec: 4981.91\n",
            "2019-12-23 08:59:02,698 epoch 35 - iter 28/280 - loss 0.89943391 - samples/sec: 147.80\n",
            "2019-12-23 08:59:08,711 epoch 35 - iter 56/280 - loss 0.87410317 - samples/sec: 149.56\n",
            "2019-12-23 08:59:14,937 epoch 35 - iter 84/280 - loss 0.89225952 - samples/sec: 144.43\n",
            "2019-12-23 08:59:21,123 epoch 35 - iter 112/280 - loss 0.88519702 - samples/sec: 145.32\n",
            "2019-12-23 08:59:27,162 epoch 35 - iter 140/280 - loss 0.87397803 - samples/sec: 148.89\n",
            "2019-12-23 08:59:33,279 epoch 35 - iter 168/280 - loss 0.87727201 - samples/sec: 146.94\n",
            "2019-12-23 08:59:39,580 epoch 35 - iter 196/280 - loss 0.87184546 - samples/sec: 142.73\n",
            "2019-12-23 08:59:45,829 epoch 35 - iter 224/280 - loss 0.87514720 - samples/sec: 143.97\n",
            "2019-12-23 08:59:51,902 epoch 35 - iter 252/280 - loss 0.87679495 - samples/sec: 148.12\n",
            "2019-12-23 08:59:57,731 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:59:57,732 EPOCH 35 done: loss 0.8783 - lr 0.1000\n",
            "2019-12-23 08:59:57,733 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 08:59:57,735 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 08:59:57,969 epoch 36 - iter 0/280 - loss 0.43286610 - samples/sec: 3872.80\n",
            "2019-12-23 09:00:04,027 epoch 36 - iter 28/280 - loss 0.82278008 - samples/sec: 148.34\n",
            "2019-12-23 09:00:10,278 epoch 36 - iter 56/280 - loss 0.79869893 - samples/sec: 143.83\n",
            "2019-12-23 09:00:16,422 epoch 36 - iter 84/280 - loss 0.81089861 - samples/sec: 146.37\n",
            "2019-12-23 09:00:22,536 epoch 36 - iter 112/280 - loss 0.81424203 - samples/sec: 147.05\n",
            "2019-12-23 09:00:30,043 epoch 36 - iter 140/280 - loss 0.81461642 - samples/sec: 119.73\n",
            "2019-12-23 09:00:36,296 epoch 36 - iter 168/280 - loss 0.83398742 - samples/sec: 143.73\n",
            "2019-12-23 09:00:42,551 epoch 36 - iter 196/280 - loss 0.84574718 - samples/sec: 143.73\n",
            "2019-12-23 09:00:48,877 epoch 36 - iter 224/280 - loss 0.84415905 - samples/sec: 142.05\n",
            "2019-12-23 09:00:55,069 epoch 36 - iter 252/280 - loss 0.84711341 - samples/sec: 145.20\n",
            "2019-12-23 09:01:00,827 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:01:00,828 EPOCH 36 done: loss 0.8525 - lr 0.1000\n",
            "2019-12-23 09:01:00,829 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:01:00,833 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:01:01,064 epoch 37 - iter 0/280 - loss 0.88878858 - samples/sec: 3917.22\n",
            "2019-12-23 09:01:07,336 epoch 37 - iter 28/280 - loss 0.94132104 - samples/sec: 143.30\n",
            "2019-12-23 09:01:13,537 epoch 37 - iter 56/280 - loss 0.86363997 - samples/sec: 144.96\n",
            "2019-12-23 09:01:19,559 epoch 37 - iter 84/280 - loss 0.83525655 - samples/sec: 149.31\n",
            "2019-12-23 09:01:25,861 epoch 37 - iter 112/280 - loss 0.85006912 - samples/sec: 142.67\n",
            "2019-12-23 09:01:32,134 epoch 37 - iter 140/280 - loss 0.85524139 - samples/sec: 143.41\n",
            "2019-12-23 09:01:38,362 epoch 37 - iter 168/280 - loss 0.84993449 - samples/sec: 144.37\n",
            "2019-12-23 09:01:44,544 epoch 37 - iter 196/280 - loss 0.85020226 - samples/sec: 145.42\n",
            "2019-12-23 09:01:50,757 epoch 37 - iter 224/280 - loss 0.84619322 - samples/sec: 144.69\n",
            "2019-12-23 09:01:56,834 epoch 37 - iter 252/280 - loss 0.84544328 - samples/sec: 147.94\n",
            "2019-12-23 09:02:02,589 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:02:02,593 EPOCH 37 done: loss 0.8581 - lr 0.1000\n",
            "2019-12-23 09:02:02,595 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:02:02,597 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:02:02,831 epoch 38 - iter 0/280 - loss 0.76458025 - samples/sec: 3944.04\n",
            "2019-12-23 09:02:09,089 epoch 38 - iter 28/280 - loss 0.76735758 - samples/sec: 143.58\n",
            "2019-12-23 09:02:15,456 epoch 38 - iter 56/280 - loss 0.78983383 - samples/sec: 141.21\n",
            "2019-12-23 09:02:21,766 epoch 38 - iter 84/280 - loss 0.76937990 - samples/sec: 142.57\n",
            "2019-12-23 09:02:27,784 epoch 38 - iter 112/280 - loss 0.78159516 - samples/sec: 149.46\n",
            "2019-12-23 09:02:34,271 epoch 38 - iter 140/280 - loss 0.77962894 - samples/sec: 138.55\n",
            "2019-12-23 09:02:40,507 epoch 38 - iter 168/280 - loss 0.79552406 - samples/sec: 144.22\n",
            "2019-12-23 09:02:46,502 epoch 38 - iter 196/280 - loss 0.80206043 - samples/sec: 149.96\n",
            "2019-12-23 09:02:52,927 epoch 38 - iter 224/280 - loss 0.80668788 - samples/sec: 139.94\n",
            "2019-12-23 09:02:58,948 epoch 38 - iter 252/280 - loss 0.80696067 - samples/sec: 149.32\n",
            "2019-12-23 09:03:04,872 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:03:04,873 EPOCH 38 done: loss 0.8154 - lr 0.1000\n",
            "2019-12-23 09:03:04,875 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:03:04,876 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:03:05,123 epoch 39 - iter 0/280 - loss 0.97243071 - samples/sec: 3670.27\n",
            "2019-12-23 09:03:11,134 epoch 39 - iter 28/280 - loss 0.89759829 - samples/sec: 149.55\n",
            "2019-12-23 09:03:17,324 epoch 39 - iter 56/280 - loss 0.80068899 - samples/sec: 145.29\n",
            "2019-12-23 09:03:23,585 epoch 39 - iter 84/280 - loss 0.82405550 - samples/sec: 143.58\n",
            "2019-12-23 09:03:29,717 epoch 39 - iter 112/280 - loss 0.82417266 - samples/sec: 146.67\n",
            "2019-12-23 09:03:36,194 epoch 39 - iter 140/280 - loss 0.83420042 - samples/sec: 138.83\n",
            "2019-12-23 09:03:42,319 epoch 39 - iter 168/280 - loss 0.82589018 - samples/sec: 146.82\n",
            "2019-12-23 09:03:48,548 epoch 39 - iter 196/280 - loss 0.81849977 - samples/sec: 144.32\n",
            "2019-12-23 09:03:54,678 epoch 39 - iter 224/280 - loss 0.82866367 - samples/sec: 146.66\n",
            "2019-12-23 09:04:00,580 epoch 39 - iter 252/280 - loss 0.83441865 - samples/sec: 152.31\n",
            "2019-12-23 09:04:06,424 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:04:06,427 EPOCH 39 done: loss 0.8353 - lr 0.1000\n",
            "2019-12-23 09:04:06,428 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:04:06,429 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:04:06,639 epoch 40 - iter 0/280 - loss 1.23112535 - samples/sec: 4325.60\n",
            "2019-12-23 09:04:12,975 epoch 40 - iter 28/280 - loss 0.84684030 - samples/sec: 141.87\n",
            "2019-12-23 09:04:19,267 epoch 40 - iter 56/280 - loss 0.84242307 - samples/sec: 142.92\n",
            "2019-12-23 09:04:25,691 epoch 40 - iter 84/280 - loss 0.83790677 - samples/sec: 139.89\n",
            "2019-12-23 09:04:31,609 epoch 40 - iter 112/280 - loss 0.83235573 - samples/sec: 152.08\n",
            "2019-12-23 09:04:37,581 epoch 40 - iter 140/280 - loss 0.82690167 - samples/sec: 150.60\n",
            "2019-12-23 09:04:43,528 epoch 40 - iter 168/280 - loss 0.81136331 - samples/sec: 151.12\n",
            "2019-12-23 09:04:49,493 epoch 40 - iter 196/280 - loss 0.82088104 - samples/sec: 150.73\n",
            "2019-12-23 09:04:55,429 epoch 40 - iter 224/280 - loss 0.81770173 - samples/sec: 151.48\n",
            "2019-12-23 09:05:01,373 epoch 40 - iter 252/280 - loss 0.81838085 - samples/sec: 151.30\n",
            "2019-12-23 09:05:06,945 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:05:06,946 EPOCH 40 done: loss 0.8158 - lr 0.1000\n",
            "2019-12-23 09:05:06,951 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:05:06,952 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:05:07,159 epoch 41 - iter 0/280 - loss 0.54387569 - samples/sec: 4382.66\n",
            "2019-12-23 09:05:13,167 epoch 41 - iter 28/280 - loss 0.78438992 - samples/sec: 149.59\n",
            "2019-12-23 09:05:19,128 epoch 41 - iter 56/280 - loss 0.81602597 - samples/sec: 150.79\n",
            "2019-12-23 09:05:25,007 epoch 41 - iter 84/280 - loss 0.78273299 - samples/sec: 152.89\n",
            "2019-12-23 09:05:30,964 epoch 41 - iter 112/280 - loss 0.76181648 - samples/sec: 150.96\n",
            "2019-12-23 09:05:37,133 epoch 41 - iter 140/280 - loss 0.77962728 - samples/sec: 145.71\n",
            "2019-12-23 09:05:43,130 epoch 41 - iter 168/280 - loss 0.78563521 - samples/sec: 149.90\n",
            "2019-12-23 09:05:49,201 epoch 41 - iter 196/280 - loss 0.79809215 - samples/sec: 148.10\n",
            "2019-12-23 09:05:55,176 epoch 41 - iter 224/280 - loss 0.78716241 - samples/sec: 150.48\n",
            "2019-12-23 09:06:01,267 epoch 41 - iter 252/280 - loss 0.77909106 - samples/sec: 147.55\n",
            "2019-12-23 09:06:06,873 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:06:06,876 EPOCH 41 done: loss 0.7774 - lr 0.1000\n",
            "2019-12-23 09:06:06,877 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:06:06,878 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:06:07,147 epoch 42 - iter 0/280 - loss 0.68909740 - samples/sec: 3365.59\n",
            "2019-12-23 09:06:13,013 epoch 42 - iter 28/280 - loss 0.75128261 - samples/sec: 153.23\n",
            "2019-12-23 09:06:19,120 epoch 42 - iter 56/280 - loss 0.72053853 - samples/sec: 147.18\n",
            "2019-12-23 09:06:25,145 epoch 42 - iter 84/280 - loss 0.74453423 - samples/sec: 149.23\n",
            "2019-12-23 09:06:31,293 epoch 42 - iter 112/280 - loss 0.71602455 - samples/sec: 146.26\n",
            "2019-12-23 09:06:37,373 epoch 42 - iter 140/280 - loss 0.72139866 - samples/sec: 148.08\n",
            "2019-12-23 09:06:43,359 epoch 42 - iter 168/280 - loss 0.74518733 - samples/sec: 150.19\n",
            "2019-12-23 09:06:49,565 epoch 42 - iter 196/280 - loss 0.75001401 - samples/sec: 144.82\n",
            "2019-12-23 09:06:55,607 epoch 42 - iter 224/280 - loss 0.75156642 - samples/sec: 148.77\n",
            "2019-12-23 09:07:01,760 epoch 42 - iter 252/280 - loss 0.75653304 - samples/sec: 146.15\n",
            "2019-12-23 09:07:07,488 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:07:07,492 EPOCH 42 done: loss 0.7598 - lr 0.1000\n",
            "2019-12-23 09:07:07,494 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:07:07,495 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:07:07,679 epoch 43 - iter 0/280 - loss 0.34666955 - samples/sec: 4908.76\n",
            "2019-12-23 09:07:13,673 epoch 43 - iter 28/280 - loss 0.69899650 - samples/sec: 150.06\n",
            "2019-12-23 09:07:19,798 epoch 43 - iter 56/280 - loss 0.69834369 - samples/sec: 146.85\n",
            "2019-12-23 09:07:25,984 epoch 43 - iter 84/280 - loss 0.71186489 - samples/sec: 145.30\n",
            "2019-12-23 09:07:32,114 epoch 43 - iter 112/280 - loss 0.72027468 - samples/sec: 146.64\n",
            "2019-12-23 09:07:38,518 epoch 43 - iter 140/280 - loss 0.74006881 - samples/sec: 140.37\n",
            "2019-12-23 09:07:44,547 epoch 43 - iter 168/280 - loss 0.75342647 - samples/sec: 149.16\n",
            "2019-12-23 09:07:50,805 epoch 43 - iter 196/280 - loss 0.76281678 - samples/sec: 143.58\n",
            "2019-12-23 09:07:56,815 epoch 43 - iter 224/280 - loss 0.76618834 - samples/sec: 149.64\n",
            "2019-12-23 09:08:02,703 epoch 43 - iter 252/280 - loss 0.76155374 - samples/sec: 152.73\n",
            "2019-12-23 09:08:08,514 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:08:08,516 EPOCH 43 done: loss 0.7588 - lr 0.1000\n",
            "2019-12-23 09:08:08,517 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:08:08,518 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:08:08,713 epoch 44 - iter 0/280 - loss 0.60234809 - samples/sec: 4656.56\n",
            "2019-12-23 09:08:14,756 epoch 44 - iter 28/280 - loss 0.74401954 - samples/sec: 148.70\n",
            "2019-12-23 09:08:20,725 epoch 44 - iter 56/280 - loss 0.71993198 - samples/sec: 150.63\n",
            "2019-12-23 09:08:26,953 epoch 44 - iter 84/280 - loss 0.71993527 - samples/sec: 144.37\n",
            "2019-12-23 09:08:32,885 epoch 44 - iter 112/280 - loss 0.74087998 - samples/sec: 151.53\n",
            "2019-12-23 09:08:39,037 epoch 44 - iter 140/280 - loss 0.74512570 - samples/sec: 146.16\n",
            "2019-12-23 09:08:44,965 epoch 44 - iter 168/280 - loss 0.75144901 - samples/sec: 151.71\n",
            "2019-12-23 09:08:51,131 epoch 44 - iter 196/280 - loss 0.75756780 - samples/sec: 145.82\n",
            "2019-12-23 09:08:57,159 epoch 44 - iter 224/280 - loss 0.75167919 - samples/sec: 149.18\n",
            "2019-12-23 09:09:03,366 epoch 44 - iter 252/280 - loss 0.75031808 - samples/sec: 144.82\n",
            "2019-12-23 09:09:09,205 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:09:09,206 EPOCH 44 done: loss 0.7474 - lr 0.1000\n",
            "2019-12-23 09:09:09,208 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:09:09,209 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:09:09,414 epoch 45 - iter 0/280 - loss 0.62848759 - samples/sec: 4412.04\n",
            "2019-12-23 09:09:15,753 epoch 45 - iter 28/280 - loss 0.70628751 - samples/sec: 141.77\n",
            "2019-12-23 09:09:21,784 epoch 45 - iter 56/280 - loss 0.70107978 - samples/sec: 149.13\n",
            "2019-12-23 09:09:27,723 epoch 45 - iter 84/280 - loss 0.72100572 - samples/sec: 151.35\n",
            "2019-12-23 09:09:33,970 epoch 45 - iter 112/280 - loss 0.71095226 - samples/sec: 144.04\n",
            "2019-12-23 09:09:40,139 epoch 45 - iter 140/280 - loss 0.71830032 - samples/sec: 145.79\n",
            "2019-12-23 09:09:46,001 epoch 45 - iter 168/280 - loss 0.72417937 - samples/sec: 153.33\n",
            "2019-12-23 09:09:52,023 epoch 45 - iter 196/280 - loss 0.73217876 - samples/sec: 149.36\n",
            "2019-12-23 09:09:58,020 epoch 45 - iter 224/280 - loss 0.72978785 - samples/sec: 149.94\n",
            "2019-12-23 09:10:04,069 epoch 45 - iter 252/280 - loss 0.73549709 - samples/sec: 148.64\n",
            "2019-12-23 09:10:09,689 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:10:09,692 EPOCH 45 done: loss 0.7396 - lr 0.1000\n",
            "2019-12-23 09:10:09,694 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:10:09,695 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:10:09,948 epoch 46 - iter 0/280 - loss 1.39407134 - samples/sec: 3588.60\n",
            "2019-12-23 09:10:15,837 epoch 46 - iter 28/280 - loss 0.72319195 - samples/sec: 152.62\n",
            "2019-12-23 09:10:21,915 epoch 46 - iter 56/280 - loss 0.70638695 - samples/sec: 147.89\n",
            "2019-12-23 09:10:27,853 epoch 46 - iter 84/280 - loss 0.69391236 - samples/sec: 151.46\n",
            "2019-12-23 09:10:34,004 epoch 46 - iter 112/280 - loss 0.70500097 - samples/sec: 146.16\n",
            "2019-12-23 09:10:40,146 epoch 46 - iter 140/280 - loss 0.72125838 - samples/sec: 146.43\n",
            "2019-12-23 09:10:46,389 epoch 46 - iter 168/280 - loss 0.72436398 - samples/sec: 143.97\n",
            "2019-12-23 09:10:52,325 epoch 46 - iter 196/280 - loss 0.72611455 - samples/sec: 151.50\n",
            "2019-12-23 09:10:58,325 epoch 46 - iter 224/280 - loss 0.73876620 - samples/sec: 149.86\n",
            "2019-12-23 09:11:04,346 epoch 46 - iter 252/280 - loss 0.74846478 - samples/sec: 149.30\n",
            "2019-12-23 09:11:09,924 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:11:09,925 EPOCH 46 done: loss 0.7395 - lr 0.1000\n",
            "2019-12-23 09:11:09,927 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:11:09,931 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:11:10,155 epoch 47 - iter 0/280 - loss 0.41505814 - samples/sec: 4040.55\n",
            "2019-12-23 09:11:16,112 epoch 47 - iter 28/280 - loss 0.65315377 - samples/sec: 150.95\n",
            "2019-12-23 09:11:22,066 epoch 47 - iter 56/280 - loss 0.71395494 - samples/sec: 151.00\n",
            "2019-12-23 09:11:28,243 epoch 47 - iter 84/280 - loss 0.73293537 - samples/sec: 145.53\n",
            "2019-12-23 09:11:34,394 epoch 47 - iter 112/280 - loss 0.72557746 - samples/sec: 146.15\n",
            "2019-12-23 09:11:40,545 epoch 47 - iter 140/280 - loss 0.73451976 - samples/sec: 146.10\n",
            "2019-12-23 09:11:46,542 epoch 47 - iter 168/280 - loss 0.72180269 - samples/sec: 149.89\n",
            "2019-12-23 09:11:52,618 epoch 47 - iter 196/280 - loss 0.71916834 - samples/sec: 147.98\n",
            "2019-12-23 09:11:58,446 epoch 47 - iter 224/280 - loss 0.71627324 - samples/sec: 154.32\n",
            "2019-12-23 09:12:04,495 epoch 47 - iter 252/280 - loss 0.70970306 - samples/sec: 148.66\n",
            "2019-12-23 09:12:11,733 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:12:11,734 EPOCH 47 done: loss 0.7117 - lr 0.1000\n",
            "2019-12-23 09:12:11,735 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:12:11,736 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:12:11,919 epoch 48 - iter 0/280 - loss 0.65750551 - samples/sec: 4941.60\n",
            "2019-12-23 09:12:18,015 epoch 48 - iter 28/280 - loss 0.71193409 - samples/sec: 147.41\n",
            "2019-12-23 09:12:23,822 epoch 48 - iter 56/280 - loss 0.69551999 - samples/sec: 154.92\n",
            "2019-12-23 09:12:29,910 epoch 48 - iter 84/280 - loss 0.68391030 - samples/sec: 147.67\n",
            "2019-12-23 09:12:36,168 epoch 48 - iter 112/280 - loss 0.70321685 - samples/sec: 143.65\n",
            "2019-12-23 09:12:42,376 epoch 48 - iter 140/280 - loss 0.69912283 - samples/sec: 144.77\n",
            "2019-12-23 09:12:48,671 epoch 48 - iter 168/280 - loss 0.70728492 - samples/sec: 142.80\n",
            "2019-12-23 09:12:54,893 epoch 48 - iter 196/280 - loss 0.71991394 - samples/sec: 144.45\n",
            "2019-12-23 09:13:00,807 epoch 48 - iter 224/280 - loss 0.72753564 - samples/sec: 152.01\n",
            "2019-12-23 09:13:06,937 epoch 48 - iter 252/280 - loss 0.73319869 - samples/sec: 146.69\n",
            "2019-12-23 09:13:12,549 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:13:12,552 EPOCH 48 done: loss 0.7389 - lr 0.1000\n",
            "2019-12-23 09:13:12,554 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:13:12,556 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:13:12,780 epoch 49 - iter 0/280 - loss 0.29534543 - samples/sec: 4038.85\n",
            "2019-12-23 09:13:18,762 epoch 49 - iter 28/280 - loss 0.70783591 - samples/sec: 150.22\n",
            "2019-12-23 09:13:24,918 epoch 49 - iter 56/280 - loss 0.67790606 - samples/sec: 146.06\n",
            "2019-12-23 09:13:30,994 epoch 49 - iter 84/280 - loss 0.67616797 - samples/sec: 147.96\n",
            "2019-12-23 09:13:37,027 epoch 49 - iter 112/280 - loss 0.68534676 - samples/sec: 149.03\n",
            "2019-12-23 09:13:43,171 epoch 49 - iter 140/280 - loss 0.68466325 - samples/sec: 146.36\n",
            "2019-12-23 09:13:49,144 epoch 49 - iter 168/280 - loss 0.68361744 - samples/sec: 150.50\n",
            "2019-12-23 09:13:55,161 epoch 49 - iter 196/280 - loss 0.67690781 - samples/sec: 149.38\n",
            "2019-12-23 09:14:01,242 epoch 49 - iter 224/280 - loss 0.67257532 - samples/sec: 147.85\n",
            "2019-12-23 09:14:07,341 epoch 49 - iter 252/280 - loss 0.67191126 - samples/sec: 147.38\n",
            "2019-12-23 09:14:13,010 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:14:13,013 EPOCH 49 done: loss 0.6762 - lr 0.1000\n",
            "2019-12-23 09:14:13,014 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:14:13,015 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:14:13,248 epoch 50 - iter 0/280 - loss 1.17450356 - samples/sec: 3882.18\n",
            "2019-12-23 09:14:19,113 epoch 50 - iter 28/280 - loss 0.70037723 - samples/sec: 153.27\n",
            "2019-12-23 09:14:25,217 epoch 50 - iter 56/280 - loss 0.70909372 - samples/sec: 147.25\n",
            "2019-12-23 09:14:31,176 epoch 50 - iter 84/280 - loss 0.70427846 - samples/sec: 150.84\n",
            "2019-12-23 09:14:37,063 epoch 50 - iter 112/280 - loss 0.70400742 - samples/sec: 152.76\n",
            "2019-12-23 09:14:43,155 epoch 50 - iter 140/280 - loss 0.70771259 - samples/sec: 147.59\n",
            "2019-12-23 09:14:49,118 epoch 50 - iter 168/280 - loss 0.70310775 - samples/sec: 150.76\n",
            "2019-12-23 09:14:55,357 epoch 50 - iter 196/280 - loss 0.70079032 - samples/sec: 144.08\n",
            "2019-12-23 09:15:01,267 epoch 50 - iter 224/280 - loss 0.69885292 - samples/sec: 152.10\n",
            "2019-12-23 09:15:07,579 epoch 50 - iter 252/280 - loss 0.70390246 - samples/sec: 142.42\n",
            "2019-12-23 09:15:13,166 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:15:13,169 EPOCH 50 done: loss 0.7110 - lr 0.1000\n",
            "2019-12-23 09:15:13,171 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:15:13,172 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:15:13,384 epoch 51 - iter 0/280 - loss 0.39530015 - samples/sec: 4292.28\n",
            "2019-12-23 09:15:19,366 epoch 51 - iter 28/280 - loss 0.62505308 - samples/sec: 150.25\n",
            "2019-12-23 09:15:25,608 epoch 51 - iter 56/280 - loss 0.63073687 - samples/sec: 143.99\n",
            "2019-12-23 09:15:31,547 epoch 51 - iter 84/280 - loss 0.63400431 - samples/sec: 151.38\n",
            "2019-12-23 09:15:37,775 epoch 51 - iter 112/280 - loss 0.63649786 - samples/sec: 144.44\n",
            "2019-12-23 09:15:43,695 epoch 51 - iter 140/280 - loss 0.63253604 - samples/sec: 151.86\n",
            "2019-12-23 09:15:49,616 epoch 51 - iter 168/280 - loss 0.64337437 - samples/sec: 151.83\n",
            "2019-12-23 09:15:55,470 epoch 51 - iter 196/280 - loss 0.65184568 - samples/sec: 153.60\n",
            "2019-12-23 09:16:01,975 epoch 51 - iter 224/280 - loss 0.65510696 - samples/sec: 138.26\n",
            "2019-12-23 09:16:08,021 epoch 51 - iter 252/280 - loss 0.67267140 - samples/sec: 148.66\n",
            "2019-12-23 09:16:13,522 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:16:13,523 EPOCH 51 done: loss 0.6701 - lr 0.1000\n",
            "2019-12-23 09:16:13,528 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:16:13,529 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:16:13,749 epoch 52 - iter 0/280 - loss 1.35694075 - samples/sec: 4123.36\n",
            "2019-12-23 09:16:19,809 epoch 52 - iter 28/280 - loss 0.68959494 - samples/sec: 148.30\n",
            "2019-12-23 09:16:25,792 epoch 52 - iter 56/280 - loss 0.67475314 - samples/sec: 150.29\n",
            "2019-12-23 09:16:31,798 epoch 52 - iter 84/280 - loss 0.67763961 - samples/sec: 149.67\n",
            "2019-12-23 09:16:37,810 epoch 52 - iter 112/280 - loss 0.68223130 - samples/sec: 149.68\n",
            "2019-12-23 09:16:43,802 epoch 52 - iter 140/280 - loss 0.67810960 - samples/sec: 150.07\n",
            "2019-12-23 09:16:49,746 epoch 52 - iter 168/280 - loss 0.67002997 - samples/sec: 151.23\n",
            "2019-12-23 09:16:55,615 epoch 52 - iter 196/280 - loss 0.66058891 - samples/sec: 153.23\n",
            "2019-12-23 09:17:01,678 epoch 52 - iter 224/280 - loss 0.66512192 - samples/sec: 148.29\n",
            "2019-12-23 09:17:07,720 epoch 52 - iter 252/280 - loss 0.66514930 - samples/sec: 148.88\n",
            "2019-12-23 09:17:13,358 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:17:13,359 EPOCH 52 done: loss 0.6657 - lr 0.1000\n",
            "2019-12-23 09:17:13,360 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:17:13,362 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:17:13,578 epoch 53 - iter 0/280 - loss 0.41289642 - samples/sec: 4222.69\n",
            "2019-12-23 09:17:19,565 epoch 53 - iter 28/280 - loss 0.61398105 - samples/sec: 150.09\n",
            "2019-12-23 09:17:25,695 epoch 53 - iter 56/280 - loss 0.62177379 - samples/sec: 146.72\n",
            "2019-12-23 09:17:31,947 epoch 53 - iter 84/280 - loss 0.62562770 - samples/sec: 143.80\n",
            "2019-12-23 09:17:38,021 epoch 53 - iter 112/280 - loss 0.61865210 - samples/sec: 148.04\n",
            "2019-12-23 09:17:44,020 epoch 53 - iter 140/280 - loss 0.61365215 - samples/sec: 149.86\n",
            "2019-12-23 09:17:50,010 epoch 53 - iter 168/280 - loss 0.61332914 - samples/sec: 150.09\n",
            "2019-12-23 09:17:55,822 epoch 53 - iter 196/280 - loss 0.61503917 - samples/sec: 154.71\n",
            "2019-12-23 09:18:02,073 epoch 53 - iter 224/280 - loss 0.63516771 - samples/sec: 143.95\n",
            "2019-12-23 09:18:08,071 epoch 53 - iter 252/280 - loss 0.63336928 - samples/sec: 149.92\n",
            "2019-12-23 09:18:13,829 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:18:13,830 EPOCH 53 done: loss 0.6376 - lr 0.1000\n",
            "2019-12-23 09:18:13,831 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:18:13,833 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:18:14,050 epoch 54 - iter 0/280 - loss 0.26068288 - samples/sec: 4156.81\n",
            "2019-12-23 09:18:20,101 epoch 54 - iter 28/280 - loss 0.71295323 - samples/sec: 148.51\n",
            "2019-12-23 09:18:26,456 epoch 54 - iter 56/280 - loss 0.65852655 - samples/sec: 141.43\n",
            "2019-12-23 09:18:32,459 epoch 54 - iter 84/280 - loss 0.65619702 - samples/sec: 149.77\n",
            "2019-12-23 09:18:38,647 epoch 54 - iter 112/280 - loss 0.64603585 - samples/sec: 145.29\n",
            "2019-12-23 09:18:44,451 epoch 54 - iter 140/280 - loss 0.65061117 - samples/sec: 155.06\n",
            "2019-12-23 09:18:50,422 epoch 54 - iter 168/280 - loss 0.65480657 - samples/sec: 150.56\n",
            "2019-12-23 09:18:56,464 epoch 54 - iter 196/280 - loss 0.65012807 - samples/sec: 148.79\n",
            "2019-12-23 09:19:02,388 epoch 54 - iter 224/280 - loss 0.65635330 - samples/sec: 151.76\n",
            "2019-12-23 09:19:08,430 epoch 54 - iter 252/280 - loss 0.65524961 - samples/sec: 148.79\n",
            "2019-12-23 09:19:14,130 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:19:14,132 EPOCH 54 done: loss 0.6625 - lr 0.1000\n",
            "2019-12-23 09:19:14,136 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:19:14,139 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:19:14,345 epoch 55 - iter 0/280 - loss 0.35481703 - samples/sec: 4393.01\n",
            "2019-12-23 09:19:20,335 epoch 55 - iter 28/280 - loss 0.68839227 - samples/sec: 150.15\n",
            "2019-12-23 09:19:26,251 epoch 55 - iter 56/280 - loss 0.66044515 - samples/sec: 152.03\n",
            "2019-12-23 09:19:32,089 epoch 55 - iter 84/280 - loss 0.66455760 - samples/sec: 153.97\n",
            "2019-12-23 09:19:38,209 epoch 55 - iter 112/280 - loss 0.64862403 - samples/sec: 146.88\n",
            "2019-12-23 09:19:44,205 epoch 55 - iter 140/280 - loss 0.64800852 - samples/sec: 150.02\n",
            "2019-12-23 09:19:50,135 epoch 55 - iter 168/280 - loss 0.63534475 - samples/sec: 151.56\n",
            "2019-12-23 09:19:55,967 epoch 55 - iter 196/280 - loss 0.62575004 - samples/sec: 154.17\n",
            "2019-12-23 09:20:01,780 epoch 55 - iter 224/280 - loss 0.62996325 - samples/sec: 154.67\n",
            "2019-12-23 09:20:07,963 epoch 55 - iter 252/280 - loss 0.64238159 - samples/sec: 145.42\n",
            "2019-12-23 09:20:13,994 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:20:13,995 EPOCH 55 done: loss 0.6455 - lr 0.1000\n",
            "2019-12-23 09:20:13,999 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:20:14,000 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:20:14,210 epoch 56 - iter 0/280 - loss 0.39060336 - samples/sec: 4298.10\n",
            "2019-12-23 09:20:20,349 epoch 56 - iter 28/280 - loss 0.62159276 - samples/sec: 146.42\n",
            "2019-12-23 09:20:26,409 epoch 56 - iter 56/280 - loss 0.62938368 - samples/sec: 148.39\n",
            "2019-12-23 09:20:32,362 epoch 56 - iter 84/280 - loss 0.63235926 - samples/sec: 151.00\n",
            "2019-12-23 09:20:38,832 epoch 56 - iter 112/280 - loss 0.66827952 - samples/sec: 138.91\n",
            "2019-12-23 09:20:45,162 epoch 56 - iter 140/280 - loss 0.66864173 - samples/sec: 142.00\n",
            "2019-12-23 09:20:50,944 epoch 56 - iter 168/280 - loss 0.66592340 - samples/sec: 155.46\n",
            "2019-12-23 09:20:56,777 epoch 56 - iter 196/280 - loss 0.65414416 - samples/sec: 154.08\n",
            "2019-12-23 09:21:02,701 epoch 56 - iter 224/280 - loss 0.64797410 - samples/sec: 151.81\n",
            "2019-12-23 09:21:08,672 epoch 56 - iter 252/280 - loss 0.65752438 - samples/sec: 150.55\n",
            "2019-12-23 09:21:14,367 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:21:14,368 EPOCH 56 done: loss 0.6574 - lr 0.1000\n",
            "2019-12-23 09:21:14,370 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 09:21:14,372 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:21:14,567 epoch 57 - iter 0/280 - loss 0.38945162 - samples/sec: 4641.45\n",
            "2019-12-23 09:21:20,627 epoch 57 - iter 28/280 - loss 0.67544440 - samples/sec: 148.32\n",
            "2019-12-23 09:21:26,797 epoch 57 - iter 56/280 - loss 0.64200741 - samples/sec: 145.69\n",
            "2019-12-23 09:21:32,845 epoch 57 - iter 84/280 - loss 0.61930214 - samples/sec: 148.66\n",
            "2019-12-23 09:21:39,058 epoch 57 - iter 112/280 - loss 0.62772803 - samples/sec: 144.72\n",
            "2019-12-23 09:21:44,957 epoch 57 - iter 140/280 - loss 0.62967698 - samples/sec: 152.42\n",
            "2019-12-23 09:21:50,826 epoch 57 - iter 168/280 - loss 0.61565956 - samples/sec: 153.16\n",
            "2019-12-23 09:21:56,958 epoch 57 - iter 196/280 - loss 0.63208346 - samples/sec: 146.58\n",
            "2019-12-23 09:22:02,841 epoch 57 - iter 224/280 - loss 0.63739351 - samples/sec: 152.84\n",
            "2019-12-23 09:22:08,913 epoch 57 - iter 252/280 - loss 0.63653882 - samples/sec: 148.04\n",
            "2019-12-23 09:22:14,589 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:22:14,590 EPOCH 57 done: loss 0.6347 - lr 0.1000\n",
            "2019-12-23 09:22:14,595 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:22:14,599 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:22:14,814 epoch 58 - iter 0/280 - loss 0.45599818 - samples/sec: 4219.31\n",
            "2019-12-23 09:22:20,891 epoch 58 - iter 28/280 - loss 0.61206173 - samples/sec: 147.89\n",
            "2019-12-23 09:22:26,931 epoch 58 - iter 56/280 - loss 0.62764446 - samples/sec: 148.82\n",
            "2019-12-23 09:22:32,823 epoch 58 - iter 84/280 - loss 0.60361205 - samples/sec: 152.67\n",
            "2019-12-23 09:22:38,893 epoch 58 - iter 112/280 - loss 0.60874624 - samples/sec: 148.20\n",
            "2019-12-23 09:22:45,001 epoch 58 - iter 140/280 - loss 0.62251794 - samples/sec: 147.19\n",
            "2019-12-23 09:22:51,195 epoch 58 - iter 168/280 - loss 0.62066977 - samples/sec: 145.15\n",
            "2019-12-23 09:22:57,046 epoch 58 - iter 196/280 - loss 0.61229594 - samples/sec: 153.63\n",
            "2019-12-23 09:23:03,096 epoch 58 - iter 224/280 - loss 0.60671632 - samples/sec: 148.64\n",
            "2019-12-23 09:23:09,243 epoch 58 - iter 252/280 - loss 0.60483486 - samples/sec: 146.31\n",
            "2019-12-23 09:23:15,237 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:23:15,238 EPOCH 58 done: loss 0.6070 - lr 0.1000\n",
            "2019-12-23 09:23:15,240 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:23:15,241 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:23:15,501 epoch 59 - iter 0/280 - loss 0.44025421 - samples/sec: 3477.95\n",
            "2019-12-23 09:23:21,577 epoch 59 - iter 28/280 - loss 0.53815652 - samples/sec: 147.89\n",
            "2019-12-23 09:23:29,051 epoch 59 - iter 56/280 - loss 0.58936321 - samples/sec: 120.20\n",
            "2019-12-23 09:23:35,202 epoch 59 - iter 84/280 - loss 0.60549134 - samples/sec: 146.10\n",
            "2019-12-23 09:23:40,990 epoch 59 - iter 112/280 - loss 0.60306482 - samples/sec: 155.31\n",
            "2019-12-23 09:23:47,294 epoch 59 - iter 140/280 - loss 0.60812702 - samples/sec: 142.62\n",
            "2019-12-23 09:23:53,416 epoch 59 - iter 168/280 - loss 0.60903154 - samples/sec: 146.83\n",
            "2019-12-23 09:23:59,462 epoch 59 - iter 196/280 - loss 0.60147315 - samples/sec: 148.71\n",
            "2019-12-23 09:24:05,224 epoch 59 - iter 224/280 - loss 0.60111602 - samples/sec: 156.08\n",
            "2019-12-23 09:24:11,088 epoch 59 - iter 252/280 - loss 0.59636055 - samples/sec: 153.36\n",
            "2019-12-23 09:24:16,797 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:24:16,798 EPOCH 59 done: loss 0.5952 - lr 0.1000\n",
            "2019-12-23 09:24:16,801 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:24:16,803 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:24:17,022 epoch 60 - iter 0/280 - loss 0.47374856 - samples/sec: 4160.90\n",
            "2019-12-23 09:24:23,185 epoch 60 - iter 28/280 - loss 0.65763395 - samples/sec: 145.80\n",
            "2019-12-23 09:24:29,383 epoch 60 - iter 56/280 - loss 0.62642308 - samples/sec: 145.08\n",
            "2019-12-23 09:24:35,488 epoch 60 - iter 84/280 - loss 0.62162147 - samples/sec: 147.30\n",
            "2019-12-23 09:24:41,540 epoch 60 - iter 112/280 - loss 0.63313424 - samples/sec: 148.58\n",
            "2019-12-23 09:24:47,437 epoch 60 - iter 140/280 - loss 0.62538481 - samples/sec: 152.42\n",
            "2019-12-23 09:24:53,487 epoch 60 - iter 168/280 - loss 0.61517814 - samples/sec: 148.57\n",
            "2019-12-23 09:24:59,580 epoch 60 - iter 196/280 - loss 0.61583827 - samples/sec: 147.57\n",
            "2019-12-23 09:25:05,549 epoch 60 - iter 224/280 - loss 0.61738462 - samples/sec: 150.64\n",
            "2019-12-23 09:25:11,547 epoch 60 - iter 252/280 - loss 0.61066206 - samples/sec: 149.84\n",
            "2019-12-23 09:25:17,183 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:25:17,184 EPOCH 60 done: loss 0.6206 - lr 0.1000\n",
            "2019-12-23 09:25:17,185 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:25:17,186 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:25:17,400 epoch 61 - iter 0/280 - loss 0.45675564 - samples/sec: 4237.48\n",
            "2019-12-23 09:25:23,307 epoch 61 - iter 28/280 - loss 0.64924086 - samples/sec: 152.17\n",
            "2019-12-23 09:25:29,417 epoch 61 - iter 56/280 - loss 0.58847217 - samples/sec: 147.21\n",
            "2019-12-23 09:25:35,457 epoch 61 - iter 84/280 - loss 0.58722941 - samples/sec: 148.85\n",
            "2019-12-23 09:25:41,317 epoch 61 - iter 112/280 - loss 0.58345143 - samples/sec: 153.40\n",
            "2019-12-23 09:25:47,454 epoch 61 - iter 140/280 - loss 0.60377159 - samples/sec: 146.48\n",
            "2019-12-23 09:25:53,514 epoch 61 - iter 168/280 - loss 0.61557440 - samples/sec: 148.38\n",
            "2019-12-23 09:25:59,584 epoch 61 - iter 196/280 - loss 0.61595247 - samples/sec: 148.12\n",
            "2019-12-23 09:26:05,840 epoch 61 - iter 224/280 - loss 0.62213640 - samples/sec: 143.89\n",
            "2019-12-23 09:26:11,784 epoch 61 - iter 252/280 - loss 0.61855836 - samples/sec: 151.25\n",
            "2019-12-23 09:26:17,407 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:26:17,410 EPOCH 61 done: loss 0.6214 - lr 0.1000\n",
            "2019-12-23 09:26:17,414 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:26:17,416 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:26:17,623 epoch 62 - iter 0/280 - loss 0.36213231 - samples/sec: 4406.24\n",
            "2019-12-23 09:26:23,563 epoch 62 - iter 28/280 - loss 0.58510548 - samples/sec: 151.49\n",
            "2019-12-23 09:26:29,660 epoch 62 - iter 56/280 - loss 0.60589897 - samples/sec: 147.43\n",
            "2019-12-23 09:26:35,852 epoch 62 - iter 84/280 - loss 0.58756643 - samples/sec: 145.18\n",
            "2019-12-23 09:26:42,098 epoch 62 - iter 112/280 - loss 0.59826913 - samples/sec: 143.97\n",
            "2019-12-23 09:26:48,192 epoch 62 - iter 140/280 - loss 0.61251580 - samples/sec: 147.56\n",
            "2019-12-23 09:26:54,316 epoch 62 - iter 168/280 - loss 0.61007096 - samples/sec: 146.75\n",
            "2019-12-23 09:27:00,334 epoch 62 - iter 196/280 - loss 0.61868676 - samples/sec: 149.41\n",
            "2019-12-23 09:27:06,244 epoch 62 - iter 224/280 - loss 0.61581699 - samples/sec: 152.22\n",
            "2019-12-23 09:27:12,285 epoch 62 - iter 252/280 - loss 0.61248168 - samples/sec: 148.81\n",
            "2019-12-23 09:27:18,001 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:27:18,002 EPOCH 62 done: loss 0.6102 - lr 0.1000\n",
            "2019-12-23 09:27:18,006 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 09:27:18,007 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:27:18,214 epoch 63 - iter 0/280 - loss 1.30678153 - samples/sec: 4384.24\n",
            "2019-12-23 09:27:24,429 epoch 63 - iter 28/280 - loss 0.63422388 - samples/sec: 144.60\n",
            "2019-12-23 09:27:30,495 epoch 63 - iter 56/280 - loss 0.62817717 - samples/sec: 148.31\n",
            "2019-12-23 09:27:36,656 epoch 63 - iter 84/280 - loss 0.62134745 - samples/sec: 145.95\n",
            "2019-12-23 09:27:42,555 epoch 63 - iter 112/280 - loss 0.62276621 - samples/sec: 152.48\n",
            "2019-12-23 09:27:48,481 epoch 63 - iter 140/280 - loss 0.60867005 - samples/sec: 151.68\n",
            "2019-12-23 09:27:54,315 epoch 63 - iter 168/280 - loss 0.60303945 - samples/sec: 154.12\n",
            "2019-12-23 09:28:00,523 epoch 63 - iter 196/280 - loss 0.59899111 - samples/sec: 144.79\n",
            "2019-12-23 09:28:06,564 epoch 63 - iter 224/280 - loss 0.59767186 - samples/sec: 148.89\n",
            "2019-12-23 09:28:12,706 epoch 63 - iter 252/280 - loss 0.59635814 - samples/sec: 146.34\n",
            "2019-12-23 09:28:18,466 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:28:18,467 EPOCH 63 done: loss 0.5974 - lr 0.1000\n",
            "Epoch    62: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2019-12-23 09:28:18,468 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 09:28:18,470 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:28:18,742 epoch 64 - iter 0/280 - loss 0.55281007 - samples/sec: 3342.02\n",
            "2019-12-23 09:28:24,898 epoch 64 - iter 28/280 - loss 0.49393992 - samples/sec: 145.98\n",
            "2019-12-23 09:28:30,898 epoch 64 - iter 56/280 - loss 0.52902752 - samples/sec: 149.85\n",
            "2019-12-23 09:28:37,169 epoch 64 - iter 84/280 - loss 0.54279958 - samples/sec: 143.34\n",
            "2019-12-23 09:28:43,060 epoch 64 - iter 112/280 - loss 0.53618564 - samples/sec: 152.58\n",
            "2019-12-23 09:28:49,276 epoch 64 - iter 140/280 - loss 0.54451408 - samples/sec: 144.74\n",
            "2019-12-23 09:28:55,338 epoch 64 - iter 168/280 - loss 0.55684866 - samples/sec: 148.27\n",
            "2019-12-23 09:29:01,243 epoch 64 - iter 196/280 - loss 0.55329301 - samples/sec: 152.21\n",
            "2019-12-23 09:29:07,160 epoch 64 - iter 224/280 - loss 0.54974693 - samples/sec: 152.00\n",
            "2019-12-23 09:29:13,234 epoch 64 - iter 252/280 - loss 0.55787809 - samples/sec: 147.97\n",
            "2019-12-23 09:29:18,857 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:29:18,859 EPOCH 64 done: loss 0.5517 - lr 0.0500\n",
            "2019-12-23 09:29:18,861 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:29:18,864 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:29:19,059 epoch 65 - iter 0/280 - loss 0.88137555 - samples/sec: 4743.42\n",
            "2019-12-23 09:29:25,021 epoch 65 - iter 28/280 - loss 0.54938857 - samples/sec: 150.76\n",
            "2019-12-23 09:29:31,216 epoch 65 - iter 56/280 - loss 0.54375543 - samples/sec: 145.09\n",
            "2019-12-23 09:29:37,268 epoch 65 - iter 84/280 - loss 0.56345942 - samples/sec: 148.55\n",
            "2019-12-23 09:29:43,298 epoch 65 - iter 112/280 - loss 0.55409677 - samples/sec: 149.19\n",
            "2019-12-23 09:29:49,503 epoch 65 - iter 140/280 - loss 0.53335654 - samples/sec: 144.87\n",
            "2019-12-23 09:29:55,418 epoch 65 - iter 168/280 - loss 0.52652300 - samples/sec: 152.01\n",
            "2019-12-23 09:30:01,433 epoch 65 - iter 196/280 - loss 0.53028707 - samples/sec: 149.49\n",
            "2019-12-23 09:30:07,417 epoch 65 - iter 224/280 - loss 0.52854842 - samples/sec: 150.24\n",
            "2019-12-23 09:30:13,386 epoch 65 - iter 252/280 - loss 0.52609572 - samples/sec: 150.63\n",
            "2019-12-23 09:30:19,156 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:30:19,157 EPOCH 65 done: loss 0.5234 - lr 0.0500\n",
            "2019-12-23 09:30:19,160 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:30:19,162 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:30:19,375 epoch 66 - iter 0/280 - loss 0.51765704 - samples/sec: 4268.73\n",
            "2019-12-23 09:30:25,411 epoch 66 - iter 28/280 - loss 0.50796761 - samples/sec: 148.87\n",
            "2019-12-23 09:30:31,379 epoch 66 - iter 56/280 - loss 0.50351569 - samples/sec: 150.63\n",
            "2019-12-23 09:30:37,604 epoch 66 - iter 84/280 - loss 0.49569189 - samples/sec: 144.46\n",
            "2019-12-23 09:30:43,594 epoch 66 - iter 112/280 - loss 0.49781998 - samples/sec: 150.07\n",
            "2019-12-23 09:30:49,623 epoch 66 - iter 140/280 - loss 0.50228869 - samples/sec: 149.19\n",
            "2019-12-23 09:30:55,710 epoch 66 - iter 168/280 - loss 0.50028296 - samples/sec: 147.65\n",
            "2019-12-23 09:31:01,683 epoch 66 - iter 196/280 - loss 0.49383216 - samples/sec: 150.55\n",
            "2019-12-23 09:31:07,800 epoch 66 - iter 224/280 - loss 0.49523364 - samples/sec: 147.01\n",
            "2019-12-23 09:31:13,700 epoch 66 - iter 252/280 - loss 0.50243979 - samples/sec: 152.36\n",
            "2019-12-23 09:31:19,386 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:31:19,388 EPOCH 66 done: loss 0.5032 - lr 0.0500\n",
            "2019-12-23 09:31:19,389 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:31:19,390 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:31:19,598 epoch 67 - iter 0/280 - loss 0.37560952 - samples/sec: 4332.81\n",
            "2019-12-23 09:31:25,653 epoch 67 - iter 28/280 - loss 0.47079856 - samples/sec: 148.44\n",
            "2019-12-23 09:31:31,607 epoch 67 - iter 56/280 - loss 0.47719989 - samples/sec: 151.04\n",
            "2019-12-23 09:31:37,671 epoch 67 - iter 84/280 - loss 0.47960850 - samples/sec: 148.22\n",
            "2019-12-23 09:31:43,958 epoch 67 - iter 112/280 - loss 0.48515571 - samples/sec: 142.99\n",
            "2019-12-23 09:31:50,353 epoch 67 - iter 140/280 - loss 0.50172170 - samples/sec: 140.56\n",
            "2019-12-23 09:31:56,378 epoch 67 - iter 168/280 - loss 0.50897529 - samples/sec: 149.19\n",
            "2019-12-23 09:32:02,164 epoch 67 - iter 196/280 - loss 0.51150039 - samples/sec: 155.35\n",
            "2019-12-23 09:32:08,041 epoch 67 - iter 224/280 - loss 0.50759992 - samples/sec: 152.96\n",
            "2019-12-23 09:32:14,027 epoch 67 - iter 252/280 - loss 0.50533119 - samples/sec: 150.16\n",
            "2019-12-23 09:32:19,951 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:32:19,952 EPOCH 67 done: loss 0.5025 - lr 0.0500\n",
            "2019-12-23 09:32:19,953 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:32:19,954 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:32:20,157 epoch 68 - iter 0/280 - loss 0.15395546 - samples/sec: 4470.75\n",
            "2019-12-23 09:32:26,136 epoch 68 - iter 28/280 - loss 0.45756240 - samples/sec: 150.30\n",
            "2019-12-23 09:32:32,261 epoch 68 - iter 56/280 - loss 0.49220538 - samples/sec: 146.77\n",
            "2019-12-23 09:32:38,473 epoch 68 - iter 84/280 - loss 0.50508135 - samples/sec: 144.78\n",
            "2019-12-23 09:32:44,665 epoch 68 - iter 112/280 - loss 0.49358453 - samples/sec: 145.17\n",
            "2019-12-23 09:32:50,529 epoch 68 - iter 140/280 - loss 0.48579988 - samples/sec: 153.36\n",
            "2019-12-23 09:32:56,539 epoch 68 - iter 168/280 - loss 0.47072901 - samples/sec: 149.62\n",
            "2019-12-23 09:33:02,550 epoch 68 - iter 196/280 - loss 0.47136546 - samples/sec: 149.52\n",
            "2019-12-23 09:33:08,697 epoch 68 - iter 224/280 - loss 0.47020355 - samples/sec: 146.34\n",
            "2019-12-23 09:33:14,732 epoch 68 - iter 252/280 - loss 0.46997740 - samples/sec: 149.03\n",
            "2019-12-23 09:33:20,522 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:33:20,525 EPOCH 68 done: loss 0.4751 - lr 0.0500\n",
            "2019-12-23 09:33:20,526 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:33:20,529 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:33:20,773 epoch 69 - iter 0/280 - loss 0.42363071 - samples/sec: 3715.77\n",
            "2019-12-23 09:33:27,178 epoch 69 - iter 28/280 - loss 0.46750936 - samples/sec: 140.27\n",
            "2019-12-23 09:33:33,235 epoch 69 - iter 56/280 - loss 0.47031614 - samples/sec: 148.48\n",
            "2019-12-23 09:33:39,402 epoch 69 - iter 84/280 - loss 0.46564450 - samples/sec: 145.80\n",
            "2019-12-23 09:33:45,126 epoch 69 - iter 112/280 - loss 0.48241563 - samples/sec: 157.12\n",
            "2019-12-23 09:33:51,311 epoch 69 - iter 140/280 - loss 0.47289495 - samples/sec: 145.36\n",
            "2019-12-23 09:33:57,320 epoch 69 - iter 168/280 - loss 0.46959335 - samples/sec: 149.66\n",
            "2019-12-23 09:34:03,255 epoch 69 - iter 196/280 - loss 0.47430654 - samples/sec: 151.46\n",
            "2019-12-23 09:34:09,199 epoch 69 - iter 224/280 - loss 0.46942445 - samples/sec: 151.27\n",
            "2019-12-23 09:34:15,197 epoch 69 - iter 252/280 - loss 0.47686676 - samples/sec: 149.83\n",
            "2019-12-23 09:34:20,923 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:34:20,924 EPOCH 69 done: loss 0.4859 - lr 0.0500\n",
            "2019-12-23 09:34:20,928 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:34:20,930 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:34:21,128 epoch 70 - iter 0/280 - loss 0.30324650 - samples/sec: 4615.44\n",
            "2019-12-23 09:34:27,165 epoch 70 - iter 28/280 - loss 0.49295376 - samples/sec: 148.93\n",
            "2019-12-23 09:34:33,328 epoch 70 - iter 56/280 - loss 0.51406114 - samples/sec: 145.84\n",
            "2019-12-23 09:34:39,166 epoch 70 - iter 84/280 - loss 0.49693895 - samples/sec: 154.06\n",
            "2019-12-23 09:34:45,054 epoch 70 - iter 112/280 - loss 0.48882948 - samples/sec: 152.68\n",
            "2019-12-23 09:34:51,123 epoch 70 - iter 140/280 - loss 0.47703704 - samples/sec: 148.09\n",
            "2019-12-23 09:34:57,198 epoch 70 - iter 168/280 - loss 0.48736143 - samples/sec: 148.01\n",
            "2019-12-23 09:35:04,728 epoch 70 - iter 196/280 - loss 0.47896377 - samples/sec: 119.30\n",
            "2019-12-23 09:35:10,723 epoch 70 - iter 224/280 - loss 0.48100605 - samples/sec: 150.02\n",
            "2019-12-23 09:35:16,609 epoch 70 - iter 252/280 - loss 0.47803752 - samples/sec: 152.78\n",
            "2019-12-23 09:35:22,385 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:35:22,386 EPOCH 70 done: loss 0.4830 - lr 0.0500\n",
            "2019-12-23 09:35:22,389 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:35:22,391 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:35:22,642 epoch 71 - iter 0/280 - loss 0.61101723 - samples/sec: 3626.30\n",
            "2019-12-23 09:35:28,696 epoch 71 - iter 28/280 - loss 0.50708850 - samples/sec: 148.49\n",
            "2019-12-23 09:35:34,696 epoch 71 - iter 56/280 - loss 0.47720996 - samples/sec: 149.79\n",
            "2019-12-23 09:35:40,566 epoch 71 - iter 84/280 - loss 0.47662819 - samples/sec: 153.19\n",
            "2019-12-23 09:35:46,553 epoch 71 - iter 112/280 - loss 0.49072919 - samples/sec: 150.19\n",
            "2019-12-23 09:35:52,434 epoch 71 - iter 140/280 - loss 0.48570479 - samples/sec: 152.88\n",
            "2019-12-23 09:35:58,613 epoch 71 - iter 168/280 - loss 0.48470612 - samples/sec: 145.51\n",
            "2019-12-23 09:36:04,465 epoch 71 - iter 196/280 - loss 0.48121109 - samples/sec: 153.67\n",
            "2019-12-23 09:36:10,455 epoch 71 - iter 224/280 - loss 0.47775885 - samples/sec: 150.07\n",
            "2019-12-23 09:36:16,684 epoch 71 - iter 252/280 - loss 0.47606761 - samples/sec: 144.31\n",
            "2019-12-23 09:36:22,336 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:36:22,338 EPOCH 71 done: loss 0.4710 - lr 0.0500\n",
            "2019-12-23 09:36:22,341 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:36:22,343 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:36:22,567 epoch 72 - iter 0/280 - loss 0.49432564 - samples/sec: 4070.65\n",
            "2019-12-23 09:36:28,579 epoch 72 - iter 28/280 - loss 0.46602568 - samples/sec: 149.51\n",
            "2019-12-23 09:36:34,630 epoch 72 - iter 56/280 - loss 0.46772834 - samples/sec: 148.57\n",
            "2019-12-23 09:36:40,522 epoch 72 - iter 84/280 - loss 0.44761758 - samples/sec: 152.59\n",
            "2019-12-23 09:36:46,531 epoch 72 - iter 112/280 - loss 0.46006298 - samples/sec: 149.67\n",
            "2019-12-23 09:36:52,570 epoch 72 - iter 140/280 - loss 0.45506939 - samples/sec: 148.90\n",
            "2019-12-23 09:36:58,532 epoch 72 - iter 168/280 - loss 0.46628855 - samples/sec: 150.82\n",
            "2019-12-23 09:37:04,782 epoch 72 - iter 196/280 - loss 0.46769317 - samples/sec: 143.81\n",
            "2019-12-23 09:37:10,791 epoch 72 - iter 224/280 - loss 0.46663146 - samples/sec: 149.62\n",
            "2019-12-23 09:37:16,859 epoch 72 - iter 252/280 - loss 0.46822606 - samples/sec: 148.29\n",
            "2019-12-23 09:37:22,286 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:37:22,287 EPOCH 72 done: loss 0.4663 - lr 0.0500\n",
            "2019-12-23 09:37:22,289 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:37:22,290 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:37:22,487 epoch 73 - iter 0/280 - loss 0.37931228 - samples/sec: 4681.81\n",
            "2019-12-23 09:37:28,358 epoch 73 - iter 28/280 - loss 0.44543234 - samples/sec: 153.16\n",
            "2019-12-23 09:37:34,596 epoch 73 - iter 56/280 - loss 0.43599130 - samples/sec: 144.08\n",
            "2019-12-23 09:37:40,806 epoch 73 - iter 84/280 - loss 0.44666430 - samples/sec: 144.71\n",
            "2019-12-23 09:37:46,880 epoch 73 - iter 112/280 - loss 0.43506245 - samples/sec: 148.00\n",
            "2019-12-23 09:37:52,851 epoch 73 - iter 140/280 - loss 0.43165316 - samples/sec: 150.61\n",
            "2019-12-23 09:37:58,710 epoch 73 - iter 168/280 - loss 0.42849869 - samples/sec: 153.44\n",
            "2019-12-23 09:38:04,955 epoch 73 - iter 196/280 - loss 0.43348542 - samples/sec: 143.92\n",
            "2019-12-23 09:38:11,080 epoch 73 - iter 224/280 - loss 0.43992593 - samples/sec: 146.77\n",
            "2019-12-23 09:38:16,954 epoch 73 - iter 252/280 - loss 0.43913295 - samples/sec: 153.07\n",
            "2019-12-23 09:38:22,565 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:38:22,566 EPOCH 73 done: loss 0.4359 - lr 0.0500\n",
            "2019-12-23 09:38:22,567 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:38:22,569 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:38:22,774 epoch 74 - iter 0/280 - loss 0.34099996 - samples/sec: 4446.66\n",
            "2019-12-23 09:38:28,570 epoch 74 - iter 28/280 - loss 0.54523479 - samples/sec: 155.06\n",
            "2019-12-23 09:38:34,819 epoch 74 - iter 56/280 - loss 0.49432214 - samples/sec: 143.83\n",
            "2019-12-23 09:38:41,048 epoch 74 - iter 84/280 - loss 0.50210677 - samples/sec: 144.32\n",
            "2019-12-23 09:38:47,019 epoch 74 - iter 112/280 - loss 0.48798307 - samples/sec: 150.60\n",
            "2019-12-23 09:38:52,867 epoch 74 - iter 140/280 - loss 0.48538699 - samples/sec: 153.72\n",
            "2019-12-23 09:38:58,750 epoch 74 - iter 168/280 - loss 0.47470787 - samples/sec: 152.82\n",
            "2019-12-23 09:39:04,898 epoch 74 - iter 196/280 - loss 0.46808502 - samples/sec: 146.30\n",
            "2019-12-23 09:39:11,031 epoch 74 - iter 224/280 - loss 0.46636109 - samples/sec: 146.66\n",
            "2019-12-23 09:39:17,123 epoch 74 - iter 252/280 - loss 0.46277720 - samples/sec: 147.60\n",
            "2019-12-23 09:39:22,708 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:39:22,709 EPOCH 74 done: loss 0.4649 - lr 0.0500\n",
            "2019-12-23 09:39:22,712 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:39:22,717 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:39:22,921 epoch 75 - iter 0/280 - loss 0.56299818 - samples/sec: 4473.67\n",
            "2019-12-23 09:39:28,969 epoch 75 - iter 28/280 - loss 0.45838962 - samples/sec: 148.57\n",
            "2019-12-23 09:39:35,039 epoch 75 - iter 56/280 - loss 0.46003960 - samples/sec: 148.10\n",
            "2019-12-23 09:39:41,022 epoch 75 - iter 84/280 - loss 0.44365846 - samples/sec: 150.33\n",
            "2019-12-23 09:39:47,228 epoch 75 - iter 112/280 - loss 0.44841830 - samples/sec: 144.89\n",
            "2019-12-23 09:39:53,153 epoch 75 - iter 140/280 - loss 0.43815768 - samples/sec: 151.79\n",
            "2019-12-23 09:39:59,123 epoch 75 - iter 168/280 - loss 0.43978327 - samples/sec: 150.62\n",
            "2019-12-23 09:40:05,020 epoch 75 - iter 196/280 - loss 0.43963057 - samples/sec: 152.44\n",
            "2019-12-23 09:40:11,002 epoch 75 - iter 224/280 - loss 0.44549081 - samples/sec: 150.32\n",
            "2019-12-23 09:40:17,075 epoch 75 - iter 252/280 - loss 0.44572123 - samples/sec: 148.04\n",
            "2019-12-23 09:40:22,761 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:40:22,765 EPOCH 75 done: loss 0.4411 - lr 0.0500\n",
            "2019-12-23 09:40:22,767 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:40:22,774 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:40:22,999 epoch 76 - iter 0/280 - loss 0.94705653 - samples/sec: 4042.79\n",
            "2019-12-23 09:40:29,211 epoch 76 - iter 28/280 - loss 0.46997530 - samples/sec: 144.70\n",
            "2019-12-23 09:40:35,459 epoch 76 - iter 56/280 - loss 0.47936374 - samples/sec: 143.95\n",
            "2019-12-23 09:40:41,639 epoch 76 - iter 84/280 - loss 0.48808641 - samples/sec: 145.53\n",
            "2019-12-23 09:40:47,705 epoch 76 - iter 112/280 - loss 0.46537469 - samples/sec: 148.23\n",
            "2019-12-23 09:40:53,401 epoch 76 - iter 140/280 - loss 0.46292899 - samples/sec: 157.84\n",
            "2019-12-23 09:40:59,377 epoch 76 - iter 168/280 - loss 0.45359412 - samples/sec: 150.46\n",
            "2019-12-23 09:41:05,197 epoch 76 - iter 196/280 - loss 0.45690197 - samples/sec: 154.52\n",
            "2019-12-23 09:41:11,472 epoch 76 - iter 224/280 - loss 0.44644150 - samples/sec: 143.25\n",
            "2019-12-23 09:41:17,493 epoch 76 - iter 252/280 - loss 0.44390221 - samples/sec: 149.33\n",
            "2019-12-23 09:41:23,178 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:41:23,181 EPOCH 76 done: loss 0.4435 - lr 0.0500\n",
            "2019-12-23 09:41:23,182 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 09:41:23,183 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:41:23,387 epoch 77 - iter 0/280 - loss 0.43972659 - samples/sec: 4545.46\n",
            "2019-12-23 09:41:29,549 epoch 77 - iter 28/280 - loss 0.43514393 - samples/sec: 145.93\n",
            "2019-12-23 09:41:35,452 epoch 77 - iter 56/280 - loss 0.44971538 - samples/sec: 152.32\n",
            "2019-12-23 09:41:41,469 epoch 77 - iter 84/280 - loss 0.45100487 - samples/sec: 149.45\n",
            "2019-12-23 09:41:47,804 epoch 77 - iter 112/280 - loss 0.44230164 - samples/sec: 142.04\n",
            "2019-12-23 09:41:53,741 epoch 77 - iter 140/280 - loss 0.44225016 - samples/sec: 151.48\n",
            "2019-12-23 09:41:59,765 epoch 77 - iter 168/280 - loss 0.44668590 - samples/sec: 149.24\n",
            "2019-12-23 09:42:05,703 epoch 77 - iter 196/280 - loss 0.44316177 - samples/sec: 151.45\n",
            "2019-12-23 09:42:11,647 epoch 77 - iter 224/280 - loss 0.44329668 - samples/sec: 151.25\n",
            "2019-12-23 09:42:17,899 epoch 77 - iter 252/280 - loss 0.44116799 - samples/sec: 143.79\n",
            "2019-12-23 09:42:23,741 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:42:23,743 EPOCH 77 done: loss 0.4395 - lr 0.0500\n",
            "Epoch    76: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2019-12-23 09:42:23,749 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 09:42:23,752 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:42:23,989 epoch 78 - iter 0/280 - loss 0.47365999 - samples/sec: 3829.67\n",
            "2019-12-23 09:42:30,212 epoch 78 - iter 28/280 - loss 0.47811738 - samples/sec: 144.39\n",
            "2019-12-23 09:42:36,333 epoch 78 - iter 56/280 - loss 0.46959274 - samples/sec: 146.88\n",
            "2019-12-23 09:42:42,611 epoch 78 - iter 84/280 - loss 0.45900373 - samples/sec: 143.25\n",
            "2019-12-23 09:42:48,544 epoch 78 - iter 112/280 - loss 0.43947604 - samples/sec: 151.77\n",
            "2019-12-23 09:42:54,450 epoch 78 - iter 140/280 - loss 0.43057101 - samples/sec: 152.19\n",
            "2019-12-23 09:43:00,478 epoch 78 - iter 168/280 - loss 0.43958902 - samples/sec: 149.21\n",
            "2019-12-23 09:43:06,386 epoch 78 - iter 196/280 - loss 0.43601640 - samples/sec: 152.18\n",
            "2019-12-23 09:43:12,718 epoch 78 - iter 224/280 - loss 0.43382622 - samples/sec: 141.94\n",
            "2019-12-23 09:43:18,888 epoch 78 - iter 252/280 - loss 0.43147829 - samples/sec: 145.73\n",
            "2019-12-23 09:43:24,309 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:43:24,310 EPOCH 78 done: loss 0.4275 - lr 0.0250\n",
            "2019-12-23 09:43:24,311 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:43:24,313 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:43:24,554 epoch 79 - iter 0/280 - loss 0.82394433 - samples/sec: 3750.34\n",
            "2019-12-23 09:43:30,563 epoch 79 - iter 28/280 - loss 0.40602225 - samples/sec: 149.56\n",
            "2019-12-23 09:43:36,564 epoch 79 - iter 56/280 - loss 0.39919837 - samples/sec: 149.81\n",
            "2019-12-23 09:43:42,574 epoch 79 - iter 84/280 - loss 0.40429878 - samples/sec: 149.66\n",
            "2019-12-23 09:43:48,730 epoch 79 - iter 112/280 - loss 0.40137223 - samples/sec: 146.07\n",
            "2019-12-23 09:43:54,859 epoch 79 - iter 140/280 - loss 0.39371291 - samples/sec: 146.69\n",
            "2019-12-23 09:44:00,955 epoch 79 - iter 168/280 - loss 0.39808623 - samples/sec: 147.55\n",
            "2019-12-23 09:44:06,710 epoch 79 - iter 196/280 - loss 0.40978985 - samples/sec: 156.27\n",
            "2019-12-23 09:44:12,788 epoch 79 - iter 224/280 - loss 0.41222417 - samples/sec: 147.91\n",
            "2019-12-23 09:44:18,643 epoch 79 - iter 252/280 - loss 0.40954473 - samples/sec: 153.57\n",
            "2019-12-23 09:44:24,464 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:44:24,466 EPOCH 79 done: loss 0.4127 - lr 0.0250\n",
            "2019-12-23 09:44:24,473 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:44:24,474 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:44:24,692 epoch 80 - iter 0/280 - loss 0.23716378 - samples/sec: 4162.89\n",
            "2019-12-23 09:44:30,792 epoch 80 - iter 28/280 - loss 0.40045735 - samples/sec: 147.36\n",
            "2019-12-23 09:44:36,699 epoch 80 - iter 56/280 - loss 0.37747679 - samples/sec: 152.23\n",
            "2019-12-23 09:44:42,879 epoch 80 - iter 84/280 - loss 0.39535954 - samples/sec: 145.47\n",
            "2019-12-23 09:44:48,968 epoch 80 - iter 112/280 - loss 0.39713581 - samples/sec: 147.89\n",
            "2019-12-23 09:44:54,791 epoch 80 - iter 140/280 - loss 0.38742282 - samples/sec: 154.38\n",
            "2019-12-23 09:45:00,893 epoch 80 - iter 168/280 - loss 0.38858261 - samples/sec: 147.32\n",
            "2019-12-23 09:45:07,160 epoch 80 - iter 196/280 - loss 0.38956637 - samples/sec: 143.48\n",
            "2019-12-23 09:45:13,292 epoch 80 - iter 224/280 - loss 0.38878282 - samples/sec: 146.57\n",
            "2019-12-23 09:45:19,101 epoch 80 - iter 252/280 - loss 0.38767598 - samples/sec: 154.78\n",
            "2019-12-23 09:45:24,779 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:45:24,780 EPOCH 80 done: loss 0.3852 - lr 0.0250\n",
            "2019-12-23 09:45:24,784 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:45:24,785 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:45:25,024 epoch 81 - iter 0/280 - loss 0.82961249 - samples/sec: 3860.42\n",
            "2019-12-23 09:45:31,147 epoch 81 - iter 28/280 - loss 0.34719855 - samples/sec: 146.80\n",
            "2019-12-23 09:45:37,059 epoch 81 - iter 56/280 - loss 0.37637120 - samples/sec: 152.18\n",
            "2019-12-23 09:45:43,008 epoch 81 - iter 84/280 - loss 0.36598949 - samples/sec: 151.14\n",
            "2019-12-23 09:45:49,108 epoch 81 - iter 112/280 - loss 0.37318160 - samples/sec: 147.40\n",
            "2019-12-23 09:45:55,026 epoch 81 - iter 140/280 - loss 0.37588604 - samples/sec: 151.90\n",
            "2019-12-23 09:46:00,863 epoch 81 - iter 168/280 - loss 0.38871473 - samples/sec: 154.05\n",
            "2019-12-23 09:46:06,842 epoch 81 - iter 196/280 - loss 0.38119570 - samples/sec: 150.39\n",
            "2019-12-23 09:46:12,894 epoch 81 - iter 224/280 - loss 0.38241494 - samples/sec: 148.52\n",
            "2019-12-23 09:46:18,962 epoch 81 - iter 252/280 - loss 0.39114492 - samples/sec: 148.12\n",
            "2019-12-23 09:46:24,635 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:46:24,636 EPOCH 81 done: loss 0.3913 - lr 0.0250\n",
            "2019-12-23 09:46:24,640 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:46:24,648 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:46:24,840 epoch 82 - iter 0/280 - loss 0.54216802 - samples/sec: 4740.06\n",
            "2019-12-23 09:46:31,081 epoch 82 - iter 28/280 - loss 0.40016726 - samples/sec: 144.01\n",
            "2019-12-23 09:46:37,233 epoch 82 - iter 56/280 - loss 0.40632651 - samples/sec: 146.15\n",
            "2019-12-23 09:46:43,455 epoch 82 - iter 84/280 - loss 0.41003040 - samples/sec: 144.53\n",
            "2019-12-23 09:46:49,476 epoch 82 - iter 112/280 - loss 0.40959554 - samples/sec: 149.28\n",
            "2019-12-23 09:46:56,807 epoch 82 - iter 140/280 - loss 0.41132368 - samples/sec: 122.57\n",
            "2019-12-23 09:47:02,696 epoch 82 - iter 168/280 - loss 0.40364865 - samples/sec: 152.68\n",
            "2019-12-23 09:47:08,720 epoch 82 - iter 196/280 - loss 0.40453593 - samples/sec: 149.32\n",
            "2019-12-23 09:47:14,707 epoch 82 - iter 224/280 - loss 0.40517142 - samples/sec: 150.21\n",
            "2019-12-23 09:47:20,529 epoch 82 - iter 252/280 - loss 0.39761221 - samples/sec: 154.46\n",
            "2019-12-23 09:47:26,243 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:47:26,246 EPOCH 82 done: loss 0.3969 - lr 0.0250\n",
            "2019-12-23 09:47:26,249 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:47:26,251 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:47:26,500 epoch 83 - iter 0/280 - loss 0.54150635 - samples/sec: 3629.93\n",
            "2019-12-23 09:47:32,469 epoch 83 - iter 28/280 - loss 0.41534603 - samples/sec: 150.58\n",
            "2019-12-23 09:47:38,730 epoch 83 - iter 56/280 - loss 0.39257065 - samples/sec: 143.66\n",
            "2019-12-23 09:47:44,731 epoch 83 - iter 84/280 - loss 0.39772026 - samples/sec: 149.81\n",
            "2019-12-23 09:47:50,867 epoch 83 - iter 112/280 - loss 0.39730110 - samples/sec: 146.52\n",
            "2019-12-23 09:47:57,003 epoch 83 - iter 140/280 - loss 0.40120882 - samples/sec: 146.50\n",
            "2019-12-23 09:48:02,902 epoch 83 - iter 168/280 - loss 0.39908871 - samples/sec: 152.43\n",
            "2019-12-23 09:48:08,853 epoch 83 - iter 196/280 - loss 0.39066471 - samples/sec: 151.12\n",
            "2019-12-23 09:48:14,723 epoch 83 - iter 224/280 - loss 0.39881833 - samples/sec: 153.13\n",
            "2019-12-23 09:48:20,724 epoch 83 - iter 252/280 - loss 0.39668655 - samples/sec: 149.88\n",
            "2019-12-23 09:48:26,619 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:48:26,620 EPOCH 83 done: loss 0.3954 - lr 0.0250\n",
            "2019-12-23 09:48:26,622 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 09:48:26,623 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:48:26,857 epoch 84 - iter 0/280 - loss 0.41335177 - samples/sec: 3864.17\n",
            "2019-12-23 09:48:33,057 epoch 84 - iter 28/280 - loss 0.43684315 - samples/sec: 144.94\n",
            "2019-12-23 09:48:39,042 epoch 84 - iter 56/280 - loss 0.41065606 - samples/sec: 150.26\n",
            "2019-12-23 09:48:45,054 epoch 84 - iter 84/280 - loss 0.40095521 - samples/sec: 149.56\n",
            "2019-12-23 09:48:51,147 epoch 84 - iter 112/280 - loss 0.39701148 - samples/sec: 147.56\n",
            "2019-12-23 09:48:56,947 epoch 84 - iter 140/280 - loss 0.38747843 - samples/sec: 154.99\n",
            "2019-12-23 09:49:03,079 epoch 84 - iter 168/280 - loss 0.39493062 - samples/sec: 146.56\n",
            "2019-12-23 09:49:08,980 epoch 84 - iter 196/280 - loss 0.39517202 - samples/sec: 152.44\n",
            "2019-12-23 09:49:15,111 epoch 84 - iter 224/280 - loss 0.39420497 - samples/sec: 146.59\n",
            "2019-12-23 09:49:21,174 epoch 84 - iter 252/280 - loss 0.40143481 - samples/sec: 148.28\n",
            "2019-12-23 09:49:26,773 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:49:26,774 EPOCH 84 done: loss 0.4051 - lr 0.0250\n",
            "Epoch    83: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2019-12-23 09:49:26,776 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 09:49:26,777 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:49:26,981 epoch 85 - iter 0/280 - loss 0.16125238 - samples/sec: 4448.97\n",
            "2019-12-23 09:49:32,884 epoch 85 - iter 28/280 - loss 0.44932393 - samples/sec: 152.22\n",
            "2019-12-23 09:49:38,904 epoch 85 - iter 56/280 - loss 0.40709968 - samples/sec: 149.45\n",
            "2019-12-23 09:49:44,910 epoch 85 - iter 84/280 - loss 0.39880120 - samples/sec: 149.69\n",
            "2019-12-23 09:49:51,034 epoch 85 - iter 112/280 - loss 0.38487977 - samples/sec: 146.80\n",
            "2019-12-23 09:49:57,031 epoch 85 - iter 140/280 - loss 0.37749597 - samples/sec: 149.87\n",
            "2019-12-23 09:50:03,027 epoch 85 - iter 168/280 - loss 0.38861825 - samples/sec: 150.01\n",
            "2019-12-23 09:50:09,150 epoch 85 - iter 196/280 - loss 0.39089967 - samples/sec: 146.96\n",
            "2019-12-23 09:50:15,008 epoch 85 - iter 224/280 - loss 0.39206376 - samples/sec: 153.45\n",
            "2019-12-23 09:50:21,203 epoch 85 - iter 252/280 - loss 0.39500449 - samples/sec: 145.08\n",
            "2019-12-23 09:50:27,028 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:50:27,031 EPOCH 85 done: loss 0.3923 - lr 0.0125\n",
            "2019-12-23 09:50:27,032 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:50:27,033 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:50:27,254 epoch 86 - iter 0/280 - loss 0.36276823 - samples/sec: 4092.15\n",
            "2019-12-23 09:50:33,404 epoch 86 - iter 28/280 - loss 0.41377225 - samples/sec: 146.12\n",
            "2019-12-23 09:50:39,483 epoch 86 - iter 56/280 - loss 0.38200565 - samples/sec: 147.90\n",
            "2019-12-23 09:50:45,542 epoch 86 - iter 84/280 - loss 0.36560718 - samples/sec: 148.40\n",
            "2019-12-23 09:50:51,476 epoch 86 - iter 112/280 - loss 0.36862042 - samples/sec: 151.48\n",
            "2019-12-23 09:50:57,443 epoch 86 - iter 140/280 - loss 0.37617235 - samples/sec: 150.63\n",
            "2019-12-23 09:51:03,599 epoch 86 - iter 168/280 - loss 0.37176092 - samples/sec: 146.03\n",
            "2019-12-23 09:51:09,795 epoch 86 - iter 196/280 - loss 0.37838602 - samples/sec: 145.12\n",
            "2019-12-23 09:51:15,765 epoch 86 - iter 224/280 - loss 0.38294192 - samples/sec: 150.55\n",
            "2019-12-23 09:51:21,671 epoch 86 - iter 252/280 - loss 0.38228716 - samples/sec: 152.20\n",
            "2019-12-23 09:51:27,196 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:51:27,197 EPOCH 86 done: loss 0.3897 - lr 0.0125\n",
            "2019-12-23 09:51:27,201 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:51:27,203 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:51:27,393 epoch 87 - iter 0/280 - loss 0.14029431 - samples/sec: 4784.34\n",
            "2019-12-23 09:51:33,542 epoch 87 - iter 28/280 - loss 0.37391307 - samples/sec: 146.16\n",
            "2019-12-23 09:51:39,717 epoch 87 - iter 56/280 - loss 0.39624716 - samples/sec: 145.61\n",
            "2019-12-23 09:51:45,759 epoch 87 - iter 84/280 - loss 0.38204883 - samples/sec: 148.89\n",
            "2019-12-23 09:51:51,689 epoch 87 - iter 112/280 - loss 0.37161554 - samples/sec: 151.61\n",
            "2019-12-23 09:51:57,827 epoch 87 - iter 140/280 - loss 0.37620465 - samples/sec: 146.49\n",
            "2019-12-23 09:52:03,532 epoch 87 - iter 168/280 - loss 0.37087287 - samples/sec: 157.71\n",
            "2019-12-23 09:52:09,537 epoch 87 - iter 196/280 - loss 0.37381413 - samples/sec: 149.74\n",
            "2019-12-23 09:52:15,672 epoch 87 - iter 224/280 - loss 0.37931601 - samples/sec: 146.55\n",
            "2019-12-23 09:52:21,717 epoch 87 - iter 252/280 - loss 0.37574854 - samples/sec: 148.81\n",
            "2019-12-23 09:52:27,582 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:52:27,584 EPOCH 87 done: loss 0.3786 - lr 0.0125\n",
            "2019-12-23 09:52:27,588 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:52:27,589 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:52:27,778 epoch 88 - iter 0/280 - loss 0.67138195 - samples/sec: 4802.88\n",
            "2019-12-23 09:52:34,076 epoch 88 - iter 28/280 - loss 0.35147079 - samples/sec: 142.69\n",
            "2019-12-23 09:52:40,330 epoch 88 - iter 56/280 - loss 0.34365944 - samples/sec: 143.76\n",
            "2019-12-23 09:52:46,544 epoch 88 - iter 84/280 - loss 0.35679649 - samples/sec: 144.68\n",
            "2019-12-23 09:52:52,648 epoch 88 - iter 112/280 - loss 0.35381360 - samples/sec: 147.28\n",
            "2019-12-23 09:52:58,597 epoch 88 - iter 140/280 - loss 0.34140344 - samples/sec: 151.27\n",
            "2019-12-23 09:53:04,520 epoch 88 - iter 168/280 - loss 0.34339342 - samples/sec: 151.78\n",
            "2019-12-23 09:53:10,556 epoch 88 - iter 196/280 - loss 0.34378901 - samples/sec: 148.98\n",
            "2019-12-23 09:53:16,590 epoch 88 - iter 224/280 - loss 0.35184520 - samples/sec: 148.97\n",
            "2019-12-23 09:53:22,522 epoch 88 - iter 252/280 - loss 0.34979712 - samples/sec: 151.59\n",
            "2019-12-23 09:53:28,249 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:53:28,250 EPOCH 88 done: loss 0.3560 - lr 0.0125\n",
            "2019-12-23 09:53:28,254 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 09:53:28,258 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:53:28,462 epoch 89 - iter 0/280 - loss 0.20951092 - samples/sec: 4441.97\n",
            "2019-12-23 09:53:34,471 epoch 89 - iter 28/280 - loss 0.36271866 - samples/sec: 149.55\n",
            "2019-12-23 09:53:40,437 epoch 89 - iter 56/280 - loss 0.35977105 - samples/sec: 150.70\n",
            "2019-12-23 09:53:46,700 epoch 89 - iter 84/280 - loss 0.36105079 - samples/sec: 143.50\n",
            "2019-12-23 09:53:52,999 epoch 89 - iter 112/280 - loss 0.37812187 - samples/sec: 142.82\n",
            "2019-12-23 09:53:58,942 epoch 89 - iter 140/280 - loss 0.37380939 - samples/sec: 151.50\n",
            "2019-12-23 09:54:04,834 epoch 89 - iter 168/280 - loss 0.36779932 - samples/sec: 152.66\n",
            "2019-12-23 09:54:10,998 epoch 89 - iter 196/280 - loss 0.37184377 - samples/sec: 145.89\n",
            "2019-12-23 09:54:17,328 epoch 89 - iter 224/280 - loss 0.37283342 - samples/sec: 142.08\n",
            "2019-12-23 09:54:23,271 epoch 89 - iter 252/280 - loss 0.37346279 - samples/sec: 151.31\n",
            "2019-12-23 09:54:28,878 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:54:28,879 EPOCH 89 done: loss 0.3724 - lr 0.0125\n",
            "2019-12-23 09:54:28,882 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:54:28,885 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:54:29,084 epoch 90 - iter 0/280 - loss 0.35368013 - samples/sec: 4573.08\n",
            "2019-12-23 09:54:35,236 epoch 90 - iter 28/280 - loss 0.32135742 - samples/sec: 146.06\n",
            "2019-12-23 09:54:40,940 epoch 90 - iter 56/280 - loss 0.35516455 - samples/sec: 157.66\n",
            "2019-12-23 09:54:46,881 epoch 90 - iter 84/280 - loss 0.35327134 - samples/sec: 151.34\n",
            "2019-12-23 09:54:53,054 epoch 90 - iter 112/280 - loss 0.36735328 - samples/sec: 145.65\n",
            "2019-12-23 09:54:59,007 epoch 90 - iter 140/280 - loss 0.36347223 - samples/sec: 151.06\n",
            "2019-12-23 09:55:05,181 epoch 90 - iter 168/280 - loss 0.36796330 - samples/sec: 145.65\n",
            "2019-12-23 09:55:11,275 epoch 90 - iter 196/280 - loss 0.37037084 - samples/sec: 147.54\n",
            "2019-12-23 09:55:17,239 epoch 90 - iter 224/280 - loss 0.36378939 - samples/sec: 150.84\n",
            "2019-12-23 09:55:23,142 epoch 90 - iter 252/280 - loss 0.36141187 - samples/sec: 152.27\n",
            "2019-12-23 09:55:29,036 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:55:29,037 EPOCH 90 done: loss 0.3592 - lr 0.0125\n",
            "2019-12-23 09:55:29,038 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:55:29,039 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:55:29,278 epoch 91 - iter 0/280 - loss 0.47819924 - samples/sec: 3778.74\n",
            "2019-12-23 09:55:35,399 epoch 91 - iter 28/280 - loss 0.28438671 - samples/sec: 146.83\n",
            "2019-12-23 09:55:41,508 epoch 91 - iter 56/280 - loss 0.34136823 - samples/sec: 147.21\n",
            "2019-12-23 09:55:47,536 epoch 91 - iter 84/280 - loss 0.36112754 - samples/sec: 149.13\n",
            "2019-12-23 09:55:53,505 epoch 91 - iter 112/280 - loss 0.37343631 - samples/sec: 150.57\n",
            "2019-12-23 09:55:59,492 epoch 91 - iter 140/280 - loss 0.37272920 - samples/sec: 150.16\n",
            "2019-12-23 09:56:05,468 epoch 91 - iter 168/280 - loss 0.37775814 - samples/sec: 150.49\n",
            "2019-12-23 09:56:11,493 epoch 91 - iter 196/280 - loss 0.37846593 - samples/sec: 149.21\n",
            "2019-12-23 09:56:17,427 epoch 91 - iter 224/280 - loss 0.37776512 - samples/sec: 151.53\n",
            "2019-12-23 09:56:23,385 epoch 91 - iter 252/280 - loss 0.38324858 - samples/sec: 150.91\n",
            "2019-12-23 09:56:29,078 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:56:29,079 EPOCH 91 done: loss 0.3793 - lr 0.0125\n",
            "2019-12-23 09:56:29,081 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 09:56:29,082 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:56:29,300 epoch 92 - iter 0/280 - loss 0.36576402 - samples/sec: 4138.89\n",
            "2019-12-23 09:56:35,404 epoch 92 - iter 28/280 - loss 0.40382648 - samples/sec: 147.24\n",
            "2019-12-23 09:56:41,471 epoch 92 - iter 56/280 - loss 0.37996720 - samples/sec: 148.15\n",
            "2019-12-23 09:56:47,409 epoch 92 - iter 84/280 - loss 0.35663867 - samples/sec: 151.37\n",
            "2019-12-23 09:56:53,460 epoch 92 - iter 112/280 - loss 0.37145361 - samples/sec: 148.64\n",
            "2019-12-23 09:56:59,584 epoch 92 - iter 140/280 - loss 0.37318239 - samples/sec: 146.83\n",
            "2019-12-23 09:57:05,619 epoch 92 - iter 168/280 - loss 0.37426273 - samples/sec: 148.95\n",
            "2019-12-23 09:57:11,555 epoch 92 - iter 196/280 - loss 0.36770673 - samples/sec: 151.43\n",
            "2019-12-23 09:57:17,481 epoch 92 - iter 224/280 - loss 0.36740707 - samples/sec: 151.70\n",
            "2019-12-23 09:57:23,524 epoch 92 - iter 252/280 - loss 0.36468150 - samples/sec: 148.74\n",
            "2019-12-23 09:57:29,414 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:57:29,417 EPOCH 92 done: loss 0.3658 - lr 0.0125\n",
            "Epoch    91: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2019-12-23 09:57:29,419 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 09:57:29,420 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:57:29,677 epoch 93 - iter 0/280 - loss 0.23450828 - samples/sec: 3516.73\n",
            "2019-12-23 09:57:35,499 epoch 93 - iter 28/280 - loss 0.32396511 - samples/sec: 154.38\n",
            "2019-12-23 09:57:41,568 epoch 93 - iter 56/280 - loss 0.38159065 - samples/sec: 148.15\n",
            "2019-12-23 09:57:47,729 epoch 93 - iter 84/280 - loss 0.39472678 - samples/sec: 146.04\n",
            "2019-12-23 09:57:53,678 epoch 93 - iter 112/280 - loss 0.38817288 - samples/sec: 151.28\n",
            "2019-12-23 09:57:59,757 epoch 93 - iter 140/280 - loss 0.39256746 - samples/sec: 147.86\n",
            "2019-12-23 09:58:05,753 epoch 93 - iter 168/280 - loss 0.39258547 - samples/sec: 149.94\n",
            "2019-12-23 09:58:11,768 epoch 93 - iter 196/280 - loss 0.39223367 - samples/sec: 149.49\n",
            "2019-12-23 09:58:19,065 epoch 93 - iter 224/280 - loss 0.39111585 - samples/sec: 123.13\n",
            "2019-12-23 09:58:25,080 epoch 93 - iter 252/280 - loss 0.39282130 - samples/sec: 149.47\n",
            "2019-12-23 09:58:30,647 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:58:30,649 EPOCH 93 done: loss 0.3896 - lr 0.0063\n",
            "2019-12-23 09:58:30,650 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 09:58:30,651 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:58:30,851 epoch 94 - iter 0/280 - loss 0.35045862 - samples/sec: 4532.14\n",
            "2019-12-23 09:58:36,787 epoch 94 - iter 28/280 - loss 0.32266901 - samples/sec: 151.44\n",
            "2019-12-23 09:58:42,773 epoch 94 - iter 56/280 - loss 0.33822053 - samples/sec: 150.21\n",
            "2019-12-23 09:58:48,811 epoch 94 - iter 84/280 - loss 0.34709315 - samples/sec: 148.93\n",
            "2019-12-23 09:58:54,852 epoch 94 - iter 112/280 - loss 0.36255373 - samples/sec: 148.81\n",
            "2019-12-23 09:59:00,733 epoch 94 - iter 140/280 - loss 0.37507285 - samples/sec: 152.84\n",
            "2019-12-23 09:59:06,654 epoch 94 - iter 168/280 - loss 0.36499502 - samples/sec: 151.86\n",
            "2019-12-23 09:59:12,702 epoch 94 - iter 196/280 - loss 0.36540230 - samples/sec: 148.67\n",
            "2019-12-23 09:59:19,054 epoch 94 - iter 224/280 - loss 0.37182586 - samples/sec: 141.54\n",
            "2019-12-23 09:59:24,995 epoch 94 - iter 252/280 - loss 0.37521169 - samples/sec: 151.30\n",
            "2019-12-23 09:59:30,669 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:59:30,670 EPOCH 94 done: loss 0.3784 - lr 0.0063\n",
            "2019-12-23 09:59:30,671 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 09:59:30,673 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 09:59:30,857 epoch 95 - iter 0/280 - loss 0.82636440 - samples/sec: 4923.43\n",
            "2019-12-23 09:59:36,962 epoch 95 - iter 28/280 - loss 0.38092757 - samples/sec: 147.20\n",
            "2019-12-23 09:59:43,315 epoch 95 - iter 56/280 - loss 0.37319907 - samples/sec: 141.52\n",
            "2019-12-23 09:59:49,389 epoch 95 - iter 84/280 - loss 0.36815218 - samples/sec: 148.06\n",
            "2019-12-23 09:59:55,341 epoch 95 - iter 112/280 - loss 0.36580134 - samples/sec: 151.07\n",
            "2019-12-23 10:00:01,381 epoch 95 - iter 140/280 - loss 0.36866009 - samples/sec: 148.91\n",
            "2019-12-23 10:00:07,444 epoch 95 - iter 168/280 - loss 0.36694673 - samples/sec: 148.32\n",
            "2019-12-23 10:00:13,580 epoch 95 - iter 196/280 - loss 0.36999146 - samples/sec: 146.55\n",
            "2019-12-23 10:00:19,343 epoch 95 - iter 224/280 - loss 0.37638410 - samples/sec: 156.04\n",
            "2019-12-23 10:00:25,272 epoch 95 - iter 252/280 - loss 0.37411076 - samples/sec: 151.62\n",
            "2019-12-23 10:00:30,806 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:00:30,807 EPOCH 95 done: loss 0.3713 - lr 0.0063\n",
            "2019-12-23 10:00:30,811 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:00:30,812 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:00:31,053 epoch 96 - iter 0/280 - loss 0.20707798 - samples/sec: 3760.07\n",
            "2019-12-23 10:00:37,242 epoch 96 - iter 28/280 - loss 0.41865546 - samples/sec: 145.25\n",
            "2019-12-23 10:00:43,105 epoch 96 - iter 56/280 - loss 0.40334628 - samples/sec: 153.45\n",
            "2019-12-23 10:00:49,230 epoch 96 - iter 84/280 - loss 0.39062204 - samples/sec: 146.80\n",
            "2019-12-23 10:00:55,305 epoch 96 - iter 112/280 - loss 0.37466232 - samples/sec: 148.00\n",
            "2019-12-23 10:01:01,181 epoch 96 - iter 140/280 - loss 0.37701749 - samples/sec: 153.05\n",
            "2019-12-23 10:01:07,032 epoch 96 - iter 168/280 - loss 0.37100934 - samples/sec: 153.74\n",
            "2019-12-23 10:01:13,136 epoch 96 - iter 196/280 - loss 0.36546597 - samples/sec: 147.31\n",
            "2019-12-23 10:01:19,047 epoch 96 - iter 224/280 - loss 0.36370482 - samples/sec: 152.12\n",
            "2019-12-23 10:01:24,970 epoch 96 - iter 252/280 - loss 0.36619312 - samples/sec: 151.81\n",
            "2019-12-23 10:01:30,595 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:01:30,596 EPOCH 96 done: loss 0.3674 - lr 0.0063\n",
            "Epoch    95: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2019-12-23 10:01:30,602 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:01:30,606 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:01:30,810 epoch 97 - iter 0/280 - loss 0.31790209 - samples/sec: 4419.81\n",
            "2019-12-23 10:01:36,637 epoch 97 - iter 28/280 - loss 0.40934565 - samples/sec: 154.32\n",
            "2019-12-23 10:01:42,651 epoch 97 - iter 56/280 - loss 0.39062421 - samples/sec: 149.53\n",
            "2019-12-23 10:01:48,803 epoch 97 - iter 84/280 - loss 0.38385847 - samples/sec: 146.16\n",
            "2019-12-23 10:01:54,675 epoch 97 - iter 112/280 - loss 0.39720887 - samples/sec: 153.18\n",
            "2019-12-23 10:02:00,764 epoch 97 - iter 140/280 - loss 0.38295251 - samples/sec: 147.65\n",
            "2019-12-23 10:02:06,843 epoch 97 - iter 168/280 - loss 0.37632499 - samples/sec: 147.92\n",
            "2019-12-23 10:02:12,985 epoch 97 - iter 196/280 - loss 0.37688019 - samples/sec: 146.40\n",
            "2019-12-23 10:02:18,884 epoch 97 - iter 224/280 - loss 0.37813946 - samples/sec: 152.38\n",
            "2019-12-23 10:02:24,920 epoch 97 - iter 252/280 - loss 0.37624404 - samples/sec: 148.93\n",
            "2019-12-23 10:02:30,625 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:02:30,627 EPOCH 97 done: loss 0.3740 - lr 0.0031\n",
            "2019-12-23 10:02:30,630 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:02:30,633 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:02:30,848 epoch 98 - iter 0/280 - loss 0.38930190 - samples/sec: 4217.19\n",
            "2019-12-23 10:02:36,855 epoch 98 - iter 28/280 - loss 0.37382785 - samples/sec: 149.64\n",
            "2019-12-23 10:02:42,981 epoch 98 - iter 56/280 - loss 0.38436990 - samples/sec: 146.82\n",
            "2019-12-23 10:02:49,202 epoch 98 - iter 84/280 - loss 0.38523011 - samples/sec: 144.54\n",
            "2019-12-23 10:02:55,153 epoch 98 - iter 112/280 - loss 0.37585289 - samples/sec: 151.10\n",
            "2019-12-23 10:03:00,976 epoch 98 - iter 140/280 - loss 0.37065297 - samples/sec: 154.40\n",
            "2019-12-23 10:03:07,088 epoch 98 - iter 168/280 - loss 0.37799290 - samples/sec: 147.12\n",
            "2019-12-23 10:03:12,905 epoch 98 - iter 196/280 - loss 0.36961804 - samples/sec: 154.56\n",
            "2019-12-23 10:03:18,973 epoch 98 - iter 224/280 - loss 0.36430089 - samples/sec: 148.21\n",
            "2019-12-23 10:03:24,730 epoch 98 - iter 252/280 - loss 0.36271999 - samples/sec: 156.20\n",
            "2019-12-23 10:03:30,686 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:03:30,688 EPOCH 98 done: loss 0.3622 - lr 0.0031\n",
            "2019-12-23 10:03:30,689 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:03:30,691 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:03:30,936 epoch 99 - iter 0/280 - loss 0.42892838 - samples/sec: 3697.38\n",
            "2019-12-23 10:03:36,919 epoch 99 - iter 28/280 - loss 0.35815264 - samples/sec: 150.23\n",
            "2019-12-23 10:03:42,762 epoch 99 - iter 56/280 - loss 0.38530846 - samples/sec: 153.92\n",
            "2019-12-23 10:03:48,929 epoch 99 - iter 84/280 - loss 0.36716765 - samples/sec: 145.79\n",
            "2019-12-23 10:03:54,839 epoch 99 - iter 112/280 - loss 0.36537734 - samples/sec: 152.14\n",
            "2019-12-23 10:04:00,836 epoch 99 - iter 140/280 - loss 0.35817377 - samples/sec: 149.88\n",
            "2019-12-23 10:04:06,768 epoch 99 - iter 168/280 - loss 0.36340771 - samples/sec: 151.63\n",
            "2019-12-23 10:04:12,609 epoch 99 - iter 196/280 - loss 0.36508332 - samples/sec: 154.00\n",
            "2019-12-23 10:04:18,634 epoch 99 - iter 224/280 - loss 0.36603778 - samples/sec: 149.28\n",
            "2019-12-23 10:04:24,653 epoch 99 - iter 252/280 - loss 0.36502525 - samples/sec: 149.43\n",
            "2019-12-23 10:04:30,523 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:04:30,525 EPOCH 99 done: loss 0.3590 - lr 0.0031\n",
            "2019-12-23 10:04:30,526 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:04:30,532 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:04:30,797 epoch 100 - iter 0/280 - loss 0.39756274 - samples/sec: 3413.96\n",
            "2019-12-23 10:04:36,919 epoch 100 - iter 28/280 - loss 0.32220599 - samples/sec: 146.86\n",
            "2019-12-23 10:04:42,948 epoch 100 - iter 56/280 - loss 0.34146151 - samples/sec: 149.17\n",
            "2019-12-23 10:04:49,062 epoch 100 - iter 84/280 - loss 0.34919056 - samples/sec: 147.09\n",
            "2019-12-23 10:04:54,978 epoch 100 - iter 112/280 - loss 0.34065872 - samples/sec: 152.25\n",
            "2019-12-23 10:05:00,946 epoch 100 - iter 140/280 - loss 0.33815749 - samples/sec: 150.70\n",
            "2019-12-23 10:05:06,876 epoch 100 - iter 168/280 - loss 0.34253810 - samples/sec: 151.70\n",
            "2019-12-23 10:05:12,939 epoch 100 - iter 196/280 - loss 0.34259557 - samples/sec: 148.33\n",
            "2019-12-23 10:05:18,782 epoch 100 - iter 224/280 - loss 0.34890231 - samples/sec: 153.91\n",
            "2019-12-23 10:05:24,716 epoch 100 - iter 252/280 - loss 0.35013049 - samples/sec: 151.50\n",
            "2019-12-23 10:05:30,506 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:05:30,507 EPOCH 100 done: loss 0.3470 - lr 0.0031\n",
            "2019-12-23 10:05:30,508 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 10:05:30,509 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:05:30,743 epoch 101 - iter 0/280 - loss 0.31830591 - samples/sec: 3848.77\n",
            "2019-12-23 10:05:36,669 epoch 101 - iter 28/280 - loss 0.29494002 - samples/sec: 151.69\n",
            "2019-12-23 10:05:42,465 epoch 101 - iter 56/280 - loss 0.33305918 - samples/sec: 155.17\n",
            "2019-12-23 10:05:48,575 epoch 101 - iter 84/280 - loss 0.35177513 - samples/sec: 147.14\n",
            "2019-12-23 10:05:54,526 epoch 101 - iter 112/280 - loss 0.34208050 - samples/sec: 151.15\n",
            "2019-12-23 10:06:00,736 epoch 101 - iter 140/280 - loss 0.34445327 - samples/sec: 144.82\n",
            "2019-12-23 10:06:06,781 epoch 101 - iter 168/280 - loss 0.34748228 - samples/sec: 148.74\n",
            "2019-12-23 10:06:12,836 epoch 101 - iter 196/280 - loss 0.34568157 - samples/sec: 148.59\n",
            "2019-12-23 10:06:18,791 epoch 101 - iter 224/280 - loss 0.34680570 - samples/sec: 151.09\n",
            "2019-12-23 10:06:24,892 epoch 101 - iter 252/280 - loss 0.34858684 - samples/sec: 147.39\n",
            "2019-12-23 10:06:30,615 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:06:30,616 EPOCH 101 done: loss 0.3463 - lr 0.0031\n",
            "2019-12-23 10:06:30,624 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 10:06:30,629 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:06:30,830 epoch 102 - iter 0/280 - loss 0.39726305 - samples/sec: 4525.31\n",
            "2019-12-23 10:06:36,802 epoch 102 - iter 28/280 - loss 0.36748464 - samples/sec: 150.52\n",
            "2019-12-23 10:06:42,788 epoch 102 - iter 56/280 - loss 0.35891369 - samples/sec: 150.25\n",
            "2019-12-23 10:06:48,767 epoch 102 - iter 84/280 - loss 0.35060337 - samples/sec: 150.37\n",
            "2019-12-23 10:06:54,832 epoch 102 - iter 112/280 - loss 0.35494949 - samples/sec: 148.41\n",
            "2019-12-23 10:07:00,999 epoch 102 - iter 140/280 - loss 0.34357074 - samples/sec: 145.83\n",
            "2019-12-23 10:07:07,063 epoch 102 - iter 168/280 - loss 0.35300266 - samples/sec: 148.27\n",
            "2019-12-23 10:07:13,062 epoch 102 - iter 196/280 - loss 0.35372444 - samples/sec: 149.92\n",
            "2019-12-23 10:07:19,078 epoch 102 - iter 224/280 - loss 0.35753550 - samples/sec: 149.47\n",
            "2019-12-23 10:07:24,864 epoch 102 - iter 252/280 - loss 0.35612962 - samples/sec: 155.41\n",
            "2019-12-23 10:07:30,806 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:07:30,807 EPOCH 102 done: loss 0.3538 - lr 0.0031\n",
            "2019-12-23 10:07:30,811 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:07:30,812 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:07:31,002 epoch 103 - iter 0/280 - loss 0.33831918 - samples/sec: 4776.31\n",
            "2019-12-23 10:07:37,049 epoch 103 - iter 28/280 - loss 0.37919275 - samples/sec: 148.67\n",
            "2019-12-23 10:07:43,010 epoch 103 - iter 56/280 - loss 0.38102714 - samples/sec: 150.94\n",
            "2019-12-23 10:07:49,110 epoch 103 - iter 84/280 - loss 0.37171880 - samples/sec: 147.39\n",
            "2019-12-23 10:07:54,975 epoch 103 - iter 112/280 - loss 0.35595027 - samples/sec: 153.29\n",
            "2019-12-23 10:08:01,042 epoch 103 - iter 140/280 - loss 0.35663444 - samples/sec: 148.22\n",
            "2019-12-23 10:08:07,023 epoch 103 - iter 168/280 - loss 0.36059901 - samples/sec: 150.37\n",
            "2019-12-23 10:08:13,031 epoch 103 - iter 196/280 - loss 0.35661040 - samples/sec: 149.67\n",
            "2019-12-23 10:08:18,969 epoch 103 - iter 224/280 - loss 0.35957363 - samples/sec: 151.43\n",
            "2019-12-23 10:08:24,995 epoch 103 - iter 252/280 - loss 0.36545479 - samples/sec: 149.17\n",
            "2019-12-23 10:08:30,570 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:08:30,572 EPOCH 103 done: loss 0.3648 - lr 0.0031\n",
            "2019-12-23 10:08:30,573 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:08:30,574 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:08:30,790 epoch 104 - iter 0/280 - loss 0.41346437 - samples/sec: 4188.51\n",
            "2019-12-23 10:08:36,874 epoch 104 - iter 28/280 - loss 0.36277761 - samples/sec: 147.73\n",
            "2019-12-23 10:08:42,805 epoch 104 - iter 56/280 - loss 0.35916362 - samples/sec: 151.68\n",
            "2019-12-23 10:08:48,591 epoch 104 - iter 84/280 - loss 0.34959724 - samples/sec: 155.40\n",
            "2019-12-23 10:08:54,712 epoch 104 - iter 112/280 - loss 0.35391201 - samples/sec: 146.94\n",
            "2019-12-23 10:09:00,871 epoch 104 - iter 140/280 - loss 0.36804037 - samples/sec: 146.02\n",
            "2019-12-23 10:09:07,069 epoch 104 - iter 168/280 - loss 0.37426420 - samples/sec: 145.22\n",
            "2019-12-23 10:09:13,236 epoch 104 - iter 196/280 - loss 0.36974521 - samples/sec: 145.81\n",
            "2019-12-23 10:09:19,049 epoch 104 - iter 224/280 - loss 0.35643520 - samples/sec: 154.76\n",
            "2019-12-23 10:09:25,040 epoch 104 - iter 252/280 - loss 0.35085355 - samples/sec: 150.10\n",
            "2019-12-23 10:09:30,669 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:09:30,671 EPOCH 104 done: loss 0.3532 - lr 0.0031\n",
            "2019-12-23 10:09:30,676 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:09:30,679 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:09:30,960 epoch 105 - iter 0/280 - loss 0.38875520 - samples/sec: 3200.07\n",
            "2019-12-23 10:09:36,912 epoch 105 - iter 28/280 - loss 0.39493929 - samples/sec: 151.03\n",
            "2019-12-23 10:09:42,942 epoch 105 - iter 56/280 - loss 0.39001945 - samples/sec: 149.08\n",
            "2019-12-23 10:09:50,419 epoch 105 - iter 84/280 - loss 0.37780877 - samples/sec: 120.20\n",
            "2019-12-23 10:09:56,405 epoch 105 - iter 112/280 - loss 0.37526712 - samples/sec: 150.22\n",
            "2019-12-23 10:10:02,503 epoch 105 - iter 140/280 - loss 0.38018277 - samples/sec: 147.49\n",
            "2019-12-23 10:10:08,579 epoch 105 - iter 168/280 - loss 0.38061984 - samples/sec: 147.92\n",
            "2019-12-23 10:10:14,507 epoch 105 - iter 196/280 - loss 0.37131792 - samples/sec: 151.68\n",
            "2019-12-23 10:10:20,470 epoch 105 - iter 224/280 - loss 0.36036473 - samples/sec: 150.86\n",
            "2019-12-23 10:10:26,616 epoch 105 - iter 252/280 - loss 0.36291927 - samples/sec: 146.32\n",
            "2019-12-23 10:10:32,206 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:10:32,207 EPOCH 105 done: loss 0.3649 - lr 0.0031\n",
            "Epoch   104: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2019-12-23 10:10:32,214 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:10:32,215 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:10:32,442 epoch 106 - iter 0/280 - loss 0.20591021 - samples/sec: 3965.25\n",
            "2019-12-23 10:10:38,632 epoch 106 - iter 28/280 - loss 0.36761954 - samples/sec: 145.27\n",
            "2019-12-23 10:10:44,543 epoch 106 - iter 56/280 - loss 0.36590600 - samples/sec: 152.14\n",
            "2019-12-23 10:10:50,576 epoch 106 - iter 84/280 - loss 0.35426433 - samples/sec: 149.09\n",
            "2019-12-23 10:10:56,692 epoch 106 - iter 112/280 - loss 0.36112881 - samples/sec: 146.99\n",
            "2019-12-23 10:11:02,578 epoch 106 - iter 140/280 - loss 0.36665708 - samples/sec: 152.76\n",
            "2019-12-23 10:11:08,911 epoch 106 - iter 168/280 - loss 0.36724569 - samples/sec: 142.00\n",
            "2019-12-23 10:11:15,239 epoch 106 - iter 196/280 - loss 0.36449389 - samples/sec: 142.12\n",
            "2019-12-23 10:11:21,190 epoch 106 - iter 224/280 - loss 0.35857872 - samples/sec: 151.15\n",
            "2019-12-23 10:11:27,159 epoch 106 - iter 252/280 - loss 0.36194496 - samples/sec: 150.61\n",
            "2019-12-23 10:11:32,814 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:11:32,816 EPOCH 106 done: loss 0.3634 - lr 0.0016\n",
            "2019-12-23 10:11:32,818 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:11:32,823 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:11:33,053 epoch 107 - iter 0/280 - loss 0.32183433 - samples/sec: 3997.63\n",
            "2019-12-23 10:11:39,048 epoch 107 - iter 28/280 - loss 0.38224593 - samples/sec: 149.97\n",
            "2019-12-23 10:11:44,980 epoch 107 - iter 56/280 - loss 0.34778369 - samples/sec: 151.63\n",
            "2019-12-23 10:11:50,952 epoch 107 - iter 84/280 - loss 0.35730732 - samples/sec: 150.64\n",
            "2019-12-23 10:11:56,894 epoch 107 - iter 112/280 - loss 0.35864923 - samples/sec: 151.34\n",
            "2019-12-23 10:12:02,755 epoch 107 - iter 140/280 - loss 0.36808791 - samples/sec: 153.45\n",
            "2019-12-23 10:12:08,692 epoch 107 - iter 168/280 - loss 0.36717508 - samples/sec: 151.48\n",
            "2019-12-23 10:12:14,871 epoch 107 - iter 196/280 - loss 0.36315547 - samples/sec: 145.54\n",
            "2019-12-23 10:12:20,940 epoch 107 - iter 224/280 - loss 0.35863957 - samples/sec: 148.16\n",
            "2019-12-23 10:12:27,016 epoch 107 - iter 252/280 - loss 0.35843274 - samples/sec: 148.05\n",
            "2019-12-23 10:12:32,627 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:12:32,628 EPOCH 107 done: loss 0.3564 - lr 0.0016\n",
            "2019-12-23 10:12:32,638 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:12:32,643 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:12:32,949 epoch 108 - iter 0/280 - loss 0.65324581 - samples/sec: 2942.80\n",
            "2019-12-23 10:12:39,139 epoch 108 - iter 28/280 - loss 0.32837436 - samples/sec: 145.23\n",
            "2019-12-23 10:12:45,265 epoch 108 - iter 56/280 - loss 0.33721809 - samples/sec: 146.73\n",
            "2019-12-23 10:12:51,075 epoch 108 - iter 84/280 - loss 0.33921192 - samples/sec: 155.12\n",
            "2019-12-23 10:12:56,999 epoch 108 - iter 112/280 - loss 0.35287676 - samples/sec: 151.80\n",
            "2019-12-23 10:13:03,032 epoch 108 - iter 140/280 - loss 0.35030349 - samples/sec: 149.08\n",
            "2019-12-23 10:13:08,842 epoch 108 - iter 168/280 - loss 0.34253267 - samples/sec: 154.82\n",
            "2019-12-23 10:13:14,832 epoch 108 - iter 196/280 - loss 0.33797418 - samples/sec: 150.11\n",
            "2019-12-23 10:13:20,841 epoch 108 - iter 224/280 - loss 0.33500284 - samples/sec: 149.76\n",
            "2019-12-23 10:13:26,762 epoch 108 - iter 252/280 - loss 0.33686555 - samples/sec: 152.01\n",
            "2019-12-23 10:13:32,424 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:13:32,428 EPOCH 108 done: loss 0.3408 - lr 0.0016\n",
            "2019-12-23 10:13:32,429 BAD EPOCHS (no improvement): 0\n",
            "2019-12-23 10:13:32,430 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:13:32,632 epoch 109 - iter 0/280 - loss 0.35529208 - samples/sec: 4504.61\n",
            "2019-12-23 10:13:38,612 epoch 109 - iter 28/280 - loss 0.38836419 - samples/sec: 150.34\n",
            "2019-12-23 10:13:44,395 epoch 109 - iter 56/280 - loss 0.37298775 - samples/sec: 155.53\n",
            "2019-12-23 10:13:50,294 epoch 109 - iter 84/280 - loss 0.36464874 - samples/sec: 152.57\n",
            "2019-12-23 10:13:56,165 epoch 109 - iter 112/280 - loss 0.36321485 - samples/sec: 153.12\n",
            "2019-12-23 10:14:02,251 epoch 109 - iter 140/280 - loss 0.36028880 - samples/sec: 147.81\n",
            "2019-12-23 10:14:08,079 epoch 109 - iter 168/280 - loss 0.37046624 - samples/sec: 154.35\n",
            "2019-12-23 10:14:13,983 epoch 109 - iter 196/280 - loss 0.37202218 - samples/sec: 152.31\n",
            "2019-12-23 10:14:20,222 epoch 109 - iter 224/280 - loss 0.37220070 - samples/sec: 144.12\n",
            "2019-12-23 10:14:26,337 epoch 109 - iter 252/280 - loss 0.36238210 - samples/sec: 147.15\n",
            "2019-12-23 10:14:31,940 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:14:31,941 EPOCH 109 done: loss 0.3635 - lr 0.0016\n",
            "2019-12-23 10:14:31,942 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:14:31,945 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:14:32,194 epoch 110 - iter 0/280 - loss 0.31262827 - samples/sec: 3733.79\n",
            "2019-12-23 10:14:38,122 epoch 110 - iter 28/280 - loss 0.37947340 - samples/sec: 151.65\n",
            "2019-12-23 10:14:44,111 epoch 110 - iter 56/280 - loss 0.36415492 - samples/sec: 150.23\n",
            "2019-12-23 10:14:50,212 epoch 110 - iter 84/280 - loss 0.36085569 - samples/sec: 147.39\n",
            "2019-12-23 10:14:56,136 epoch 110 - iter 112/280 - loss 0.36418558 - samples/sec: 151.87\n",
            "2019-12-23 10:15:02,248 epoch 110 - iter 140/280 - loss 0.36837006 - samples/sec: 147.13\n",
            "2019-12-23 10:15:08,170 epoch 110 - iter 168/280 - loss 0.36620752 - samples/sec: 151.86\n",
            "2019-12-23 10:15:14,579 epoch 110 - iter 196/280 - loss 0.36351720 - samples/sec: 140.28\n",
            "2019-12-23 10:15:20,656 epoch 110 - iter 224/280 - loss 0.36730369 - samples/sec: 147.95\n",
            "2019-12-23 10:15:26,576 epoch 110 - iter 252/280 - loss 0.36144906 - samples/sec: 151.93\n",
            "2019-12-23 10:15:32,267 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:15:32,268 EPOCH 110 done: loss 0.3652 - lr 0.0016\n",
            "2019-12-23 10:15:32,274 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:15:32,275 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:15:32,483 epoch 111 - iter 0/280 - loss 0.19897056 - samples/sec: 4351.35\n",
            "2019-12-23 10:15:38,597 epoch 111 - iter 28/280 - loss 0.34875625 - samples/sec: 147.05\n",
            "2019-12-23 10:15:44,654 epoch 111 - iter 56/280 - loss 0.35283365 - samples/sec: 148.47\n",
            "2019-12-23 10:15:50,788 epoch 111 - iter 84/280 - loss 0.35231257 - samples/sec: 146.57\n",
            "2019-12-23 10:15:56,795 epoch 111 - iter 112/280 - loss 0.35029926 - samples/sec: 149.68\n",
            "2019-12-23 10:16:02,597 epoch 111 - iter 140/280 - loss 0.33951938 - samples/sec: 155.08\n",
            "2019-12-23 10:16:08,480 epoch 111 - iter 168/280 - loss 0.35039348 - samples/sec: 152.89\n",
            "2019-12-23 10:16:14,528 epoch 111 - iter 196/280 - loss 0.35305395 - samples/sec: 148.68\n",
            "2019-12-23 10:16:20,562 epoch 111 - iter 224/280 - loss 0.35549817 - samples/sec: 149.05\n",
            "2019-12-23 10:16:26,496 epoch 111 - iter 252/280 - loss 0.35866032 - samples/sec: 151.54\n",
            "2019-12-23 10:16:32,179 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:16:32,180 EPOCH 111 done: loss 0.3601 - lr 0.0016\n",
            "2019-12-23 10:16:32,186 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:16:32,194 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:16:32,393 epoch 112 - iter 0/280 - loss 0.39578515 - samples/sec: 4574.34\n",
            "2019-12-23 10:16:38,437 epoch 112 - iter 28/280 - loss 0.32167996 - samples/sec: 148.72\n",
            "2019-12-23 10:16:44,385 epoch 112 - iter 56/280 - loss 0.36247970 - samples/sec: 151.22\n",
            "2019-12-23 10:16:50,464 epoch 112 - iter 84/280 - loss 0.37416172 - samples/sec: 148.21\n",
            "2019-12-23 10:16:56,580 epoch 112 - iter 112/280 - loss 0.36812256 - samples/sec: 147.13\n",
            "2019-12-23 10:17:02,644 epoch 112 - iter 140/280 - loss 0.37334657 - samples/sec: 148.29\n",
            "2019-12-23 10:17:08,677 epoch 112 - iter 168/280 - loss 0.37899226 - samples/sec: 149.04\n",
            "2019-12-23 10:17:14,605 epoch 112 - iter 196/280 - loss 0.38055624 - samples/sec: 151.70\n",
            "2019-12-23 10:17:20,556 epoch 112 - iter 224/280 - loss 0.37554909 - samples/sec: 151.09\n",
            "2019-12-23 10:17:26,586 epoch 112 - iter 252/280 - loss 0.37775690 - samples/sec: 149.31\n",
            "2019-12-23 10:17:32,477 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:17:32,482 EPOCH 112 done: loss 0.3714 - lr 0.0016\n",
            "Epoch   111: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2019-12-23 10:17:32,483 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:17:32,484 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:17:32,711 epoch 113 - iter 0/280 - loss 0.32220972 - samples/sec: 4120.90\n",
            "2019-12-23 10:17:38,854 epoch 113 - iter 28/280 - loss 0.37626460 - samples/sec: 146.34\n",
            "2019-12-23 10:17:44,713 epoch 113 - iter 56/280 - loss 0.35891563 - samples/sec: 153.51\n",
            "2019-12-23 10:17:50,754 epoch 113 - iter 84/280 - loss 0.34354527 - samples/sec: 148.94\n",
            "2019-12-23 10:17:56,763 epoch 113 - iter 112/280 - loss 0.35062597 - samples/sec: 149.65\n",
            "2019-12-23 10:18:02,653 epoch 113 - iter 140/280 - loss 0.35331000 - samples/sec: 152.65\n",
            "2019-12-23 10:18:08,746 epoch 113 - iter 168/280 - loss 0.35300757 - samples/sec: 147.58\n",
            "2019-12-23 10:18:14,677 epoch 113 - iter 196/280 - loss 0.34947123 - samples/sec: 151.69\n",
            "2019-12-23 10:18:20,614 epoch 113 - iter 224/280 - loss 0.34490825 - samples/sec: 151.44\n",
            "2019-12-23 10:18:26,675 epoch 113 - iter 252/280 - loss 0.34871128 - samples/sec: 148.34\n",
            "2019-12-23 10:18:32,241 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:18:32,242 EPOCH 113 done: loss 0.3449 - lr 0.0008\n",
            "2019-12-23 10:18:32,249 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:18:32,250 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:18:32,458 epoch 114 - iter 0/280 - loss 0.62249231 - samples/sec: 4361.06\n",
            "2019-12-23 10:18:38,418 epoch 114 - iter 28/280 - loss 0.40061393 - samples/sec: 150.88\n",
            "2019-12-23 10:18:44,476 epoch 114 - iter 56/280 - loss 0.40962941 - samples/sec: 148.45\n",
            "2019-12-23 10:18:50,535 epoch 114 - iter 84/280 - loss 0.38944373 - samples/sec: 148.42\n",
            "2019-12-23 10:18:56,501 epoch 114 - iter 112/280 - loss 0.40513885 - samples/sec: 150.76\n",
            "2019-12-23 10:19:02,640 epoch 114 - iter 140/280 - loss 0.40587987 - samples/sec: 146.46\n",
            "2019-12-23 10:19:08,587 epoch 114 - iter 168/280 - loss 0.38947643 - samples/sec: 151.23\n",
            "2019-12-23 10:19:14,585 epoch 114 - iter 196/280 - loss 0.38690234 - samples/sec: 149.91\n",
            "2019-12-23 10:19:20,415 epoch 114 - iter 224/280 - loss 0.38099799 - samples/sec: 154.30\n",
            "2019-12-23 10:19:26,482 epoch 114 - iter 252/280 - loss 0.37706391 - samples/sec: 148.20\n",
            "2019-12-23 10:19:32,016 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:19:32,020 EPOCH 114 done: loss 0.3697 - lr 0.0008\n",
            "2019-12-23 10:19:32,021 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:19:32,022 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:19:32,254 epoch 115 - iter 0/280 - loss 0.23148775 - samples/sec: 3899.09\n",
            "2019-12-23 10:19:38,494 epoch 115 - iter 28/280 - loss 0.41378190 - samples/sec: 144.07\n",
            "2019-12-23 10:19:44,430 epoch 115 - iter 56/280 - loss 0.37823905 - samples/sec: 151.52\n",
            "2019-12-23 10:19:50,591 epoch 115 - iter 84/280 - loss 0.37036184 - samples/sec: 145.93\n",
            "2019-12-23 10:19:56,410 epoch 115 - iter 112/280 - loss 0.37393906 - samples/sec: 154.57\n",
            "2019-12-23 10:20:02,300 epoch 115 - iter 140/280 - loss 0.36740269 - samples/sec: 152.69\n",
            "2019-12-23 10:20:08,336 epoch 115 - iter 168/280 - loss 0.36825828 - samples/sec: 149.03\n",
            "2019-12-23 10:20:14,602 epoch 115 - iter 196/280 - loss 0.37263117 - samples/sec: 143.48\n",
            "2019-12-23 10:20:20,423 epoch 115 - iter 224/280 - loss 0.36639586 - samples/sec: 154.50\n",
            "2019-12-23 10:20:26,197 epoch 115 - iter 252/280 - loss 0.36124824 - samples/sec: 155.74\n",
            "2019-12-23 10:20:31,988 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:20:31,990 EPOCH 115 done: loss 0.3618 - lr 0.0008\n",
            "2019-12-23 10:20:31,991 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:20:31,995 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:20:32,254 epoch 116 - iter 0/280 - loss 0.69453740 - samples/sec: 3517.06\n",
            "2019-12-23 10:20:38,413 epoch 116 - iter 28/280 - loss 0.32509419 - samples/sec: 145.93\n",
            "2019-12-23 10:20:44,533 epoch 116 - iter 56/280 - loss 0.34877842 - samples/sec: 146.94\n",
            "2019-12-23 10:20:50,379 epoch 116 - iter 84/280 - loss 0.33120128 - samples/sec: 153.85\n",
            "2019-12-23 10:20:56,357 epoch 116 - iter 112/280 - loss 0.34284023 - samples/sec: 150.41\n",
            "2019-12-23 10:21:03,810 epoch 116 - iter 140/280 - loss 0.34259095 - samples/sec: 120.56\n",
            "2019-12-23 10:21:09,698 epoch 116 - iter 168/280 - loss 0.34598580 - samples/sec: 152.73\n",
            "2019-12-23 10:21:15,752 epoch 116 - iter 196/280 - loss 0.34510076 - samples/sec: 148.54\n",
            "2019-12-23 10:21:21,783 epoch 116 - iter 224/280 - loss 0.34416989 - samples/sec: 149.12\n",
            "2019-12-23 10:21:27,709 epoch 116 - iter 252/280 - loss 0.35114713 - samples/sec: 151.69\n",
            "2019-12-23 10:21:33,405 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:21:33,406 EPOCH 116 done: loss 0.3491 - lr 0.0008\n",
            "Epoch   115: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2019-12-23 10:21:33,410 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:21:33,412 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:21:33,633 epoch 117 - iter 0/280 - loss 0.53974962 - samples/sec: 4230.46\n",
            "2019-12-23 10:21:39,662 epoch 117 - iter 28/280 - loss 0.37758485 - samples/sec: 149.07\n",
            "2019-12-23 10:21:45,614 epoch 117 - iter 56/280 - loss 0.34711774 - samples/sec: 151.12\n",
            "2019-12-23 10:21:51,574 epoch 117 - iter 84/280 - loss 0.34908216 - samples/sec: 150.86\n",
            "2019-12-23 10:21:57,738 epoch 117 - iter 112/280 - loss 0.35751495 - samples/sec: 145.86\n",
            "2019-12-23 10:22:03,800 epoch 117 - iter 140/280 - loss 0.36246119 - samples/sec: 148.35\n",
            "2019-12-23 10:22:09,668 epoch 117 - iter 168/280 - loss 0.35926994 - samples/sec: 153.27\n",
            "2019-12-23 10:22:15,652 epoch 117 - iter 196/280 - loss 0.36365661 - samples/sec: 150.26\n",
            "2019-12-23 10:22:21,444 epoch 117 - iter 224/280 - loss 0.35980626 - samples/sec: 155.30\n",
            "2019-12-23 10:22:27,368 epoch 117 - iter 252/280 - loss 0.36117569 - samples/sec: 151.80\n",
            "2019-12-23 10:22:33,169 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:22:33,170 EPOCH 117 done: loss 0.3636 - lr 0.0004\n",
            "2019-12-23 10:22:33,176 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:22:33,177 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:22:33,410 epoch 118 - iter 0/280 - loss 0.52717686 - samples/sec: 3876.00\n",
            "2019-12-23 10:22:39,543 epoch 118 - iter 28/280 - loss 0.37596860 - samples/sec: 146.67\n",
            "2019-12-23 10:22:45,917 epoch 118 - iter 56/280 - loss 0.34779792 - samples/sec: 141.04\n",
            "2019-12-23 10:22:51,993 epoch 118 - iter 84/280 - loss 0.36390596 - samples/sec: 147.99\n",
            "2019-12-23 10:22:57,944 epoch 118 - iter 112/280 - loss 0.35624970 - samples/sec: 151.08\n",
            "2019-12-23 10:23:03,776 epoch 118 - iter 140/280 - loss 0.35850303 - samples/sec: 154.24\n",
            "2019-12-23 10:23:09,783 epoch 118 - iter 168/280 - loss 0.36226681 - samples/sec: 149.71\n",
            "2019-12-23 10:23:15,904 epoch 118 - iter 196/280 - loss 0.36091711 - samples/sec: 146.85\n",
            "2019-12-23 10:23:22,010 epoch 118 - iter 224/280 - loss 0.36150823 - samples/sec: 147.32\n",
            "2019-12-23 10:23:28,015 epoch 118 - iter 252/280 - loss 0.35707202 - samples/sec: 149.76\n",
            "2019-12-23 10:23:33,553 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:23:33,557 EPOCH 118 done: loss 0.3521 - lr 0.0004\n",
            "2019-12-23 10:23:33,558 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:23:33,559 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:23:33,829 epoch 119 - iter 0/280 - loss 0.19879556 - samples/sec: 3343.42\n",
            "2019-12-23 10:23:39,961 epoch 119 - iter 28/280 - loss 0.34203378 - samples/sec: 146.58\n",
            "2019-12-23 10:23:45,964 epoch 119 - iter 56/280 - loss 0.33569558 - samples/sec: 149.88\n",
            "2019-12-23 10:23:52,059 epoch 119 - iter 84/280 - loss 0.35198761 - samples/sec: 147.56\n",
            "2019-12-23 10:23:57,965 epoch 119 - iter 112/280 - loss 0.35458108 - samples/sec: 152.25\n",
            "2019-12-23 10:24:03,993 epoch 119 - iter 140/280 - loss 0.35821320 - samples/sec: 149.26\n",
            "2019-12-23 10:24:10,027 epoch 119 - iter 168/280 - loss 0.35383106 - samples/sec: 149.14\n",
            "2019-12-23 10:24:15,957 epoch 119 - iter 196/280 - loss 0.36397003 - samples/sec: 151.63\n",
            "2019-12-23 10:24:21,929 epoch 119 - iter 224/280 - loss 0.36406677 - samples/sec: 150.62\n",
            "2019-12-23 10:24:28,051 epoch 119 - iter 252/280 - loss 0.36189769 - samples/sec: 146.84\n",
            "2019-12-23 10:24:33,861 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:24:33,862 EPOCH 119 done: loss 0.3612 - lr 0.0004\n",
            "2019-12-23 10:24:33,864 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:24:33,868 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:24:34,113 epoch 120 - iter 0/280 - loss 0.22678089 - samples/sec: 3698.99\n",
            "2019-12-23 10:24:39,917 epoch 120 - iter 28/280 - loss 0.28377158 - samples/sec: 154.89\n",
            "2019-12-23 10:24:45,873 epoch 120 - iter 56/280 - loss 0.32171256 - samples/sec: 150.99\n",
            "2019-12-23 10:24:51,895 epoch 120 - iter 84/280 - loss 0.33480903 - samples/sec: 149.37\n",
            "2019-12-23 10:24:57,958 epoch 120 - iter 112/280 - loss 0.35288874 - samples/sec: 148.31\n",
            "2019-12-23 10:25:04,225 epoch 120 - iter 140/280 - loss 0.35781345 - samples/sec: 143.46\n",
            "2019-12-23 10:25:10,205 epoch 120 - iter 168/280 - loss 0.35311044 - samples/sec: 150.36\n",
            "2019-12-23 10:25:16,397 epoch 120 - iter 196/280 - loss 0.36742945 - samples/sec: 145.29\n",
            "2019-12-23 10:25:22,378 epoch 120 - iter 224/280 - loss 0.36502210 - samples/sec: 150.35\n",
            "2019-12-23 10:25:28,511 epoch 120 - iter 252/280 - loss 0.36374912 - samples/sec: 146.64\n",
            "2019-12-23 10:25:34,040 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:25:34,041 EPOCH 120 done: loss 0.3686 - lr 0.0004\n",
            "Epoch   119: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2019-12-23 10:25:34,042 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:25:34,043 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:25:34,255 epoch 121 - iter 0/280 - loss 0.32940412 - samples/sec: 4348.44\n",
            "2019-12-23 10:25:40,211 epoch 121 - iter 28/280 - loss 0.35280876 - samples/sec: 150.92\n",
            "2019-12-23 10:25:46,172 epoch 121 - iter 56/280 - loss 0.37359093 - samples/sec: 150.84\n",
            "2019-12-23 10:25:52,380 epoch 121 - iter 84/280 - loss 0.35824115 - samples/sec: 144.83\n",
            "2019-12-23 10:25:58,672 epoch 121 - iter 112/280 - loss 0.36223225 - samples/sec: 142.92\n",
            "2019-12-23 10:26:04,650 epoch 121 - iter 140/280 - loss 0.36504542 - samples/sec: 150.47\n",
            "2019-12-23 10:26:10,866 epoch 121 - iter 168/280 - loss 0.35929817 - samples/sec: 144.85\n",
            "2019-12-23 10:26:16,943 epoch 121 - iter 196/280 - loss 0.36058056 - samples/sec: 147.98\n",
            "2019-12-23 10:26:22,714 epoch 121 - iter 224/280 - loss 0.35674732 - samples/sec: 155.86\n",
            "2019-12-23 10:26:28,715 epoch 121 - iter 252/280 - loss 0.36163014 - samples/sec: 149.83\n",
            "2019-12-23 10:26:34,460 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:26:34,461 EPOCH 121 done: loss 0.3625 - lr 0.0002\n",
            "2019-12-23 10:26:34,467 BAD EPOCHS (no improvement): 1\n",
            "2019-12-23 10:26:34,470 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:26:34,679 epoch 122 - iter 0/280 - loss 0.32259250 - samples/sec: 4348.65\n",
            "2019-12-23 10:26:40,583 epoch 122 - iter 28/280 - loss 0.34289411 - samples/sec: 152.28\n",
            "2019-12-23 10:26:46,587 epoch 122 - iter 56/280 - loss 0.36572506 - samples/sec: 149.78\n",
            "2019-12-23 10:26:52,680 epoch 122 - iter 84/280 - loss 0.35542445 - samples/sec: 147.60\n",
            "2019-12-23 10:26:58,690 epoch 122 - iter 112/280 - loss 0.34909400 - samples/sec: 149.75\n",
            "2019-12-23 10:27:04,482 epoch 122 - iter 140/280 - loss 0.36401462 - samples/sec: 155.25\n",
            "2019-12-23 10:27:10,477 epoch 122 - iter 168/280 - loss 0.36637780 - samples/sec: 150.02\n",
            "2019-12-23 10:27:16,430 epoch 122 - iter 196/280 - loss 0.37189465 - samples/sec: 151.04\n",
            "2019-12-23 10:27:22,334 epoch 122 - iter 224/280 - loss 0.37656905 - samples/sec: 152.33\n",
            "2019-12-23 10:27:28,427 epoch 122 - iter 252/280 - loss 0.37300454 - samples/sec: 147.58\n",
            "2019-12-23 10:27:34,138 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:27:34,141 EPOCH 122 done: loss 0.3702 - lr 0.0002\n",
            "2019-12-23 10:27:34,142 BAD EPOCHS (no improvement): 2\n",
            "2019-12-23 10:27:34,144 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:27:34,375 epoch 123 - iter 0/280 - loss 0.46667480 - samples/sec: 4012.94\n",
            "2019-12-23 10:27:40,333 epoch 123 - iter 28/280 - loss 0.35715297 - samples/sec: 150.85\n",
            "2019-12-23 10:27:46,534 epoch 123 - iter 56/280 - loss 0.34888369 - samples/sec: 145.01\n",
            "2019-12-23 10:27:52,651 epoch 123 - iter 84/280 - loss 0.36488421 - samples/sec: 147.00\n",
            "2019-12-23 10:27:58,822 epoch 123 - iter 112/280 - loss 0.35682320 - samples/sec: 145.69\n",
            "2019-12-23 10:28:04,949 epoch 123 - iter 140/280 - loss 0.35437708 - samples/sec: 146.75\n",
            "2019-12-23 10:28:10,909 epoch 123 - iter 168/280 - loss 0.34115180 - samples/sec: 150.90\n",
            "2019-12-23 10:28:16,662 epoch 123 - iter 196/280 - loss 0.34452883 - samples/sec: 156.34\n",
            "2019-12-23 10:28:22,462 epoch 123 - iter 224/280 - loss 0.35114642 - samples/sec: 155.10\n",
            "2019-12-23 10:28:28,522 epoch 123 - iter 252/280 - loss 0.35243680 - samples/sec: 148.34\n",
            "2019-12-23 10:28:34,196 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:28:34,197 EPOCH 123 done: loss 0.3508 - lr 0.0002\n",
            "2019-12-23 10:28:34,200 BAD EPOCHS (no improvement): 3\n",
            "2019-12-23 10:28:34,201 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:28:34,417 epoch 124 - iter 0/280 - loss 0.35641050 - samples/sec: 4182.19\n",
            "2019-12-23 10:28:40,542 epoch 124 - iter 28/280 - loss 0.34547096 - samples/sec: 146.77\n",
            "2019-12-23 10:28:46,434 epoch 124 - iter 56/280 - loss 0.38763438 - samples/sec: 152.63\n",
            "2019-12-23 10:28:52,613 epoch 124 - iter 84/280 - loss 0.36283487 - samples/sec: 145.52\n",
            "2019-12-23 10:28:58,516 epoch 124 - iter 112/280 - loss 0.35820373 - samples/sec: 152.33\n",
            "2019-12-23 10:29:04,508 epoch 124 - iter 140/280 - loss 0.35860127 - samples/sec: 150.06\n",
            "2019-12-23 10:29:10,378 epoch 124 - iter 168/280 - loss 0.35510508 - samples/sec: 153.31\n",
            "2019-12-23 10:29:16,429 epoch 124 - iter 196/280 - loss 0.36203231 - samples/sec: 148.60\n",
            "2019-12-23 10:29:22,530 epoch 124 - iter 224/280 - loss 0.35889869 - samples/sec: 147.43\n",
            "2019-12-23 10:29:28,536 epoch 124 - iter 252/280 - loss 0.35441925 - samples/sec: 149.74\n",
            "2019-12-23 10:29:34,251 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:29:34,255 EPOCH 124 done: loss 0.3519 - lr 0.0002\n",
            "Epoch   123: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2019-12-23 10:29:34,258 BAD EPOCHS (no improvement): 4\n",
            "2019-12-23 10:29:34,262 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:29:34,262 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-23 10:29:34,263 learning rate too small - quitting training!\n",
            "2019-12-23 10:29:34,264 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S49KbFx42ncr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}