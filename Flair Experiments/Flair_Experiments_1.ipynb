{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flair Experiments - 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bgjxTItqd9T",
        "colab_type": "code",
        "outputId": "d9d21926-b86c-4a96-93c4-7b30d04b9e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.7)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.1)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: ipython==7.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (7.6.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.9)\n",
            "Requirement already satisfied: transformers>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (1.17.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: SudachiPy; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.4.2)\n",
            "Requirement already satisfied: kytea; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.1.4)\n",
            "Requirement already satisfied: natto-py; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.9.0)\n",
            "Requirement already satisfied: sentencepiece; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.0.10)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (42.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (4.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.40)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (0.0.35)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: dartsclone~=0.6.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.6)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.14.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair) (0.46)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers>=2.0.0->flair) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1shQ56YcpD6I",
        "colab_type": "text"
      },
      "source": [
        "# Best Configurations per Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSu1ITnzpeUe",
        "colab_type": "text"
      },
      "source": [
        "## I. CoNLL-03 Named Entity Recognition (English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjNf_1SEpp1t",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0pCGClKntf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/content/conll_03\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2wmkx0CQ1pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## download from\n",
        "## https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I9D-0MoOLy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.move(\"eng.testa\", \"conll_03\")\n",
        "shutil.move(\"eng.testb\", \"conll_03\")\n",
        "shutil.move(\"eng.train\", \"conll_03\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p80A7gh-pz2I",
        "colab_type": "code",
        "outputId": "cb43fb40-f8f5-42a7-f668-c9d29b5036c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "from flair.datasets import CONLL_03\n",
        "from flair.data import Corpus\n",
        "\n",
        "corpus: Corpus = CONLL_03(base_path = \"/content\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 16:00:31,871 Reading data from /content/conll_03\n",
            "2019-12-22 16:00:31,872 Train: /content/conll_03/eng.train\n",
            "2019-12-22 16:00:31,873 Dev: /content/conll_03/eng.testa\n",
            "2019-12-22 16:00:31,874 Test: /content/conll_03/eng.testb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuQD-5q0DwrD",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N09VKnS7qb5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpm9R-oESE3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e3a31ad6-8f1f-440c-91f4-71180b862515"
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = CONLL_03(base_path = \"/content\").downsample(0.1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 16:01:02,048 Reading data from /content/conll_03\n",
            "2019-12-22 16:01:02,050 Train: /content/conll_03/eng.train\n",
            "2019-12-22 16:01:02,050 Dev: /content/conll_03/eng.testa\n",
            "2019-12-22 16:01:02,051 Test: /content/conll_03/eng.testb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uiK5aesSOLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU2aG1KOSUKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f36abcb6-a25c-453d-f250-ee12fc6f5375"
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "              # GloVe embeddings                            \n",
        "              WordEmbeddings('glove'),\n",
        "\n",
        "              # contextual string embeddings, forward\n",
        "              PooledFlairEmbeddings('news-forward', pooling = 'min'),\n",
        "\n",
        "              # contextual string embeddings, backward\n",
        "              PooledFlairEmbeddings('news-backward', pooling = 'min'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5oxb46tS_T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWRYwYD3TQWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('content/conll_03/example-ner',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGBGLyyzXnOl",
        "colab_type": "text"
      },
      "source": [
        "## II. CoNLL-03 Named Entity Recognition (German)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvDRsgW0Y1bn",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoD3TGBwTg77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/content/conll_03_german\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNrOjYwUX6Bc",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "e2184e67-0927-40d9-c749-e830565d4797"
      },
      "source": [
        "## download from\n",
        "## https://github.com/MaviccPRP/ger_ner_evals/tree/master/corpora/conll2003\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61bfe4f7-170d-4026-abb1-f706438042c0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-61bfe4f7-170d-4026-abb1-f706438042c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deu.testa to deu.testa\n",
            "Saving deu.testb to deu.testb\n",
            "Saving deu.train to deu.train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axOyu8UAYvcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03944359-99ba-4951-953e-01e329fd3706"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.move(\"deu.testa\", \"conll_03_german\")\n",
        "shutil.move(\"deu.testb\", \"conll_03_german\")\n",
        "shutil.move(\"deu.train\", \"conll_03_german\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'conll_03_german/deu.train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a6QGqg6ZiHu",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTNaRTGrZbTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03_GERMAN\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-isjgNxsZz6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = CONLL_03_GERMAN(base_path = '/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2yY540wZ9mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfOiWKtLitbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "          WordEmbeddings('de'),\n",
        "          PooledFlairEmbeddings('german-forward'),\n",
        "          PooledFlairEmbeddings('german-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esoRLjySjET5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhPNH5e6kl5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('content/conll_03_german/example-ner',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leoczL1Pk6F8",
        "colab_type": "text"
      },
      "source": [
        "## III. CoNLL-03 Named Entity Recognition (Dutch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx45AalklGfD",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPKUk4TUlLaq",
        "colab_type": "text"
      },
      "source": [
        "Data is included in Flair and will get automatically downloaded when you run the script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAfaLdAdlQVZ",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28qyxVQk-AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03_DUTCH\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUWbQD3QlnRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "4d20d683-28a5-4533-93f5-471d83b769da"
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = CONLL_03_DUTCH()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:10:26,936 https://www.clips.uantwerpen.be/conll2002/ner/data/ned.testa not found in cache, downloading to /tmp/tmpyoofjmp6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 450512/450512 [00:00<00:00, 4927734.87B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:10:27,120 copying /tmp/tmpyoofjmp6 to cache at /root/.flair/datasets/conll_03_dutch/ned.testa\n",
            "2019-12-22 17:10:27,122 removing temp file /tmp/tmpyoofjmp6\n",
            "2019-12-22 17:10:27,215 https://www.clips.uantwerpen.be/conll2002/ner/data/ned.testb not found in cache, downloading to /tmp/tmp021xys8a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 813815/813815 [00:00<00:00, 6698038.72B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:10:27,431 copying /tmp/tmp021xys8a to cache at /root/.flair/datasets/conll_03_dutch/ned.testb\n",
            "2019-12-22 17:10:27,437 removing temp file /tmp/tmp021xys8a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:10:27,530 https://www.clips.uantwerpen.be/conll2002/ner/data/ned.train not found in cache, downloading to /tmp/tmphe68e9r8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2375449/2375449 [00:00<00:00, 9154828.27B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:10:27,885 copying /tmp/tmphe68e9r8 to cache at /root/.flair/datasets/conll_03_dutch/ned.train\n",
            "2019-12-22 17:10:27,889 removing temp file /tmp/tmphe68e9r8\n",
            "2019-12-22 17:10:27,893 Reading data from /root/.flair/datasets/conll_03_dutch\n",
            "2019-12-22 17:10:27,894 Train: /root/.flair/datasets/conll_03_dutch/ned.train\n",
            "2019-12-22 17:10:27,895 Dev: /root/.flair/datasets/conll_03_dutch/ned.testa\n",
            "2019-12-22 17:10:27,896 Test: /root/.flair/datasets/conll_03_dutch/ned.testb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOK1CayElq_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it8ih9uRl3cH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "83d2c47b-d073-40e0-9c32-a31ddeb9617b"
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "              WordEmbeddings('nl'),\n",
        "              PooledFlairEmbeddings('dutch-forward', pooling = 'mean'),\n",
        "              PooledFlairEmbeddings('dutch-backward', pooling = 'mean'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:16:45,406 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-stefan-it/lm-nl-opus-large-forward-v0.1.pt not found in cache, downloading to /tmp/tmpa3eh9ags\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 136162055/136162055 [00:02<00:00, 62961449.76B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:16:47,746 copying /tmp/tmpa3eh9ags to cache at /root/.flair/embeddings/lm-nl-opus-large-forward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:16:47,936 removing temp file /tmp/tmpa3eh9ags\n",
            "2019-12-22 17:16:48,558 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-stefan-it/lm-nl-opus-large-backward-v0.1.pt not found in cache, downloading to /tmp/tmpk0q1d6qn\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 136162055/136162055 [00:02<00:00, 59715492.02B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:16:51,016 copying /tmp/tmpk0q1d6qn to cache at /root/.flair/embeddings/lm-nl-opus-large-backward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:16:51,263 removing temp file /tmp/tmpk0q1d6qn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7BgjmIpmV5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QU_9m11nYeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab80311a-47c1-400e-911e-ebab4a8d1f1d"
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:20:11,535 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,537 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('nl')\n",
            "    (list_embedding_1): PooledFlairEmbeddings(\n",
            "      (context_embeddings): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.1, inplace=False)\n",
            "          (encoder): Embedding(7632, 100)\n",
            "          (rnn): LSTM(100, 2048)\n",
            "          (decoder): Linear(in_features=2048, out_features=7632, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): PooledFlairEmbeddings(\n",
            "      (context_embeddings): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.1, inplace=False)\n",
            "          (encoder): Embedding(7632, 100)\n",
            "          (rnn): LSTM(100, 2048)\n",
            "          (decoder): Linear(in_features=2048, out_features=7632, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=8492, out_features=8492, bias=True)\n",
            "  (rnn): LSTM(8492, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
            ")\"\n",
            "2019-12-22 17:20:11,540 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,542 Corpus: \"Corpus: 15806 train + 2895 dev + 5195 test sentences\"\n",
            "2019-12-22 17:20:11,544 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,546 Parameters:\n",
            "2019-12-22 17:20:11,548  - learning_rate: \"0.1\"\n",
            "2019-12-22 17:20:11,550  - mini_batch_size: \"32\"\n",
            "2019-12-22 17:20:11,552  - patience: \"3\"\n",
            "2019-12-22 17:20:11,553  - anneal_factor: \"0.5\"\n",
            "2019-12-22 17:20:11,555  - max_epochs: \"150\"\n",
            "2019-12-22 17:20:11,557  - shuffle: \"True\"\n",
            "2019-12-22 17:20:11,559  - train_with_dev: \"True\"\n",
            "2019-12-22 17:20:11,561  - batch_growth_annealing: \"False\"\n",
            "2019-12-22 17:20:11,563 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,565 Model training base path: \"resources/taggers/example-ner\"\n",
            "2019-12-22 17:20:11,567 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,569 Device: cuda:0\n",
            "2019-12-22 17:20:11,571 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:20:11,572 Embeddings storage mode: cpu\n",
            "2019-12-22 17:20:11,575 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2019-12-22 17:20:12,093 epoch 1 - iter 0/585 - loss 19.85281563 - samples/sec: 3625.08\n",
            "2019-12-22 17:20:39,373 epoch 1 - iter 58/585 - loss 4.25090864 - samples/sec: 68.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9acd04417279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer.train('resources/taggers/example-ner',\n\u001b[1;32m      6\u001b[0m               \u001b[0mtrain_with_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               max_epochs = 150)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, anneal_factor, patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;31m# do the optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 0; 15.90 GiB total capacity; 13.29 GiB already allocated; 65.50 MiB free; 1.84 GiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m6VYgFPoKqf",
        "colab_type": "text"
      },
      "source": [
        "## IV. WNUT-17 Emerging Entity Detection (English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DiGuH4IoQRd",
        "colab_type": "text"
      },
      "source": [
        "### a. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks8j6QojoRkU",
        "colab_type": "text"
      },
      "source": [
        "Data is included in Flair and will get automatically downloaded when you run the script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV1tVYw8oV2i",
        "colab_type": "text"
      },
      "source": [
        "### b. Best Known Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3s5O_5n6Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import WNUT_17\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings, FlairEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2zrANyJoksX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "824a1405-3dcd-4063-d69c-c0f6d1f327b5"
      },
      "source": [
        "# 1. get the corpus\n",
        "\n",
        "corpus: Corpus = WNUT_17()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,155 https://noisy-text.github.io/2017/files/wnut17train.conll not found in cache, downloading to /tmp/tmpeliykho8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 493781/493781 [00:00<00:00, 23155426.12B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,214 copying /tmp/tmpeliykho8 to cache at /root/.flair/datasets/wnut_17/wnut17train.conll\n",
            "2019-12-22 17:23:20,216 removing temp file /tmp/tmpeliykho8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,508 https://noisy-text.github.io/2017/files/emerging.dev.conll not found in cache, downloading to /tmp/tmpi8bjtts8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 114752/114752 [00:00<00:00, 10843861.05B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,550 copying /tmp/tmpi8bjtts8 to cache at /root/.flair/datasets/wnut_17/emerging.dev.conll\n",
            "2019-12-22 17:23:20,553 removing temp file /tmp/tmpi8bjtts8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,849 https://noisy-text.github.io/2017/files/emerging.test.annotated not found in cache, downloading to /tmp/tmp_ytp13a7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 192425/192425 [00:00<00:00, 9813586.09B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:23:20,899 copying /tmp/tmp_ytp13a7 to cache at /root/.flair/datasets/wnut_17/emerging.test.annotated\n",
            "2019-12-22 17:23:20,900 removing temp file /tmp/tmp_ytp13a7\n",
            "2019-12-22 17:23:20,902 Reading data from /root/.flair/datasets/wnut_17\n",
            "2019-12-22 17:23:20,903 Train: /root/.flair/datasets/wnut_17/wnut17train.conll\n",
            "2019-12-22 17:23:20,904 Dev: /root/.flair/datasets/wnut_17/emerging.dev.conll\n",
            "2019-12-22 17:23:20,906 Test: /root/.flair/datasets/wnut_17/emerging.test.annotated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4m9-x0oonx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b_TaUuuozj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "48a6d66d-29c1-488d-b7d1-30e8e95d0a63"
      },
      "source": [
        "# 4. initialize embeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "             WordEmbeddings('crawl'),\n",
        "             WordEmbeddings('twitter'),\n",
        "             FlairEmbeddings('news-forward'),\n",
        "             FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings = embedding_types)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:25:45,246 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-crawl-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpjpvybuj7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200000128/1200000128 [00:15<00:00, 78882270.97B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:00,646 copying /tmp/tmpjpvybuj7 to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:16,114 removing temp file /tmp/tmpjpvybuj7\n",
            "2019-12-22 17:26:19,926 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-crawl-300d-1M not found in cache, downloading to /tmp/tmperc51pct\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39323680/39323680 [00:00<00:00, 71242883.54B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:20,633 copying /tmp/tmperc51pct to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M\n",
            "2019-12-22 17:26:20,671 removing temp file /tmp/tmperc51pct\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:27,182 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/twitter.gensim.vectors.npy not found in cache, downloading to /tmp/tmp0295wbnj\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 477405728/477405728 [00:06<00:00, 78672191.36B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:33,440 copying /tmp/tmp0295wbnj to cache at /root/.flair/embeddings/twitter.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:42,817 removing temp file /tmp/tmp0295wbnj\n",
            "2019-12-22 17:26:43,370 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/twitter.gensim not found in cache, downloading to /tmp/tmp450p_mpp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68268001/68268001 [00:01<00:00, 38180310.30B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:45,330 copying /tmp/tmp450p_mpp to cache at /root/.flair/embeddings/twitter.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:26:46,375 removing temp file /tmp/tmp450p_mpp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZr4joxhpLPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size = 256,\n",
        "                                        embeddings = embeddings,\n",
        "                                        tag_dictionary = tag_dictionary,\n",
        "                                        tag_type = tag_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjiiUwCMprGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2921ba6-e4e1-4728-abfb-3cd925c98715"
      },
      "source": [
        "# 6. initialize trainer\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              train_with_dev = True,\n",
        "              max_epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-22 17:28:44,942 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,944 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('crawl')\n",
            "    (list_embedding_1): WordEmbeddings('twitter')\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_3): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4496, out_features=4496, bias=True)\n",
            "  (rnn): LSTM(4496, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=29, bias=True)\n",
            ")\"\n",
            "2019-12-22 17:28:44,946 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,953 Corpus: \"Corpus: 3394 train + 1009 dev + 1287 test sentences\"\n",
            "2019-12-22 17:28:44,954 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,958 Parameters:\n",
            "2019-12-22 17:28:44,960  - learning_rate: \"0.1\"\n",
            "2019-12-22 17:28:44,962  - mini_batch_size: \"32\"\n",
            "2019-12-22 17:28:44,964  - patience: \"3\"\n",
            "2019-12-22 17:28:44,966  - anneal_factor: \"0.5\"\n",
            "2019-12-22 17:28:44,968  - max_epochs: \"150\"\n",
            "2019-12-22 17:28:44,970  - shuffle: \"True\"\n",
            "2019-12-22 17:28:44,973  - train_with_dev: \"True\"\n",
            "2019-12-22 17:28:44,975  - batch_growth_annealing: \"False\"\n",
            "2019-12-22 17:28:44,977 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,978 Model training base path: \"resources/taggers/example-ner\"\n",
            "2019-12-22 17:28:44,980 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,981 Device: cuda:0\n",
            "2019-12-22 17:28:44,982 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:44,984 Embeddings storage mode: cpu\n",
            "2019-12-22 17:28:44,988 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:28:45,322 epoch 1 - iter 0/138 - loss 49.18029022 - samples/sec: 1259.24\n",
            "2019-12-22 17:28:50,044 epoch 1 - iter 13/138 - loss 13.05766164 - samples/sec: 88.49\n",
            "2019-12-22 17:28:54,636 epoch 1 - iter 26/138 - loss 9.65582655 - samples/sec: 91.07\n",
            "2019-12-22 17:28:59,285 epoch 1 - iter 39/138 - loss 8.24608452 - samples/sec: 89.92\n",
            "2019-12-22 17:29:03,860 epoch 1 - iter 52/138 - loss 7.56829331 - samples/sec: 91.31\n",
            "2019-12-22 17:29:08,428 epoch 1 - iter 65/138 - loss 7.06672901 - samples/sec: 91.45\n",
            "2019-12-22 17:29:12,836 epoch 1 - iter 78/138 - loss 6.73145508 - samples/sec: 94.92\n",
            "2019-12-22 17:29:17,528 epoch 1 - iter 91/138 - loss 6.45357561 - samples/sec: 89.02\n",
            "2019-12-22 17:29:21,956 epoch 1 - iter 104/138 - loss 6.22033850 - samples/sec: 94.35\n",
            "2019-12-22 17:29:26,632 epoch 1 - iter 117/138 - loss 6.10602057 - samples/sec: 89.36\n",
            "2019-12-22 17:29:31,024 epoch 1 - iter 130/138 - loss 5.99252912 - samples/sec: 95.16\n",
            "2019-12-22 17:29:33,483 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:29:33,484 EPOCH 1 done: loss 5.8977 - lr 0.1000\n",
            "2019-12-22 17:29:33,484 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:29:33,490 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:29:33,681 epoch 2 - iter 0/138 - loss 5.57679939 - samples/sec: 2202.55\n",
            "2019-12-22 17:29:35,768 epoch 2 - iter 13/138 - loss 4.00383551 - samples/sec: 201.02\n",
            "2019-12-22 17:29:38,087 epoch 2 - iter 26/138 - loss 4.16629196 - samples/sec: 180.89\n",
            "2019-12-22 17:29:40,278 epoch 2 - iter 39/138 - loss 4.04413391 - samples/sec: 191.60\n",
            "2019-12-22 17:29:42,461 epoch 2 - iter 52/138 - loss 3.86394042 - samples/sec: 192.41\n",
            "2019-12-22 17:29:44,754 epoch 2 - iter 65/138 - loss 3.92994961 - samples/sec: 182.77\n",
            "2019-12-22 17:29:46,966 epoch 2 - iter 78/138 - loss 3.87847177 - samples/sec: 189.61\n",
            "2019-12-22 17:29:49,225 epoch 2 - iter 91/138 - loss 3.74128792 - samples/sec: 186.00\n",
            "2019-12-22 17:29:51,554 epoch 2 - iter 104/138 - loss 3.66635291 - samples/sec: 180.16\n",
            "2019-12-22 17:29:53,804 epoch 2 - iter 117/138 - loss 3.65749243 - samples/sec: 186.56\n",
            "2019-12-22 17:29:56,021 epoch 2 - iter 130/138 - loss 3.62759504 - samples/sec: 189.41\n",
            "2019-12-22 17:29:57,348 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:29:57,349 EPOCH 2 done: loss 3.5830 - lr 0.1000\n",
            "2019-12-22 17:29:57,350 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:29:57,351 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:29:57,543 epoch 3 - iter 0/138 - loss 3.12928867 - samples/sec: 2178.98\n",
            "2019-12-22 17:29:59,750 epoch 3 - iter 13/138 - loss 3.17120796 - samples/sec: 190.01\n",
            "2019-12-22 17:30:02,004 epoch 3 - iter 26/138 - loss 2.79361090 - samples/sec: 186.45\n",
            "2019-12-22 17:30:04,222 epoch 3 - iter 39/138 - loss 2.83031530 - samples/sec: 189.46\n",
            "2019-12-22 17:30:06,522 epoch 3 - iter 52/138 - loss 2.98383528 - samples/sec: 182.62\n",
            "2019-12-22 17:30:08,934 epoch 3 - iter 65/138 - loss 2.98951517 - samples/sec: 174.24\n",
            "2019-12-22 17:30:11,154 epoch 3 - iter 78/138 - loss 2.98051832 - samples/sec: 188.94\n",
            "2019-12-22 17:30:13,323 epoch 3 - iter 91/138 - loss 2.97431287 - samples/sec: 193.86\n",
            "2019-12-22 17:30:15,525 epoch 3 - iter 104/138 - loss 2.93346045 - samples/sec: 190.84\n",
            "2019-12-22 17:30:17,699 epoch 3 - iter 117/138 - loss 2.91818241 - samples/sec: 193.11\n",
            "2019-12-22 17:30:19,952 epoch 3 - iter 130/138 - loss 2.92991619 - samples/sec: 186.16\n",
            "2019-12-22 17:30:21,175 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:30:21,177 EPOCH 3 done: loss 2.9347 - lr 0.1000\n",
            "2019-12-22 17:30:21,177 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:30:21,179 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:30:21,345 epoch 4 - iter 0/138 - loss 2.72402477 - samples/sec: 2524.61\n",
            "2019-12-22 17:30:23,676 epoch 4 - iter 13/138 - loss 2.68477545 - samples/sec: 179.77\n",
            "2019-12-22 17:30:25,863 epoch 4 - iter 26/138 - loss 2.55870460 - samples/sec: 191.97\n",
            "2019-12-22 17:30:28,209 epoch 4 - iter 39/138 - loss 2.64078317 - samples/sec: 178.85\n",
            "2019-12-22 17:30:30,549 epoch 4 - iter 52/138 - loss 2.70177785 - samples/sec: 179.52\n",
            "2019-12-22 17:30:32,863 epoch 4 - iter 65/138 - loss 2.64735822 - samples/sec: 181.44\n",
            "2019-12-22 17:30:35,011 epoch 4 - iter 78/138 - loss 2.59451872 - samples/sec: 195.41\n",
            "2019-12-22 17:30:37,254 epoch 4 - iter 91/138 - loss 2.56068651 - samples/sec: 186.99\n",
            "2019-12-22 17:30:39,463 epoch 4 - iter 104/138 - loss 2.58140547 - samples/sec: 190.05\n",
            "2019-12-22 17:30:41,694 epoch 4 - iter 117/138 - loss 2.58677014 - samples/sec: 187.93\n",
            "2019-12-22 17:30:43,817 epoch 4 - iter 130/138 - loss 2.55824115 - samples/sec: 197.72\n",
            "2019-12-22 17:30:45,033 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:30:45,034 EPOCH 4 done: loss 2.5642 - lr 0.1000\n",
            "2019-12-22 17:30:45,035 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:30:45,036 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:30:45,205 epoch 5 - iter 0/138 - loss 2.37829423 - samples/sec: 2477.64\n",
            "2019-12-22 17:30:47,441 epoch 5 - iter 13/138 - loss 2.22090517 - samples/sec: 187.52\n",
            "2019-12-22 17:30:49,792 epoch 5 - iter 26/138 - loss 2.10103339 - samples/sec: 178.55\n",
            "2019-12-22 17:30:52,100 epoch 5 - iter 39/138 - loss 2.18246371 - samples/sec: 181.64\n",
            "2019-12-22 17:30:54,579 epoch 5 - iter 52/138 - loss 2.11011172 - samples/sec: 169.28\n",
            "2019-12-22 17:30:56,856 epoch 5 - iter 65/138 - loss 2.18341736 - samples/sec: 184.34\n",
            "2019-12-22 17:30:59,135 epoch 5 - iter 78/138 - loss 2.24070108 - samples/sec: 184.65\n",
            "2019-12-22 17:31:01,360 epoch 5 - iter 91/138 - loss 2.25162500 - samples/sec: 188.67\n",
            "2019-12-22 17:31:03,546 epoch 5 - iter 104/138 - loss 2.27221317 - samples/sec: 192.08\n",
            "2019-12-22 17:31:05,757 epoch 5 - iter 117/138 - loss 2.29337875 - samples/sec: 189.90\n",
            "2019-12-22 17:31:07,983 epoch 5 - iter 130/138 - loss 2.35250086 - samples/sec: 188.58\n",
            "2019-12-22 17:31:09,170 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:09,170 EPOCH 5 done: loss 2.3625 - lr 0.1000\n",
            "2019-12-22 17:31:09,171 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:31:09,172 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:09,349 epoch 6 - iter 0/138 - loss 2.10887909 - samples/sec: 2374.36\n",
            "2019-12-22 17:31:11,601 epoch 6 - iter 13/138 - loss 2.37406440 - samples/sec: 186.17\n",
            "2019-12-22 17:31:13,860 epoch 6 - iter 26/138 - loss 2.18193730 - samples/sec: 185.86\n",
            "2019-12-22 17:31:16,016 epoch 6 - iter 39/138 - loss 2.24192720 - samples/sec: 194.63\n",
            "2019-12-22 17:31:18,342 epoch 6 - iter 52/138 - loss 2.20427158 - samples/sec: 180.27\n",
            "2019-12-22 17:31:20,597 epoch 6 - iter 65/138 - loss 2.14243356 - samples/sec: 186.13\n",
            "2019-12-22 17:31:22,719 epoch 6 - iter 78/138 - loss 2.12747359 - samples/sec: 197.75\n",
            "2019-12-22 17:31:24,900 epoch 6 - iter 91/138 - loss 2.11409488 - samples/sec: 192.54\n",
            "2019-12-22 17:31:27,083 epoch 6 - iter 104/138 - loss 2.10729755 - samples/sec: 192.32\n",
            "2019-12-22 17:31:29,533 epoch 6 - iter 117/138 - loss 2.18530137 - samples/sec: 171.28\n",
            "2019-12-22 17:31:31,783 epoch 6 - iter 130/138 - loss 2.15381599 - samples/sec: 186.43\n",
            "2019-12-22 17:31:32,919 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:32,920 EPOCH 6 done: loss 2.1578 - lr 0.1000\n",
            "2019-12-22 17:31:32,921 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:31:32,922 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:33,100 epoch 7 - iter 0/138 - loss 1.41762280 - samples/sec: 2358.45\n",
            "2019-12-22 17:31:35,378 epoch 7 - iter 13/138 - loss 1.98426494 - samples/sec: 184.05\n",
            "2019-12-22 17:31:37,431 epoch 7 - iter 26/138 - loss 2.00681847 - samples/sec: 204.53\n",
            "2019-12-22 17:31:39,582 epoch 7 - iter 39/138 - loss 2.02348568 - samples/sec: 195.55\n",
            "2019-12-22 17:31:41,731 epoch 7 - iter 52/138 - loss 2.06150800 - samples/sec: 195.30\n",
            "2019-12-22 17:31:43,926 epoch 7 - iter 65/138 - loss 2.03985955 - samples/sec: 191.34\n",
            "2019-12-22 17:31:46,241 epoch 7 - iter 78/138 - loss 2.02339026 - samples/sec: 181.18\n",
            "2019-12-22 17:31:48,556 epoch 7 - iter 91/138 - loss 2.04543534 - samples/sec: 181.26\n",
            "2019-12-22 17:31:50,763 epoch 7 - iter 104/138 - loss 2.04681300 - samples/sec: 190.28\n",
            "2019-12-22 17:31:53,086 epoch 7 - iter 117/138 - loss 2.04666434 - samples/sec: 180.58\n",
            "2019-12-22 17:31:55,369 epoch 7 - iter 130/138 - loss 2.03609156 - samples/sec: 183.98\n",
            "2019-12-22 17:31:56,557 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:56,558 EPOCH 7 done: loss 2.0166 - lr 0.1000\n",
            "2019-12-22 17:31:56,562 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:31:56,563 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:31:56,744 epoch 8 - iter 0/138 - loss 1.17342782 - samples/sec: 2323.08\n",
            "2019-12-22 17:31:58,875 epoch 8 - iter 13/138 - loss 1.77687312 - samples/sec: 196.73\n",
            "2019-12-22 17:32:01,132 epoch 8 - iter 26/138 - loss 1.79454969 - samples/sec: 185.80\n",
            "2019-12-22 17:32:03,243 epoch 8 - iter 39/138 - loss 1.86317146 - samples/sec: 198.92\n",
            "2019-12-22 17:32:05,516 epoch 8 - iter 52/138 - loss 1.82011841 - samples/sec: 184.64\n",
            "2019-12-22 17:32:07,716 epoch 8 - iter 65/138 - loss 1.82038372 - samples/sec: 190.81\n",
            "2019-12-22 17:32:09,950 epoch 8 - iter 78/138 - loss 1.84086320 - samples/sec: 188.21\n",
            "2019-12-22 17:32:12,056 epoch 8 - iter 91/138 - loss 1.83307859 - samples/sec: 199.42\n",
            "2019-12-22 17:32:14,207 epoch 8 - iter 104/138 - loss 1.83892781 - samples/sec: 195.15\n",
            "2019-12-22 17:32:16,470 epoch 8 - iter 117/138 - loss 1.85379532 - samples/sec: 185.57\n",
            "2019-12-22 17:32:18,840 epoch 8 - iter 130/138 - loss 1.85255390 - samples/sec: 177.18\n",
            "2019-12-22 17:32:19,971 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:32:19,972 EPOCH 8 done: loss 1.8560 - lr 0.1000\n",
            "2019-12-22 17:32:19,974 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:32:19,976 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:32:20,161 epoch 9 - iter 0/138 - loss 1.62811756 - samples/sec: 2268.78\n",
            "2019-12-22 17:32:22,207 epoch 9 - iter 13/138 - loss 1.85442694 - samples/sec: 205.04\n",
            "2019-12-22 17:32:24,517 epoch 9 - iter 26/138 - loss 1.73672139 - samples/sec: 181.65\n",
            "2019-12-22 17:32:26,770 epoch 9 - iter 39/138 - loss 1.82628439 - samples/sec: 186.27\n",
            "2019-12-22 17:32:29,020 epoch 9 - iter 52/138 - loss 1.82362214 - samples/sec: 186.37\n",
            "2019-12-22 17:32:31,130 epoch 9 - iter 65/138 - loss 1.74982154 - samples/sec: 199.21\n",
            "2019-12-22 17:32:33,336 epoch 9 - iter 78/138 - loss 1.76940206 - samples/sec: 190.07\n",
            "2019-12-22 17:32:35,630 epoch 9 - iter 91/138 - loss 1.79160313 - samples/sec: 182.98\n",
            "2019-12-22 17:32:37,811 epoch 9 - iter 104/138 - loss 1.81763525 - samples/sec: 192.41\n",
            "2019-12-22 17:32:40,028 epoch 9 - iter 117/138 - loss 1.79440126 - samples/sec: 189.24\n",
            "2019-12-22 17:32:42,220 epoch 9 - iter 130/138 - loss 1.79576764 - samples/sec: 191.55\n",
            "2019-12-22 17:32:43,437 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:32:43,438 EPOCH 9 done: loss 1.7847 - lr 0.1000\n",
            "2019-12-22 17:32:43,442 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:32:43,443 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:32:43,604 epoch 10 - iter 0/138 - loss 1.23157406 - samples/sec: 2621.76\n",
            "2019-12-22 17:32:45,753 epoch 10 - iter 13/138 - loss 1.53250013 - samples/sec: 195.08\n",
            "2019-12-22 17:32:48,196 epoch 10 - iter 26/138 - loss 1.59573003 - samples/sec: 171.58\n",
            "2019-12-22 17:32:50,377 epoch 10 - iter 39/138 - loss 1.54036715 - samples/sec: 192.50\n",
            "2019-12-22 17:32:52,580 epoch 10 - iter 52/138 - loss 1.48847683 - samples/sec: 190.44\n",
            "2019-12-22 17:32:54,786 epoch 10 - iter 65/138 - loss 1.49100526 - samples/sec: 190.24\n",
            "2019-12-22 17:32:56,921 epoch 10 - iter 78/138 - loss 1.53379787 - samples/sec: 196.55\n",
            "2019-12-22 17:32:59,079 epoch 10 - iter 91/138 - loss 1.54379864 - samples/sec: 194.51\n",
            "2019-12-22 17:33:01,304 epoch 10 - iter 104/138 - loss 1.56859808 - samples/sec: 188.75\n",
            "2019-12-22 17:33:03,527 epoch 10 - iter 117/138 - loss 1.60384217 - samples/sec: 188.57\n",
            "2019-12-22 17:33:05,795 epoch 10 - iter 130/138 - loss 1.63367613 - samples/sec: 184.99\n",
            "2019-12-22 17:33:06,886 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:06,887 EPOCH 10 done: loss 1.6508 - lr 0.1000\n",
            "2019-12-22 17:33:06,888 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:33:06,888 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:07,056 epoch 11 - iter 0/138 - loss 1.73099542 - samples/sec: 2496.96\n",
            "2019-12-22 17:33:09,303 epoch 11 - iter 13/138 - loss 1.62257745 - samples/sec: 186.52\n",
            "2019-12-22 17:33:11,513 epoch 11 - iter 26/138 - loss 1.65944179 - samples/sec: 189.73\n",
            "2019-12-22 17:33:13,698 epoch 11 - iter 39/138 - loss 1.58608279 - samples/sec: 192.00\n",
            "2019-12-22 17:33:15,867 epoch 11 - iter 52/138 - loss 1.59491207 - samples/sec: 193.68\n",
            "2019-12-22 17:33:18,006 epoch 11 - iter 65/138 - loss 1.55958919 - samples/sec: 196.20\n",
            "2019-12-22 17:33:20,172 epoch 11 - iter 78/138 - loss 1.56895217 - samples/sec: 193.72\n",
            "2019-12-22 17:33:22,349 epoch 11 - iter 91/138 - loss 1.57917969 - samples/sec: 192.57\n",
            "2019-12-22 17:33:24,634 epoch 11 - iter 104/138 - loss 1.54772293 - samples/sec: 183.49\n",
            "2019-12-22 17:33:26,812 epoch 11 - iter 117/138 - loss 1.55833232 - samples/sec: 192.69\n",
            "2019-12-22 17:33:29,169 epoch 11 - iter 130/138 - loss 1.54492568 - samples/sec: 177.91\n",
            "2019-12-22 17:33:30,248 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:30,249 EPOCH 11 done: loss 1.5444 - lr 0.1000\n",
            "2019-12-22 17:33:30,250 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:33:30,251 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:30,413 epoch 12 - iter 0/138 - loss 1.05313623 - samples/sec: 2589.27\n",
            "2019-12-22 17:33:32,614 epoch 12 - iter 13/138 - loss 1.72509206 - samples/sec: 190.64\n",
            "2019-12-22 17:33:34,728 epoch 12 - iter 26/138 - loss 1.60912943 - samples/sec: 198.93\n",
            "2019-12-22 17:33:36,942 epoch 12 - iter 39/138 - loss 1.55213268 - samples/sec: 189.86\n",
            "2019-12-22 17:33:39,264 epoch 12 - iter 52/138 - loss 1.57758786 - samples/sec: 180.45\n",
            "2019-12-22 17:33:41,394 epoch 12 - iter 65/138 - loss 1.59945222 - samples/sec: 197.22\n",
            "2019-12-22 17:33:43,605 epoch 12 - iter 78/138 - loss 1.55410978 - samples/sec: 189.68\n",
            "2019-12-22 17:33:45,686 epoch 12 - iter 91/138 - loss 1.52100814 - samples/sec: 201.65\n",
            "2019-12-22 17:33:47,845 epoch 12 - iter 104/138 - loss 1.50688421 - samples/sec: 194.38\n",
            "2019-12-22 17:33:49,997 epoch 12 - iter 117/138 - loss 1.48935598 - samples/sec: 195.06\n",
            "2019-12-22 17:33:52,231 epoch 12 - iter 130/138 - loss 1.50209213 - samples/sec: 187.76\n",
            "2019-12-22 17:33:53,427 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:53,428 EPOCH 12 done: loss 1.5275 - lr 0.1000\n",
            "2019-12-22 17:33:53,429 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:33:53,430 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:33:53,612 epoch 13 - iter 0/138 - loss 1.47494662 - samples/sec: 2296.41\n",
            "2019-12-22 17:33:55,715 epoch 13 - iter 13/138 - loss 1.30302989 - samples/sec: 199.40\n",
            "2019-12-22 17:33:57,945 epoch 13 - iter 26/138 - loss 1.22688118 - samples/sec: 188.23\n",
            "2019-12-22 17:34:00,140 epoch 13 - iter 39/138 - loss 1.23914547 - samples/sec: 191.15\n",
            "2019-12-22 17:34:02,422 epoch 13 - iter 52/138 - loss 1.28637904 - samples/sec: 183.68\n",
            "2019-12-22 17:34:04,577 epoch 13 - iter 65/138 - loss 1.38777397 - samples/sec: 194.51\n",
            "2019-12-22 17:34:06,753 epoch 13 - iter 78/138 - loss 1.35918751 - samples/sec: 192.75\n",
            "2019-12-22 17:34:08,938 epoch 13 - iter 91/138 - loss 1.37952977 - samples/sec: 192.05\n",
            "2019-12-22 17:34:11,101 epoch 13 - iter 104/138 - loss 1.40417098 - samples/sec: 194.06\n",
            "2019-12-22 17:34:13,235 epoch 13 - iter 117/138 - loss 1.42774539 - samples/sec: 196.68\n",
            "2019-12-22 17:34:15,480 epoch 13 - iter 130/138 - loss 1.41896403 - samples/sec: 186.90\n",
            "2019-12-22 17:34:16,620 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:34:16,621 EPOCH 13 done: loss 1.4174 - lr 0.1000\n",
            "2019-12-22 17:34:16,621 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:34:16,622 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:34:16,791 epoch 14 - iter 0/138 - loss 1.50513768 - samples/sec: 2488.13\n",
            "2019-12-22 17:34:18,975 epoch 14 - iter 13/138 - loss 1.28754448 - samples/sec: 191.85\n",
            "2019-12-22 17:34:21,120 epoch 14 - iter 26/138 - loss 1.27440429 - samples/sec: 195.70\n",
            "2019-12-22 17:34:23,368 epoch 14 - iter 39/138 - loss 1.29203318 - samples/sec: 186.63\n",
            "2019-12-22 17:34:25,582 epoch 14 - iter 52/138 - loss 1.29730479 - samples/sec: 189.35\n",
            "2019-12-22 17:34:27,718 epoch 14 - iter 65/138 - loss 1.32066652 - samples/sec: 196.56\n",
            "2019-12-22 17:34:30,085 epoch 14 - iter 78/138 - loss 1.33758679 - samples/sec: 177.13\n",
            "2019-12-22 17:34:32,443 epoch 14 - iter 91/138 - loss 1.29591779 - samples/sec: 177.84\n",
            "2019-12-22 17:34:34,631 epoch 14 - iter 104/138 - loss 1.31509729 - samples/sec: 191.65\n",
            "2019-12-22 17:34:36,843 epoch 14 - iter 117/138 - loss 1.35409156 - samples/sec: 189.75\n",
            "2019-12-22 17:34:39,106 epoch 14 - iter 130/138 - loss 1.35438956 - samples/sec: 185.38\n",
            "2019-12-22 17:34:40,187 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:34:40,188 EPOCH 14 done: loss 1.3776 - lr 0.1000\n",
            "2019-12-22 17:34:40,189 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:34:40,190 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:34:40,374 epoch 15 - iter 0/138 - loss 2.38727856 - samples/sec: 2275.75\n",
            "2019-12-22 17:34:42,531 epoch 15 - iter 13/138 - loss 1.13070562 - samples/sec: 194.34\n",
            "2019-12-22 17:34:44,677 epoch 15 - iter 26/138 - loss 1.20863852 - samples/sec: 195.52\n",
            "2019-12-22 17:34:46,955 epoch 15 - iter 39/138 - loss 1.29120001 - samples/sec: 184.25\n",
            "2019-12-22 17:34:49,191 epoch 15 - iter 52/138 - loss 1.23310394 - samples/sec: 187.58\n",
            "2019-12-22 17:34:51,466 epoch 15 - iter 65/138 - loss 1.26126188 - samples/sec: 184.50\n",
            "2019-12-22 17:34:53,693 epoch 15 - iter 78/138 - loss 1.28102708 - samples/sec: 188.57\n",
            "2019-12-22 17:34:55,850 epoch 15 - iter 91/138 - loss 1.26427595 - samples/sec: 194.64\n",
            "2019-12-22 17:34:57,998 epoch 15 - iter 104/138 - loss 1.26492763 - samples/sec: 195.48\n",
            "2019-12-22 17:35:00,190 epoch 15 - iter 117/138 - loss 1.28517675 - samples/sec: 191.44\n",
            "2019-12-22 17:35:02,396 epoch 15 - iter 130/138 - loss 1.28479052 - samples/sec: 190.06\n",
            "2019-12-22 17:35:03,570 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:03,571 EPOCH 15 done: loss 1.2908 - lr 0.1000\n",
            "2019-12-22 17:35:03,572 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:35:03,573 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:03,748 epoch 16 - iter 0/138 - loss 1.18213272 - samples/sec: 2406.62\n",
            "2019-12-22 17:35:06,099 epoch 16 - iter 13/138 - loss 1.06606807 - samples/sec: 178.18\n",
            "2019-12-22 17:35:08,315 epoch 16 - iter 26/138 - loss 1.29139022 - samples/sec: 189.50\n",
            "2019-12-22 17:35:10,536 epoch 16 - iter 39/138 - loss 1.29255490 - samples/sec: 188.88\n",
            "2019-12-22 17:35:12,617 epoch 16 - iter 52/138 - loss 1.26706198 - samples/sec: 201.63\n",
            "2019-12-22 17:35:14,783 epoch 16 - iter 65/138 - loss 1.27785658 - samples/sec: 193.86\n",
            "2019-12-22 17:35:16,906 epoch 16 - iter 78/138 - loss 1.23727065 - samples/sec: 197.78\n",
            "2019-12-22 17:35:19,124 epoch 16 - iter 91/138 - loss 1.23383244 - samples/sec: 189.78\n",
            "2019-12-22 17:35:21,345 epoch 16 - iter 104/138 - loss 1.21452288 - samples/sec: 188.95\n",
            "2019-12-22 17:35:23,642 epoch 16 - iter 117/138 - loss 1.21886234 - samples/sec: 182.45\n",
            "2019-12-22 17:35:25,716 epoch 16 - iter 130/138 - loss 1.22111616 - samples/sec: 202.88\n",
            "2019-12-22 17:35:26,790 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:26,791 EPOCH 16 done: loss 1.2152 - lr 0.1000\n",
            "2019-12-22 17:35:26,791 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:35:26,792 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:26,989 epoch 17 - iter 0/138 - loss 1.36460423 - samples/sec: 2138.56\n",
            "2019-12-22 17:35:29,274 epoch 17 - iter 13/138 - loss 1.14320980 - samples/sec: 183.39\n",
            "2019-12-22 17:35:31,435 epoch 17 - iter 26/138 - loss 1.13040200 - samples/sec: 194.24\n",
            "2019-12-22 17:35:33,756 epoch 17 - iter 39/138 - loss 1.14036807 - samples/sec: 180.78\n",
            "2019-12-22 17:35:35,854 epoch 17 - iter 52/138 - loss 1.12861260 - samples/sec: 199.92\n",
            "2019-12-22 17:35:38,036 epoch 17 - iter 65/138 - loss 1.17652876 - samples/sec: 192.27\n",
            "2019-12-22 17:35:40,261 epoch 17 - iter 78/138 - loss 1.15650446 - samples/sec: 188.58\n",
            "2019-12-22 17:35:42,544 epoch 17 - iter 91/138 - loss 1.19138031 - samples/sec: 184.33\n",
            "2019-12-22 17:35:44,704 epoch 17 - iter 104/138 - loss 1.20212669 - samples/sec: 194.31\n",
            "2019-12-22 17:35:46,845 epoch 17 - iter 117/138 - loss 1.18291815 - samples/sec: 196.01\n",
            "2019-12-22 17:35:49,269 epoch 17 - iter 130/138 - loss 1.16544133 - samples/sec: 172.79\n",
            "2019-12-22 17:35:50,330 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:50,331 EPOCH 17 done: loss 1.1569 - lr 0.1000\n",
            "2019-12-22 17:35:50,331 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:35:50,332 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:35:50,503 epoch 18 - iter 0/138 - loss 2.66943407 - samples/sec: 2462.60\n",
            "2019-12-22 17:35:52,739 epoch 18 - iter 13/138 - loss 1.18465367 - samples/sec: 187.30\n",
            "2019-12-22 17:35:54,944 epoch 18 - iter 26/138 - loss 1.08698016 - samples/sec: 190.38\n",
            "2019-12-22 17:35:57,114 epoch 18 - iter 39/138 - loss 1.06450541 - samples/sec: 193.38\n",
            "2019-12-22 17:35:59,304 epoch 18 - iter 52/138 - loss 1.07459435 - samples/sec: 191.66\n",
            "2019-12-22 17:36:01,478 epoch 18 - iter 65/138 - loss 1.08139445 - samples/sec: 192.93\n",
            "2019-12-22 17:36:03,760 epoch 18 - iter 78/138 - loss 1.09872198 - samples/sec: 183.91\n",
            "2019-12-22 17:36:06,153 epoch 18 - iter 91/138 - loss 1.10574306 - samples/sec: 175.11\n",
            "2019-12-22 17:36:08,356 epoch 18 - iter 104/138 - loss 1.10350985 - samples/sec: 190.70\n",
            "2019-12-22 17:36:10,542 epoch 18 - iter 117/138 - loss 1.09690367 - samples/sec: 192.48\n",
            "2019-12-22 17:36:12,736 epoch 18 - iter 130/138 - loss 1.10817208 - samples/sec: 191.33\n",
            "2019-12-22 17:36:13,945 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:36:13,948 EPOCH 18 done: loss 1.1034 - lr 0.1000\n",
            "2019-12-22 17:36:13,949 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:36:13,951 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:36:14,128 epoch 19 - iter 0/138 - loss 0.61275303 - samples/sec: 2430.99\n",
            "2019-12-22 17:36:16,307 epoch 19 - iter 13/138 - loss 0.91587092 - samples/sec: 192.63\n",
            "2019-12-22 17:36:18,520 epoch 19 - iter 26/138 - loss 0.92398293 - samples/sec: 189.69\n",
            "2019-12-22 17:36:20,640 epoch 19 - iter 39/138 - loss 0.99604861 - samples/sec: 197.90\n",
            "2019-12-22 17:36:22,753 epoch 19 - iter 52/138 - loss 1.04667581 - samples/sec: 198.82\n",
            "2019-12-22 17:36:24,840 epoch 19 - iter 65/138 - loss 1.05415112 - samples/sec: 201.24\n",
            "2019-12-22 17:36:27,112 epoch 19 - iter 78/138 - loss 1.03644048 - samples/sec: 184.54\n",
            "2019-12-22 17:36:29,306 epoch 19 - iter 91/138 - loss 1.04105894 - samples/sec: 191.44\n",
            "2019-12-22 17:36:31,579 epoch 19 - iter 104/138 - loss 1.02117221 - samples/sec: 184.47\n",
            "2019-12-22 17:36:33,709 epoch 19 - iter 117/138 - loss 1.02047035 - samples/sec: 197.42\n",
            "2019-12-22 17:36:35,868 epoch 19 - iter 130/138 - loss 1.03169243 - samples/sec: 194.60\n",
            "2019-12-22 17:36:36,927 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:36:36,927 EPOCH 19 done: loss 1.0495 - lr 0.1000\n",
            "2019-12-22 17:36:36,928 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:36:36,929 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:36:37,092 epoch 20 - iter 0/138 - loss 1.25033057 - samples/sec: 2577.46\n",
            "2019-12-22 17:36:39,243 epoch 20 - iter 13/138 - loss 0.97025562 - samples/sec: 194.85\n",
            "2019-12-22 17:36:41,486 epoch 20 - iter 26/138 - loss 1.04981814 - samples/sec: 186.89\n",
            "2019-12-22 17:36:43,619 epoch 20 - iter 39/138 - loss 1.09099205 - samples/sec: 196.83\n",
            "2019-12-22 17:36:45,725 epoch 20 - iter 52/138 - loss 1.09895258 - samples/sec: 199.36\n",
            "2019-12-22 17:36:47,961 epoch 20 - iter 65/138 - loss 1.05898948 - samples/sec: 187.51\n",
            "2019-12-22 17:36:50,197 epoch 20 - iter 78/138 - loss 1.03496353 - samples/sec: 187.90\n",
            "2019-12-22 17:36:52,517 epoch 20 - iter 91/138 - loss 1.04504584 - samples/sec: 180.80\n",
            "2019-12-22 17:36:54,771 epoch 20 - iter 104/138 - loss 1.03844986 - samples/sec: 186.49\n",
            "2019-12-22 17:36:56,992 epoch 20 - iter 117/138 - loss 1.05010796 - samples/sec: 188.99\n",
            "2019-12-22 17:36:59,114 epoch 20 - iter 130/138 - loss 1.05118271 - samples/sec: 197.90\n",
            "2019-12-22 17:37:00,263 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:00,264 EPOCH 20 done: loss 1.0452 - lr 0.1000\n",
            "2019-12-22 17:37:00,265 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:37:00,266 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:00,448 epoch 21 - iter 0/138 - loss 0.73695683 - samples/sec: 2304.00\n",
            "2019-12-22 17:37:02,725 epoch 21 - iter 13/138 - loss 0.93910452 - samples/sec: 184.02\n",
            "2019-12-22 17:37:04,934 epoch 21 - iter 26/138 - loss 0.94112861 - samples/sec: 190.06\n",
            "2019-12-22 17:37:07,147 epoch 21 - iter 39/138 - loss 0.87108886 - samples/sec: 189.72\n",
            "2019-12-22 17:37:09,533 epoch 21 - iter 52/138 - loss 0.93681143 - samples/sec: 176.12\n",
            "2019-12-22 17:37:11,654 epoch 21 - iter 65/138 - loss 0.96451560 - samples/sec: 197.94\n",
            "2019-12-22 17:37:13,856 epoch 21 - iter 78/138 - loss 0.94769684 - samples/sec: 190.88\n",
            "2019-12-22 17:37:16,024 epoch 21 - iter 91/138 - loss 0.95292924 - samples/sec: 194.17\n",
            "2019-12-22 17:37:18,201 epoch 21 - iter 104/138 - loss 0.95299269 - samples/sec: 192.72\n",
            "2019-12-22 17:37:20,477 epoch 21 - iter 117/138 - loss 0.95282442 - samples/sec: 184.52\n",
            "2019-12-22 17:37:22,593 epoch 21 - iter 130/138 - loss 0.97232386 - samples/sec: 198.31\n",
            "2019-12-22 17:37:23,712 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:23,715 EPOCH 21 done: loss 0.9767 - lr 0.1000\n",
            "2019-12-22 17:37:23,716 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:37:23,717 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:23,916 epoch 22 - iter 0/138 - loss 0.62086511 - samples/sec: 2108.53\n",
            "2019-12-22 17:37:26,207 epoch 22 - iter 13/138 - loss 0.96363490 - samples/sec: 183.02\n",
            "2019-12-22 17:37:28,405 epoch 22 - iter 26/138 - loss 0.98976011 - samples/sec: 191.05\n",
            "2019-12-22 17:37:30,567 epoch 22 - iter 39/138 - loss 0.94092045 - samples/sec: 194.15\n",
            "2019-12-22 17:37:32,786 epoch 22 - iter 52/138 - loss 0.93288620 - samples/sec: 189.01\n",
            "2019-12-22 17:37:35,259 epoch 22 - iter 65/138 - loss 0.97469438 - samples/sec: 169.40\n",
            "2019-12-22 17:37:37,457 epoch 22 - iter 78/138 - loss 0.93749448 - samples/sec: 191.72\n",
            "2019-12-22 17:37:39,552 epoch 22 - iter 91/138 - loss 0.93326534 - samples/sec: 200.96\n",
            "2019-12-22 17:37:41,751 epoch 22 - iter 104/138 - loss 0.93195627 - samples/sec: 190.77\n",
            "2019-12-22 17:37:43,998 epoch 22 - iter 117/138 - loss 0.94664934 - samples/sec: 186.74\n",
            "2019-12-22 17:37:46,126 epoch 22 - iter 130/138 - loss 0.93535398 - samples/sec: 197.20\n",
            "2019-12-22 17:37:47,236 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:47,237 EPOCH 22 done: loss 0.9432 - lr 0.1000\n",
            "2019-12-22 17:37:47,238 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:37:47,239 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:37:47,433 epoch 23 - iter 0/138 - loss 2.09904766 - samples/sec: 2164.53\n",
            "2019-12-22 17:37:49,530 epoch 23 - iter 13/138 - loss 0.93166871 - samples/sec: 199.96\n",
            "2019-12-22 17:37:51,632 epoch 23 - iter 26/138 - loss 0.98683133 - samples/sec: 199.74\n",
            "2019-12-22 17:37:53,903 epoch 23 - iter 39/138 - loss 0.94134329 - samples/sec: 184.76\n",
            "2019-12-22 17:37:56,183 epoch 23 - iter 52/138 - loss 0.87814351 - samples/sec: 183.83\n",
            "2019-12-22 17:37:58,407 epoch 23 - iter 65/138 - loss 0.91835512 - samples/sec: 189.12\n",
            "2019-12-22 17:38:02,748 epoch 23 - iter 78/138 - loss 0.93489253 - samples/sec: 96.25\n",
            "2019-12-22 17:38:04,880 epoch 23 - iter 91/138 - loss 0.93942028 - samples/sec: 196.70\n",
            "2019-12-22 17:38:06,994 epoch 23 - iter 104/138 - loss 0.93075536 - samples/sec: 198.41\n",
            "2019-12-22 17:38:09,253 epoch 23 - iter 117/138 - loss 0.93095149 - samples/sec: 185.71\n",
            "2019-12-22 17:38:11,392 epoch 23 - iter 130/138 - loss 0.93892093 - samples/sec: 196.28\n",
            "2019-12-22 17:38:12,486 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:12,487 EPOCH 23 done: loss 0.9370 - lr 0.1000\n",
            "2019-12-22 17:38:12,488 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:38:12,489 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:12,676 epoch 24 - iter 0/138 - loss 0.61249638 - samples/sec: 2234.94\n",
            "2019-12-22 17:38:14,817 epoch 24 - iter 13/138 - loss 0.80489294 - samples/sec: 195.79\n",
            "2019-12-22 17:38:17,083 epoch 24 - iter 26/138 - loss 0.84763485 - samples/sec: 185.13\n",
            "2019-12-22 17:38:19,280 epoch 24 - iter 39/138 - loss 0.84397666 - samples/sec: 191.03\n",
            "2019-12-22 17:38:21,325 epoch 24 - iter 52/138 - loss 0.87527230 - samples/sec: 205.38\n",
            "2019-12-22 17:38:23,544 epoch 24 - iter 65/138 - loss 0.86803668 - samples/sec: 189.13\n",
            "2019-12-22 17:38:25,798 epoch 24 - iter 78/138 - loss 0.87177949 - samples/sec: 186.01\n",
            "2019-12-22 17:38:28,033 epoch 24 - iter 91/138 - loss 0.87280987 - samples/sec: 188.21\n",
            "2019-12-22 17:38:30,271 epoch 24 - iter 104/138 - loss 0.91533374 - samples/sec: 187.38\n",
            "2019-12-22 17:38:32,430 epoch 24 - iter 117/138 - loss 0.89468994 - samples/sec: 194.48\n",
            "2019-12-22 17:38:34,536 epoch 24 - iter 130/138 - loss 0.87684849 - samples/sec: 199.53\n",
            "2019-12-22 17:38:35,661 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:35,662 EPOCH 24 done: loss 0.8647 - lr 0.1000\n",
            "2019-12-22 17:38:35,662 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:38:35,663 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:35,853 epoch 25 - iter 0/138 - loss 1.11683977 - samples/sec: 2204.44\n",
            "2019-12-22 17:38:37,990 epoch 25 - iter 13/138 - loss 0.89773283 - samples/sec: 196.26\n",
            "2019-12-22 17:38:40,148 epoch 25 - iter 26/138 - loss 0.80644921 - samples/sec: 194.37\n",
            "2019-12-22 17:38:42,294 epoch 25 - iter 39/138 - loss 0.78138882 - samples/sec: 195.62\n",
            "2019-12-22 17:38:44,396 epoch 25 - iter 52/138 - loss 0.79100871 - samples/sec: 199.71\n",
            "2019-12-22 17:38:46,658 epoch 25 - iter 65/138 - loss 0.76291034 - samples/sec: 185.52\n",
            "2019-12-22 17:38:48,818 epoch 25 - iter 78/138 - loss 0.77883736 - samples/sec: 194.29\n",
            "2019-12-22 17:38:51,069 epoch 25 - iter 91/138 - loss 0.78784328 - samples/sec: 186.20\n",
            "2019-12-22 17:38:53,161 epoch 25 - iter 104/138 - loss 0.81305431 - samples/sec: 200.65\n",
            "2019-12-22 17:38:55,326 epoch 25 - iter 117/138 - loss 0.83241472 - samples/sec: 193.94\n",
            "2019-12-22 17:38:57,487 epoch 25 - iter 130/138 - loss 0.81884039 - samples/sec: 194.21\n",
            "2019-12-22 17:38:58,733 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:58,734 EPOCH 25 done: loss 0.8242 - lr 0.1000\n",
            "2019-12-22 17:38:58,735 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:38:58,736 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:38:58,910 epoch 26 - iter 0/138 - loss 0.37473941 - samples/sec: 2413.10\n",
            "2019-12-22 17:39:01,135 epoch 26 - iter 13/138 - loss 0.84862541 - samples/sec: 188.34\n",
            "2019-12-22 17:39:03,268 epoch 26 - iter 26/138 - loss 0.78002508 - samples/sec: 196.86\n",
            "2019-12-22 17:39:05,394 epoch 26 - iter 39/138 - loss 0.74700172 - samples/sec: 197.48\n",
            "2019-12-22 17:39:07,526 epoch 26 - iter 52/138 - loss 0.76881442 - samples/sec: 196.74\n",
            "2019-12-22 17:39:09,843 epoch 26 - iter 65/138 - loss 0.76589043 - samples/sec: 181.18\n",
            "2019-12-22 17:39:12,023 epoch 26 - iter 78/138 - loss 0.77739808 - samples/sec: 192.45\n",
            "2019-12-22 17:39:14,217 epoch 26 - iter 91/138 - loss 0.78278864 - samples/sec: 191.22\n",
            "2019-12-22 17:39:16,443 epoch 26 - iter 104/138 - loss 0.78583427 - samples/sec: 188.21\n",
            "2019-12-22 17:39:18,671 epoch 26 - iter 117/138 - loss 0.79078586 - samples/sec: 188.32\n",
            "2019-12-22 17:39:20,814 epoch 26 - iter 130/138 - loss 0.78104911 - samples/sec: 196.10\n",
            "2019-12-22 17:39:21,875 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:39:21,876 EPOCH 26 done: loss 0.7900 - lr 0.1000\n",
            "2019-12-22 17:39:21,877 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:39:21,879 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:39:22,054 epoch 27 - iter 0/138 - loss 0.58854681 - samples/sec: 2398.50\n",
            "2019-12-22 17:39:24,306 epoch 27 - iter 13/138 - loss 0.64825975 - samples/sec: 186.10\n",
            "2019-12-22 17:39:26,477 epoch 27 - iter 26/138 - loss 0.71566381 - samples/sec: 193.32\n",
            "2019-12-22 17:39:28,704 epoch 27 - iter 39/138 - loss 0.70517116 - samples/sec: 188.59\n",
            "2019-12-22 17:39:30,907 epoch 27 - iter 52/138 - loss 0.71453753 - samples/sec: 190.87\n",
            "2019-12-22 17:39:33,270 epoch 27 - iter 65/138 - loss 0.72913195 - samples/sec: 177.41\n",
            "2019-12-22 17:39:35,538 epoch 27 - iter 78/138 - loss 0.72974202 - samples/sec: 185.20\n",
            "2019-12-22 17:39:37,682 epoch 27 - iter 91/138 - loss 0.72081656 - samples/sec: 195.67\n",
            "2019-12-22 17:39:39,816 epoch 27 - iter 104/138 - loss 0.73055771 - samples/sec: 196.68\n",
            "2019-12-22 17:39:41,992 epoch 27 - iter 117/138 - loss 0.72734210 - samples/sec: 192.88\n",
            "2019-12-22 17:39:44,117 epoch 27 - iter 130/138 - loss 0.73980501 - samples/sec: 197.92\n",
            "2019-12-22 17:39:45,224 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:39:45,225 EPOCH 27 done: loss 0.7465 - lr 0.1000\n",
            "2019-12-22 17:39:45,225 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:39:45,226 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:39:45,405 epoch 28 - iter 0/138 - loss 0.75308460 - samples/sec: 2360.43\n",
            "2019-12-22 17:39:47,556 epoch 28 - iter 13/138 - loss 0.72940871 - samples/sec: 194.86\n",
            "2019-12-22 17:39:49,685 epoch 28 - iter 26/138 - loss 0.73778214 - samples/sec: 197.19\n",
            "2019-12-22 17:39:52,010 epoch 28 - iter 39/138 - loss 0.73558672 - samples/sec: 180.59\n",
            "2019-12-22 17:39:54,254 epoch 28 - iter 52/138 - loss 0.74176269 - samples/sec: 187.04\n",
            "2019-12-22 17:39:56,386 epoch 28 - iter 65/138 - loss 0.74917876 - samples/sec: 196.87\n",
            "2019-12-22 17:39:58,583 epoch 28 - iter 78/138 - loss 0.73791324 - samples/sec: 191.25\n",
            "2019-12-22 17:40:00,842 epoch 28 - iter 91/138 - loss 0.74576817 - samples/sec: 185.92\n",
            "2019-12-22 17:40:03,069 epoch 28 - iter 104/138 - loss 0.74702128 - samples/sec: 188.41\n",
            "2019-12-22 17:40:05,285 epoch 28 - iter 117/138 - loss 0.74420421 - samples/sec: 189.47\n",
            "2019-12-22 17:40:07,381 epoch 28 - iter 130/138 - loss 0.74301556 - samples/sec: 200.44\n",
            "2019-12-22 17:40:08,527 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:08,528 EPOCH 28 done: loss 0.7461 - lr 0.1000\n",
            "2019-12-22 17:40:08,529 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:40:08,529 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:08,673 epoch 29 - iter 0/138 - loss 0.84618151 - samples/sec: 2942.99\n",
            "2019-12-22 17:40:10,772 epoch 29 - iter 13/138 - loss 0.82592517 - samples/sec: 199.75\n",
            "2019-12-22 17:40:12,961 epoch 29 - iter 26/138 - loss 0.74419480 - samples/sec: 191.66\n",
            "2019-12-22 17:40:15,133 epoch 29 - iter 39/138 - loss 0.73921815 - samples/sec: 193.24\n",
            "2019-12-22 17:40:17,303 epoch 29 - iter 52/138 - loss 0.71167272 - samples/sec: 193.24\n",
            "2019-12-22 17:40:19,394 epoch 29 - iter 65/138 - loss 0.72139240 - samples/sec: 200.82\n",
            "2019-12-22 17:40:21,681 epoch 29 - iter 78/138 - loss 0.72576505 - samples/sec: 183.48\n",
            "2019-12-22 17:40:23,787 epoch 29 - iter 91/138 - loss 0.72620505 - samples/sec: 199.37\n",
            "2019-12-22 17:40:25,981 epoch 29 - iter 104/138 - loss 0.71037227 - samples/sec: 191.30\n",
            "2019-12-22 17:40:28,122 epoch 29 - iter 117/138 - loss 0.70486848 - samples/sec: 195.95\n",
            "2019-12-22 17:40:30,272 epoch 29 - iter 130/138 - loss 0.68987784 - samples/sec: 195.04\n",
            "2019-12-22 17:40:31,500 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:31,501 EPOCH 29 done: loss 0.7052 - lr 0.1000\n",
            "2019-12-22 17:40:31,504 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:40:31,505 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:31,702 epoch 30 - iter 0/138 - loss 0.62476563 - samples/sec: 2121.40\n",
            "2019-12-22 17:40:33,888 epoch 30 - iter 13/138 - loss 0.56989694 - samples/sec: 192.19\n",
            "2019-12-22 17:40:36,192 epoch 30 - iter 26/138 - loss 0.59942351 - samples/sec: 181.95\n",
            "2019-12-22 17:40:38,511 epoch 30 - iter 39/138 - loss 0.61605202 - samples/sec: 180.95\n",
            "2019-12-22 17:40:40,615 epoch 30 - iter 52/138 - loss 0.62740872 - samples/sec: 199.35\n",
            "2019-12-22 17:40:42,786 epoch 30 - iter 65/138 - loss 0.62963770 - samples/sec: 193.27\n",
            "2019-12-22 17:40:44,860 epoch 30 - iter 78/138 - loss 0.62280946 - samples/sec: 202.45\n",
            "2019-12-22 17:40:47,114 epoch 30 - iter 91/138 - loss 0.62990985 - samples/sec: 186.20\n",
            "2019-12-22 17:40:49,214 epoch 30 - iter 104/138 - loss 0.65105220 - samples/sec: 199.95\n",
            "2019-12-22 17:40:51,270 epoch 30 - iter 117/138 - loss 0.64980315 - samples/sec: 204.43\n",
            "2019-12-22 17:40:53,429 epoch 30 - iter 130/138 - loss 0.66830498 - samples/sec: 194.88\n",
            "2019-12-22 17:40:54,576 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:54,577 EPOCH 30 done: loss 0.6730 - lr 0.1000\n",
            "2019-12-22 17:40:54,578 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:40:54,579 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:40:54,729 epoch 31 - iter 0/138 - loss 0.31684348 - samples/sec: 2807.74\n",
            "2019-12-22 17:40:56,898 epoch 31 - iter 13/138 - loss 0.66684964 - samples/sec: 193.18\n",
            "2019-12-22 17:40:59,102 epoch 31 - iter 26/138 - loss 0.59209414 - samples/sec: 190.30\n",
            "2019-12-22 17:41:01,274 epoch 31 - iter 39/138 - loss 0.61533299 - samples/sec: 193.24\n",
            "2019-12-22 17:41:03,387 epoch 31 - iter 52/138 - loss 0.64786441 - samples/sec: 198.98\n",
            "2019-12-22 17:41:05,593 epoch 31 - iter 65/138 - loss 0.65460461 - samples/sec: 190.22\n",
            "2019-12-22 17:41:07,907 epoch 31 - iter 78/138 - loss 0.67193174 - samples/sec: 181.53\n",
            "2019-12-22 17:41:10,148 epoch 31 - iter 91/138 - loss 0.67205786 - samples/sec: 187.52\n",
            "2019-12-22 17:41:12,393 epoch 31 - iter 104/138 - loss 0.66258395 - samples/sec: 186.92\n",
            "2019-12-22 17:41:14,786 epoch 31 - iter 117/138 - loss 0.66077811 - samples/sec: 175.22\n",
            "2019-12-22 17:41:16,935 epoch 31 - iter 130/138 - loss 0.65750957 - samples/sec: 195.30\n",
            "2019-12-22 17:41:18,123 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:41:18,124 EPOCH 31 done: loss 0.6605 - lr 0.1000\n",
            "2019-12-22 17:41:18,125 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:41:18,126 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:41:18,278 epoch 32 - iter 0/138 - loss 0.78840309 - samples/sec: 2763.67\n",
            "2019-12-22 17:41:20,389 epoch 32 - iter 13/138 - loss 0.55355116 - samples/sec: 198.57\n",
            "2019-12-22 17:41:22,629 epoch 32 - iter 26/138 - loss 0.59899194 - samples/sec: 187.27\n",
            "2019-12-22 17:41:24,839 epoch 32 - iter 39/138 - loss 0.64869802 - samples/sec: 189.87\n",
            "2019-12-22 17:41:27,121 epoch 32 - iter 52/138 - loss 0.64248731 - samples/sec: 183.62\n",
            "2019-12-22 17:41:29,158 epoch 32 - iter 65/138 - loss 0.63286371 - samples/sec: 206.30\n",
            "2019-12-22 17:41:31,314 epoch 32 - iter 78/138 - loss 0.64230112 - samples/sec: 194.81\n",
            "2019-12-22 17:41:33,571 epoch 32 - iter 91/138 - loss 0.64835388 - samples/sec: 186.02\n",
            "2019-12-22 17:41:35,877 epoch 32 - iter 104/138 - loss 0.64527755 - samples/sec: 181.87\n",
            "2019-12-22 17:41:38,222 epoch 32 - iter 117/138 - loss 0.65799331 - samples/sec: 178.88\n",
            "2019-12-22 17:41:40,391 epoch 32 - iter 130/138 - loss 0.65026045 - samples/sec: 193.40\n",
            "2019-12-22 17:41:41,505 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:41:41,506 EPOCH 32 done: loss 0.6509 - lr 0.1000\n",
            "2019-12-22 17:41:41,507 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:41:41,508 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:41:41,692 epoch 33 - iter 0/138 - loss 0.76644301 - samples/sec: 2274.73\n",
            "2019-12-22 17:41:43,800 epoch 33 - iter 13/138 - loss 0.46853748 - samples/sec: 199.01\n",
            "2019-12-22 17:41:45,951 epoch 33 - iter 26/138 - loss 0.49605208 - samples/sec: 195.71\n",
            "2019-12-22 17:41:48,074 epoch 33 - iter 39/138 - loss 0.54750295 - samples/sec: 197.85\n",
            "2019-12-22 17:41:50,228 epoch 33 - iter 52/138 - loss 0.61200625 - samples/sec: 195.00\n",
            "2019-12-22 17:41:52,394 epoch 33 - iter 65/138 - loss 0.61418440 - samples/sec: 193.57\n",
            "2019-12-22 17:41:54,619 epoch 33 - iter 78/138 - loss 0.61837684 - samples/sec: 188.46\n",
            "2019-12-22 17:41:56,960 epoch 33 - iter 91/138 - loss 0.62067429 - samples/sec: 179.10\n",
            "2019-12-22 17:41:59,196 epoch 33 - iter 104/138 - loss 0.62105417 - samples/sec: 188.30\n",
            "2019-12-22 17:42:01,329 epoch 33 - iter 117/138 - loss 0.61728206 - samples/sec: 196.58\n",
            "2019-12-22 17:42:03,593 epoch 33 - iter 130/138 - loss 0.63233594 - samples/sec: 185.27\n",
            "2019-12-22 17:42:04,723 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:04,724 EPOCH 33 done: loss 0.6339 - lr 0.1000\n",
            "2019-12-22 17:42:04,728 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:42:04,732 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:04,886 epoch 34 - iter 0/138 - loss 1.05569005 - samples/sec: 2743.85\n",
            "2019-12-22 17:42:07,094 epoch 34 - iter 13/138 - loss 0.61313177 - samples/sec: 189.82\n",
            "2019-12-22 17:42:09,379 epoch 34 - iter 26/138 - loss 0.61540599 - samples/sec: 183.58\n",
            "2019-12-22 17:42:11,471 epoch 34 - iter 39/138 - loss 0.61246531 - samples/sec: 200.94\n",
            "2019-12-22 17:42:13,636 epoch 34 - iter 52/138 - loss 0.60964047 - samples/sec: 193.83\n",
            "2019-12-22 17:42:15,798 epoch 34 - iter 65/138 - loss 0.60992725 - samples/sec: 194.40\n",
            "2019-12-22 17:42:18,561 epoch 34 - iter 78/138 - loss 0.59240513 - samples/sec: 151.53\n",
            "2019-12-22 17:42:20,715 epoch 34 - iter 91/138 - loss 0.58785130 - samples/sec: 194.90\n",
            "2019-12-22 17:42:22,930 epoch 34 - iter 104/138 - loss 0.59579976 - samples/sec: 189.43\n",
            "2019-12-22 17:42:25,230 epoch 34 - iter 117/138 - loss 0.60389344 - samples/sec: 182.85\n",
            "2019-12-22 17:42:27,397 epoch 34 - iter 130/138 - loss 0.60628137 - samples/sec: 193.57\n",
            "2019-12-22 17:42:28,698 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:28,701 EPOCH 34 done: loss 0.6106 - lr 0.1000\n",
            "2019-12-22 17:42:28,703 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:42:28,705 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:28,866 epoch 35 - iter 0/138 - loss 0.29788357 - samples/sec: 2675.73\n",
            "2019-12-22 17:42:31,068 epoch 35 - iter 13/138 - loss 0.56307557 - samples/sec: 190.43\n",
            "2019-12-22 17:42:33,200 epoch 35 - iter 26/138 - loss 0.57781091 - samples/sec: 196.91\n",
            "2019-12-22 17:42:35,402 epoch 35 - iter 39/138 - loss 0.58760447 - samples/sec: 190.53\n",
            "2019-12-22 17:42:37,715 epoch 35 - iter 52/138 - loss 0.56804337 - samples/sec: 181.18\n",
            "2019-12-22 17:42:39,915 epoch 35 - iter 65/138 - loss 0.61348390 - samples/sec: 190.76\n",
            "2019-12-22 17:42:42,142 epoch 35 - iter 78/138 - loss 0.60320101 - samples/sec: 188.40\n",
            "2019-12-22 17:42:44,304 epoch 35 - iter 91/138 - loss 0.60961940 - samples/sec: 194.07\n",
            "2019-12-22 17:42:46,446 epoch 35 - iter 104/138 - loss 0.59899198 - samples/sec: 196.46\n",
            "2019-12-22 17:42:48,581 epoch 35 - iter 117/138 - loss 0.58725007 - samples/sec: 196.51\n",
            "2019-12-22 17:42:50,834 epoch 35 - iter 130/138 - loss 0.58733682 - samples/sec: 186.30\n",
            "2019-12-22 17:42:51,870 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:51,871 EPOCH 35 done: loss 0.5916 - lr 0.1000\n",
            "2019-12-22 17:42:51,872 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:42:51,874 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:42:52,061 epoch 36 - iter 0/138 - loss 0.32696617 - samples/sec: 2259.86\n",
            "2019-12-22 17:42:54,098 epoch 36 - iter 13/138 - loss 0.50221378 - samples/sec: 205.97\n",
            "2019-12-22 17:42:56,286 epoch 36 - iter 26/138 - loss 0.50750605 - samples/sec: 192.12\n",
            "2019-12-22 17:42:58,543 epoch 36 - iter 39/138 - loss 0.54133713 - samples/sec: 186.07\n",
            "2019-12-22 17:43:00,864 epoch 36 - iter 52/138 - loss 0.55764992 - samples/sec: 180.77\n",
            "2019-12-22 17:43:03,002 epoch 36 - iter 65/138 - loss 0.54798493 - samples/sec: 196.35\n",
            "2019-12-22 17:43:05,148 epoch 36 - iter 78/138 - loss 0.55307804 - samples/sec: 195.54\n",
            "2019-12-22 17:43:07,306 epoch 36 - iter 91/138 - loss 0.55296621 - samples/sec: 194.31\n",
            "2019-12-22 17:43:09,443 epoch 36 - iter 104/138 - loss 0.54542026 - samples/sec: 196.24\n",
            "2019-12-22 17:43:11,516 epoch 36 - iter 117/138 - loss 0.54877466 - samples/sec: 202.79\n",
            "2019-12-22 17:43:13,760 epoch 36 - iter 130/138 - loss 0.56517914 - samples/sec: 187.10\n",
            "2019-12-22 17:43:14,970 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:43:14,971 EPOCH 36 done: loss 0.5578 - lr 0.1000\n",
            "2019-12-22 17:43:14,972 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:43:14,973 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:43:15,140 epoch 37 - iter 0/138 - loss 0.58787113 - samples/sec: 2517.15\n",
            "2019-12-22 17:43:17,447 epoch 37 - iter 13/138 - loss 0.55295789 - samples/sec: 181.57\n",
            "2019-12-22 17:43:19,553 epoch 37 - iter 26/138 - loss 0.47163865 - samples/sec: 199.38\n",
            "2019-12-22 17:43:21,819 epoch 37 - iter 39/138 - loss 0.49129370 - samples/sec: 185.04\n",
            "2019-12-22 17:43:23,983 epoch 37 - iter 52/138 - loss 0.52826460 - samples/sec: 194.19\n",
            "2019-12-22 17:43:26,180 epoch 37 - iter 65/138 - loss 0.51264356 - samples/sec: 190.72\n",
            "2019-12-22 17:43:28,384 epoch 37 - iter 78/138 - loss 0.53486200 - samples/sec: 190.18\n",
            "2019-12-22 17:43:30,531 epoch 37 - iter 91/138 - loss 0.53468994 - samples/sec: 195.30\n",
            "2019-12-22 17:43:32,658 epoch 37 - iter 104/138 - loss 0.53807440 - samples/sec: 197.19\n",
            "2019-12-22 17:43:34,887 epoch 37 - iter 117/138 - loss 0.54913722 - samples/sec: 188.05\n",
            "2019-12-22 17:43:37,100 epoch 37 - iter 130/138 - loss 0.55587453 - samples/sec: 189.36\n",
            "2019-12-22 17:43:38,211 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:43:38,212 EPOCH 37 done: loss 0.5605 - lr 0.1000\n",
            "2019-12-22 17:43:38,213 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:43:38,214 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:43:38,405 epoch 38 - iter 0/138 - loss 0.35484529 - samples/sec: 2195.56\n",
            "2019-12-22 17:43:40,637 epoch 38 - iter 13/138 - loss 0.52613617 - samples/sec: 187.75\n",
            "2019-12-22 17:43:42,780 epoch 38 - iter 26/138 - loss 0.57692347 - samples/sec: 195.58\n",
            "2019-12-22 17:43:44,990 epoch 38 - iter 39/138 - loss 0.57235794 - samples/sec: 189.71\n",
            "2019-12-22 17:43:47,208 epoch 38 - iter 52/138 - loss 0.56036466 - samples/sec: 188.98\n",
            "2019-12-22 17:43:49,370 epoch 38 - iter 65/138 - loss 0.55509158 - samples/sec: 193.93\n",
            "2019-12-22 17:43:51,611 epoch 38 - iter 78/138 - loss 0.56039225 - samples/sec: 187.03\n",
            "2019-12-22 17:43:53,871 epoch 38 - iter 91/138 - loss 0.56020924 - samples/sec: 185.50\n",
            "2019-12-22 17:43:55,959 epoch 38 - iter 104/138 - loss 0.55980687 - samples/sec: 200.81\n",
            "2019-12-22 17:43:58,130 epoch 38 - iter 117/138 - loss 0.55406385 - samples/sec: 193.05\n",
            "2019-12-22 17:44:00,310 epoch 38 - iter 130/138 - loss 0.54858874 - samples/sec: 192.45\n",
            "2019-12-22 17:44:01,439 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:01,440 EPOCH 38 done: loss 0.5437 - lr 0.1000\n",
            "2019-12-22 17:44:01,441 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:44:01,442 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:01,638 epoch 39 - iter 0/138 - loss 0.31836718 - samples/sec: 2132.46\n",
            "2019-12-22 17:44:03,792 epoch 39 - iter 13/138 - loss 0.48817781 - samples/sec: 194.71\n",
            "2019-12-22 17:44:05,985 epoch 39 - iter 26/138 - loss 0.47239713 - samples/sec: 191.15\n",
            "2019-12-22 17:44:08,181 epoch 39 - iter 39/138 - loss 0.46859604 - samples/sec: 190.90\n",
            "2019-12-22 17:44:10,389 epoch 39 - iter 52/138 - loss 0.43901187 - samples/sec: 189.84\n",
            "2019-12-22 17:44:12,648 epoch 39 - iter 65/138 - loss 0.45030880 - samples/sec: 185.45\n",
            "2019-12-22 17:44:14,839 epoch 39 - iter 78/138 - loss 0.47654683 - samples/sec: 191.48\n",
            "2019-12-22 17:44:17,211 epoch 39 - iter 91/138 - loss 0.47270109 - samples/sec: 177.04\n",
            "2019-12-22 17:44:19,358 epoch 39 - iter 104/138 - loss 0.48826284 - samples/sec: 195.76\n",
            "2019-12-22 17:44:21,439 epoch 39 - iter 117/138 - loss 0.49073303 - samples/sec: 201.71\n",
            "2019-12-22 17:44:23,542 epoch 39 - iter 130/138 - loss 0.49127185 - samples/sec: 199.48\n",
            "2019-12-22 17:44:24,623 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:24,624 EPOCH 39 done: loss 0.4864 - lr 0.1000\n",
            "2019-12-22 17:44:24,625 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:44:24,626 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:24,768 epoch 40 - iter 0/138 - loss 0.88828707 - samples/sec: 2948.54\n",
            "2019-12-22 17:44:26,953 epoch 40 - iter 13/138 - loss 0.43559466 - samples/sec: 191.91\n",
            "2019-12-22 17:44:29,094 epoch 40 - iter 26/138 - loss 0.45121949 - samples/sec: 196.05\n",
            "2019-12-22 17:44:31,304 epoch 40 - iter 39/138 - loss 0.48837765 - samples/sec: 189.77\n",
            "2019-12-22 17:44:33,564 epoch 40 - iter 52/138 - loss 0.50188676 - samples/sec: 185.83\n",
            "2019-12-22 17:44:35,713 epoch 40 - iter 65/138 - loss 0.53571442 - samples/sec: 195.39\n",
            "2019-12-22 17:44:37,884 epoch 40 - iter 78/138 - loss 0.53147726 - samples/sec: 193.25\n",
            "2019-12-22 17:44:40,161 epoch 40 - iter 91/138 - loss 0.52396960 - samples/sec: 185.15\n",
            "2019-12-22 17:44:42,253 epoch 40 - iter 104/138 - loss 0.51599314 - samples/sec: 200.54\n",
            "2019-12-22 17:44:44,488 epoch 40 - iter 117/138 - loss 0.50641005 - samples/sec: 187.73\n",
            "2019-12-22 17:44:46,676 epoch 40 - iter 130/138 - loss 0.50546111 - samples/sec: 191.86\n",
            "2019-12-22 17:44:47,785 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:47,786 EPOCH 40 done: loss 0.5025 - lr 0.1000\n",
            "2019-12-22 17:44:47,787 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:44:47,788 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:44:47,993 epoch 41 - iter 0/138 - loss 0.27454066 - samples/sec: 2041.37\n",
            "2019-12-22 17:44:50,105 epoch 41 - iter 13/138 - loss 0.37775119 - samples/sec: 198.53\n",
            "2019-12-22 17:44:52,270 epoch 41 - iter 26/138 - loss 0.44235615 - samples/sec: 193.86\n",
            "2019-12-22 17:44:54,414 epoch 41 - iter 39/138 - loss 0.46815926 - samples/sec: 196.06\n",
            "2019-12-22 17:44:56,546 epoch 41 - iter 52/138 - loss 0.48547698 - samples/sec: 197.12\n",
            "2019-12-22 17:44:58,751 epoch 41 - iter 65/138 - loss 0.47346868 - samples/sec: 190.42\n",
            "2019-12-22 17:45:00,914 epoch 41 - iter 78/138 - loss 0.46905287 - samples/sec: 194.12\n",
            "2019-12-22 17:45:03,112 epoch 41 - iter 91/138 - loss 0.47252993 - samples/sec: 190.97\n",
            "2019-12-22 17:45:05,444 epoch 41 - iter 104/138 - loss 0.46978165 - samples/sec: 179.86\n",
            "2019-12-22 17:45:07,616 epoch 41 - iter 117/138 - loss 0.46758553 - samples/sec: 193.37\n",
            "2019-12-22 17:45:09,813 epoch 41 - iter 130/138 - loss 0.46159129 - samples/sec: 191.57\n",
            "2019-12-22 17:45:10,987 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:10,988 EPOCH 41 done: loss 0.4700 - lr 0.1000\n",
            "2019-12-22 17:45:10,993 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:45:10,994 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:11,169 epoch 42 - iter 0/138 - loss 0.66173989 - samples/sec: 2409.20\n",
            "2019-12-22 17:45:13,376 epoch 42 - iter 13/138 - loss 0.40538560 - samples/sec: 190.26\n",
            "2019-12-22 17:45:15,617 epoch 42 - iter 26/138 - loss 0.46659730 - samples/sec: 187.30\n",
            "2019-12-22 17:45:17,757 epoch 42 - iter 39/138 - loss 0.44082556 - samples/sec: 196.13\n",
            "2019-12-22 17:45:19,933 epoch 42 - iter 52/138 - loss 0.44357285 - samples/sec: 192.91\n",
            "2019-12-22 17:45:22,136 epoch 42 - iter 65/138 - loss 0.45127065 - samples/sec: 190.76\n",
            "2019-12-22 17:45:24,286 epoch 42 - iter 78/138 - loss 0.45334217 - samples/sec: 195.02\n",
            "2019-12-22 17:45:26,465 epoch 42 - iter 91/138 - loss 0.45856210 - samples/sec: 192.84\n",
            "2019-12-22 17:45:28,692 epoch 42 - iter 104/138 - loss 0.45800670 - samples/sec: 188.38\n",
            "2019-12-22 17:45:30,834 epoch 42 - iter 117/138 - loss 0.45477640 - samples/sec: 196.22\n",
            "2019-12-22 17:45:33,138 epoch 42 - iter 130/138 - loss 0.46780230 - samples/sec: 181.97\n",
            "2019-12-22 17:45:34,274 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:34,275 EPOCH 42 done: loss 0.4657 - lr 0.1000\n",
            "2019-12-22 17:45:34,279 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:45:34,281 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:34,464 epoch 43 - iter 0/138 - loss 0.36705995 - samples/sec: 2303.21\n",
            "2019-12-22 17:45:36,593 epoch 43 - iter 13/138 - loss 0.44647379 - samples/sec: 197.09\n",
            "2019-12-22 17:45:38,761 epoch 43 - iter 26/138 - loss 0.43021531 - samples/sec: 193.50\n",
            "2019-12-22 17:45:40,866 epoch 43 - iter 39/138 - loss 0.46656912 - samples/sec: 199.48\n",
            "2019-12-22 17:45:43,078 epoch 43 - iter 52/138 - loss 0.46707619 - samples/sec: 189.75\n",
            "2019-12-22 17:45:45,295 epoch 43 - iter 65/138 - loss 0.44885511 - samples/sec: 189.11\n",
            "2019-12-22 17:45:47,410 epoch 43 - iter 78/138 - loss 0.45589803 - samples/sec: 198.40\n",
            "2019-12-22 17:45:49,587 epoch 43 - iter 91/138 - loss 0.46190493 - samples/sec: 192.56\n",
            "2019-12-22 17:45:51,815 epoch 43 - iter 104/138 - loss 0.45926846 - samples/sec: 188.38\n",
            "2019-12-22 17:45:53,970 epoch 43 - iter 117/138 - loss 0.45628027 - samples/sec: 194.62\n",
            "2019-12-22 17:45:56,117 epoch 43 - iter 130/138 - loss 0.45425277 - samples/sec: 195.38\n",
            "2019-12-22 17:45:57,331 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:57,332 EPOCH 43 done: loss 0.4548 - lr 0.1000\n",
            "2019-12-22 17:45:57,335 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:45:57,339 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:45:57,516 epoch 44 - iter 0/138 - loss 0.30512393 - samples/sec: 2375.17\n",
            "2019-12-22 17:45:59,646 epoch 44 - iter 13/138 - loss 0.44460317 - samples/sec: 197.02\n",
            "2019-12-22 17:46:01,947 epoch 44 - iter 26/138 - loss 0.42359517 - samples/sec: 182.13\n",
            "2019-12-22 17:46:04,092 epoch 44 - iter 39/138 - loss 0.40714995 - samples/sec: 195.84\n",
            "2019-12-22 17:46:06,364 epoch 44 - iter 52/138 - loss 0.40607942 - samples/sec: 184.76\n",
            "2019-12-22 17:46:08,550 epoch 44 - iter 65/138 - loss 0.44610482 - samples/sec: 191.82\n",
            "2019-12-22 17:46:10,766 epoch 44 - iter 78/138 - loss 0.45404372 - samples/sec: 189.18\n",
            "2019-12-22 17:46:12,983 epoch 44 - iter 91/138 - loss 0.46608338 - samples/sec: 189.13\n",
            "2019-12-22 17:46:15,107 epoch 44 - iter 104/138 - loss 0.46853657 - samples/sec: 197.67\n",
            "2019-12-22 17:46:17,363 epoch 44 - iter 117/138 - loss 0.46311384 - samples/sec: 185.84\n",
            "2019-12-22 17:46:19,519 epoch 44 - iter 130/138 - loss 0.47057343 - samples/sec: 194.49\n",
            "2019-12-22 17:46:20,664 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:46:20,669 EPOCH 44 done: loss 0.4699 - lr 0.1000\n",
            "2019-12-22 17:46:20,673 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:46:20,678 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:46:20,878 epoch 45 - iter 0/138 - loss 0.37681270 - samples/sec: 2083.89\n",
            "2019-12-22 17:46:23,204 epoch 45 - iter 13/138 - loss 0.38820687 - samples/sec: 180.43\n",
            "2019-12-22 17:46:25,501 epoch 45 - iter 26/138 - loss 0.39446460 - samples/sec: 182.85\n",
            "2019-12-22 17:46:27,661 epoch 45 - iter 39/138 - loss 0.42416122 - samples/sec: 194.16\n",
            "2019-12-22 17:46:30,000 epoch 45 - iter 52/138 - loss 0.43313746 - samples/sec: 179.45\n",
            "2019-12-22 17:46:32,176 epoch 45 - iter 65/138 - loss 0.43254695 - samples/sec: 192.92\n",
            "2019-12-22 17:46:34,331 epoch 45 - iter 78/138 - loss 0.43445900 - samples/sec: 194.74\n",
            "2019-12-22 17:46:36,680 epoch 45 - iter 91/138 - loss 0.44141771 - samples/sec: 178.63\n",
            "2019-12-22 17:46:38,881 epoch 45 - iter 104/138 - loss 0.45115841 - samples/sec: 190.55\n",
            "2019-12-22 17:46:40,890 epoch 45 - iter 117/138 - loss 0.44761933 - samples/sec: 209.03\n",
            "2019-12-22 17:46:43,094 epoch 45 - iter 130/138 - loss 0.44473857 - samples/sec: 190.47\n",
            "2019-12-22 17:46:44,212 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:46:44,213 EPOCH 45 done: loss 0.4462 - lr 0.1000\n",
            "2019-12-22 17:46:44,214 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:46:44,215 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:46:44,385 epoch 46 - iter 0/138 - loss 0.23117462 - samples/sec: 2476.25\n",
            "2019-12-22 17:46:46,527 epoch 46 - iter 13/138 - loss 0.37917070 - samples/sec: 195.81\n",
            "2019-12-22 17:46:48,724 epoch 46 - iter 26/138 - loss 0.34548817 - samples/sec: 191.10\n",
            "2019-12-22 17:46:51,031 epoch 46 - iter 39/138 - loss 0.39885158 - samples/sec: 181.89\n",
            "2019-12-22 17:46:53,200 epoch 46 - iter 52/138 - loss 0.41172652 - samples/sec: 193.49\n",
            "2019-12-22 17:46:55,323 epoch 46 - iter 65/138 - loss 0.39561083 - samples/sec: 197.72\n",
            "2019-12-22 17:46:57,491 epoch 46 - iter 78/138 - loss 0.40957453 - samples/sec: 193.83\n",
            "2019-12-22 17:47:00,044 epoch 46 - iter 91/138 - loss 0.41563989 - samples/sec: 164.18\n",
            "2019-12-22 17:47:02,250 epoch 46 - iter 104/138 - loss 0.42149848 - samples/sec: 190.36\n",
            "2019-12-22 17:47:04,477 epoch 46 - iter 117/138 - loss 0.42443610 - samples/sec: 188.90\n",
            "2019-12-22 17:47:06,541 epoch 46 - iter 130/138 - loss 0.41683427 - samples/sec: 203.50\n",
            "2019-12-22 17:47:07,664 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:07,665 EPOCH 46 done: loss 0.4142 - lr 0.1000\n",
            "2019-12-22 17:47:07,673 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:47:07,674 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:07,859 epoch 47 - iter 0/138 - loss 0.40205121 - samples/sec: 2318.52\n",
            "2019-12-22 17:47:09,964 epoch 47 - iter 13/138 - loss 0.42243441 - samples/sec: 199.34\n",
            "2019-12-22 17:47:12,096 epoch 47 - iter 26/138 - loss 0.44027055 - samples/sec: 196.96\n",
            "2019-12-22 17:47:14,243 epoch 47 - iter 39/138 - loss 0.46280419 - samples/sec: 195.55\n",
            "2019-12-22 17:47:16,463 epoch 47 - iter 52/138 - loss 0.46869193 - samples/sec: 189.10\n",
            "2019-12-22 17:47:18,683 epoch 47 - iter 65/138 - loss 0.45466969 - samples/sec: 189.09\n",
            "2019-12-22 17:47:20,890 epoch 47 - iter 78/138 - loss 0.45719259 - samples/sec: 189.95\n",
            "2019-12-22 17:47:23,009 epoch 47 - iter 91/138 - loss 0.45795754 - samples/sec: 198.18\n",
            "2019-12-22 17:47:25,156 epoch 47 - iter 104/138 - loss 0.45919334 - samples/sec: 196.21\n",
            "2019-12-22 17:47:27,283 epoch 47 - iter 117/138 - loss 0.45795461 - samples/sec: 197.34\n",
            "2019-12-22 17:47:29,453 epoch 47 - iter 130/138 - loss 0.45335885 - samples/sec: 193.77\n",
            "2019-12-22 17:47:30,585 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:30,586 EPOCH 47 done: loss 0.4501 - lr 0.1000\n",
            "2019-12-22 17:47:30,586 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:47:30,587 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:30,738 epoch 48 - iter 0/138 - loss 0.55583930 - samples/sec: 2788.45\n",
            "2019-12-22 17:47:32,888 epoch 48 - iter 13/138 - loss 0.41393678 - samples/sec: 194.97\n",
            "2019-12-22 17:47:35,002 epoch 48 - iter 26/138 - loss 0.38158966 - samples/sec: 198.72\n",
            "2019-12-22 17:47:37,250 epoch 48 - iter 39/138 - loss 0.38144592 - samples/sec: 186.62\n",
            "2019-12-22 17:47:39,433 epoch 48 - iter 52/138 - loss 0.36372570 - samples/sec: 192.24\n",
            "2019-12-22 17:47:41,586 epoch 48 - iter 65/138 - loss 0.36610781 - samples/sec: 195.27\n",
            "2019-12-22 17:47:43,717 epoch 48 - iter 78/138 - loss 0.40195719 - samples/sec: 197.06\n",
            "2019-12-22 17:47:45,841 epoch 48 - iter 91/138 - loss 0.39282676 - samples/sec: 197.78\n",
            "2019-12-22 17:47:48,092 epoch 48 - iter 104/138 - loss 0.39283545 - samples/sec: 186.41\n",
            "2019-12-22 17:47:50,253 epoch 48 - iter 117/138 - loss 0.38823213 - samples/sec: 194.07\n",
            "2019-12-22 17:47:52,349 epoch 48 - iter 130/138 - loss 0.39306571 - samples/sec: 200.68\n",
            "2019-12-22 17:47:53,530 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:53,531 EPOCH 48 done: loss 0.3933 - lr 0.1000\n",
            "2019-12-22 17:47:53,531 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:47:53,532 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:47:53,678 epoch 49 - iter 0/138 - loss 0.26848543 - samples/sec: 2883.69\n",
            "2019-12-22 17:47:55,769 epoch 49 - iter 13/138 - loss 0.39848445 - samples/sec: 200.58\n",
            "2019-12-22 17:47:57,964 epoch 49 - iter 26/138 - loss 0.36458269 - samples/sec: 191.21\n",
            "2019-12-22 17:48:00,169 epoch 49 - iter 39/138 - loss 0.38374927 - samples/sec: 190.34\n",
            "2019-12-22 17:48:02,550 epoch 49 - iter 52/138 - loss 0.36909419 - samples/sec: 176.25\n",
            "2019-12-22 17:48:04,654 epoch 49 - iter 65/138 - loss 0.36701539 - samples/sec: 199.58\n",
            "2019-12-22 17:48:06,845 epoch 49 - iter 78/138 - loss 0.36336991 - samples/sec: 191.74\n",
            "2019-12-22 17:48:09,057 epoch 49 - iter 91/138 - loss 0.37908961 - samples/sec: 190.86\n",
            "2019-12-22 17:48:11,258 epoch 49 - iter 104/138 - loss 0.38278108 - samples/sec: 190.80\n",
            "2019-12-22 17:48:13,462 epoch 49 - iter 117/138 - loss 0.38032352 - samples/sec: 191.48\n",
            "2019-12-22 17:48:15,656 epoch 49 - iter 130/138 - loss 0.39050820 - samples/sec: 191.35\n",
            "2019-12-22 17:48:16,741 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:48:16,742 EPOCH 49 done: loss 0.3888 - lr 0.1000\n",
            "2019-12-22 17:48:16,746 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:48:16,747 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:48:16,918 epoch 50 - iter 0/138 - loss 0.21994042 - samples/sec: 2461.12\n",
            "2019-12-22 17:48:19,091 epoch 50 - iter 13/138 - loss 0.36508736 - samples/sec: 193.55\n",
            "2019-12-22 17:48:21,303 epoch 50 - iter 26/138 - loss 0.42178285 - samples/sec: 189.75\n",
            "2019-12-22 17:48:23,503 epoch 50 - iter 39/138 - loss 0.45532248 - samples/sec: 190.92\n",
            "2019-12-22 17:48:25,577 epoch 50 - iter 52/138 - loss 0.44744559 - samples/sec: 202.67\n",
            "2019-12-22 17:48:27,690 epoch 50 - iter 65/138 - loss 0.42368917 - samples/sec: 198.72\n",
            "2019-12-22 17:48:29,995 epoch 50 - iter 78/138 - loss 0.41728339 - samples/sec: 182.04\n",
            "2019-12-22 17:48:32,195 epoch 50 - iter 91/138 - loss 0.42143289 - samples/sec: 190.87\n",
            "2019-12-22 17:48:34,406 epoch 50 - iter 104/138 - loss 0.41163779 - samples/sec: 189.84\n",
            "2019-12-22 17:48:36,629 epoch 50 - iter 117/138 - loss 0.40393304 - samples/sec: 188.87\n",
            "2019-12-22 17:48:38,883 epoch 50 - iter 130/138 - loss 0.41263963 - samples/sec: 186.30\n",
            "2019-12-22 17:48:39,995 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:48:39,996 EPOCH 50 done: loss 0.4113 - lr 0.1000\n",
            "2019-12-22 17:48:39,999 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:48:40,001 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:48:40,198 epoch 51 - iter 0/138 - loss 0.56419539 - samples/sec: 2139.72\n",
            "2019-12-22 17:48:42,387 epoch 51 - iter 13/138 - loss 0.35456134 - samples/sec: 191.60\n",
            "2019-12-22 17:48:44,520 epoch 51 - iter 26/138 - loss 0.36947109 - samples/sec: 197.08\n",
            "2019-12-22 17:48:46,675 epoch 51 - iter 39/138 - loss 0.36937912 - samples/sec: 194.77\n",
            "2019-12-22 17:48:48,889 epoch 51 - iter 52/138 - loss 0.34663563 - samples/sec: 189.57\n",
            "2019-12-22 17:48:50,999 epoch 51 - iter 65/138 - loss 0.35498773 - samples/sec: 199.02\n",
            "2019-12-22 17:48:53,325 epoch 51 - iter 78/138 - loss 0.35340260 - samples/sec: 180.33\n",
            "2019-12-22 17:48:55,514 epoch 51 - iter 91/138 - loss 0.34808710 - samples/sec: 192.45\n",
            "2019-12-22 17:48:57,759 epoch 51 - iter 104/138 - loss 0.35800817 - samples/sec: 187.34\n",
            "2019-12-22 17:48:59,988 epoch 51 - iter 117/138 - loss 0.36986972 - samples/sec: 188.34\n",
            "2019-12-22 17:49:02,186 epoch 51 - iter 130/138 - loss 0.38029733 - samples/sec: 191.10\n",
            "2019-12-22 17:49:03,320 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:03,321 EPOCH 51 done: loss 0.3884 - lr 0.1000\n",
            "2019-12-22 17:49:03,322 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:49:03,323 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:03,527 epoch 52 - iter 0/138 - loss 0.36382830 - samples/sec: 2050.26\n",
            "2019-12-22 17:49:05,699 epoch 52 - iter 13/138 - loss 0.45036307 - samples/sec: 193.16\n",
            "2019-12-22 17:49:07,907 epoch 52 - iter 26/138 - loss 0.40105997 - samples/sec: 190.04\n",
            "2019-12-22 17:49:10,119 epoch 52 - iter 39/138 - loss 0.37996151 - samples/sec: 189.80\n",
            "2019-12-22 17:49:12,256 epoch 52 - iter 52/138 - loss 0.38887075 - samples/sec: 196.66\n",
            "2019-12-22 17:49:14,415 epoch 52 - iter 65/138 - loss 0.37435135 - samples/sec: 194.39\n",
            "2019-12-22 17:49:16,509 epoch 52 - iter 78/138 - loss 0.38379304 - samples/sec: 200.56\n",
            "2019-12-22 17:49:18,629 epoch 52 - iter 91/138 - loss 0.37648153 - samples/sec: 198.32\n",
            "2019-12-22 17:49:20,726 epoch 52 - iter 104/138 - loss 0.37364590 - samples/sec: 200.27\n",
            "2019-12-22 17:49:23,075 epoch 52 - iter 117/138 - loss 0.37039798 - samples/sec: 178.65\n",
            "2019-12-22 17:49:25,261 epoch 52 - iter 130/138 - loss 0.37133546 - samples/sec: 192.43\n",
            "2019-12-22 17:49:26,384 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:26,385 EPOCH 52 done: loss 0.3706 - lr 0.1000\n",
            "2019-12-22 17:49:26,388 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:49:26,391 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:26,564 epoch 53 - iter 0/138 - loss 0.29384342 - samples/sec: 2420.21\n",
            "2019-12-22 17:49:28,618 epoch 53 - iter 13/138 - loss 0.27144605 - samples/sec: 204.37\n",
            "2019-12-22 17:49:30,808 epoch 53 - iter 26/138 - loss 0.31852345 - samples/sec: 191.66\n",
            "2019-12-22 17:49:32,961 epoch 53 - iter 39/138 - loss 0.33765914 - samples/sec: 195.01\n",
            "2019-12-22 17:49:35,258 epoch 53 - iter 52/138 - loss 0.34663141 - samples/sec: 182.65\n",
            "2019-12-22 17:49:37,424 epoch 53 - iter 65/138 - loss 0.34739526 - samples/sec: 193.90\n",
            "2019-12-22 17:49:39,773 epoch 53 - iter 78/138 - loss 0.35291043 - samples/sec: 178.55\n",
            "2019-12-22 17:49:41,853 epoch 53 - iter 91/138 - loss 0.35506473 - samples/sec: 202.22\n",
            "2019-12-22 17:49:44,024 epoch 53 - iter 104/138 - loss 0.35147570 - samples/sec: 193.45\n",
            "2019-12-22 17:49:46,082 epoch 53 - iter 117/138 - loss 0.34292897 - samples/sec: 204.22\n",
            "2019-12-22 17:49:48,387 epoch 53 - iter 130/138 - loss 0.34118410 - samples/sec: 182.03\n",
            "2019-12-22 17:49:49,481 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:49,482 EPOCH 53 done: loss 0.3421 - lr 0.1000\n",
            "2019-12-22 17:49:49,483 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:49:49,484 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:49:49,643 epoch 54 - iter 0/138 - loss 0.25579524 - samples/sec: 2640.58\n",
            "2019-12-22 17:49:51,938 epoch 54 - iter 13/138 - loss 0.30080678 - samples/sec: 182.59\n",
            "2019-12-22 17:49:54,001 epoch 54 - iter 26/138 - loss 0.33980336 - samples/sec: 203.73\n",
            "2019-12-22 17:49:56,144 epoch 54 - iter 39/138 - loss 0.33988429 - samples/sec: 195.88\n",
            "2019-12-22 17:49:58,386 epoch 54 - iter 52/138 - loss 0.35852789 - samples/sec: 187.00\n",
            "2019-12-22 17:50:00,488 epoch 54 - iter 65/138 - loss 0.35614837 - samples/sec: 199.77\n",
            "2019-12-22 17:50:02,661 epoch 54 - iter 78/138 - loss 0.34766080 - samples/sec: 193.19\n",
            "2019-12-22 17:50:04,843 epoch 54 - iter 91/138 - loss 0.34750206 - samples/sec: 192.31\n",
            "2019-12-22 17:50:07,045 epoch 54 - iter 104/138 - loss 0.35378699 - samples/sec: 190.80\n",
            "2019-12-22 17:50:09,544 epoch 54 - iter 117/138 - loss 0.35023570 - samples/sec: 167.85\n",
            "2019-12-22 17:50:11,633 epoch 54 - iter 130/138 - loss 0.35457861 - samples/sec: 200.88\n",
            "2019-12-22 17:50:12,711 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:12,712 EPOCH 54 done: loss 0.3613 - lr 0.1000\n",
            "2019-12-22 17:50:12,717 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:50:12,719 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:12,927 epoch 55 - iter 0/138 - loss 0.19667929 - samples/sec: 2020.28\n",
            "2019-12-22 17:50:15,091 epoch 55 - iter 13/138 - loss 0.40341700 - samples/sec: 193.81\n",
            "2019-12-22 17:50:17,352 epoch 55 - iter 26/138 - loss 0.38208225 - samples/sec: 185.76\n",
            "2019-12-22 17:50:19,457 epoch 55 - iter 39/138 - loss 0.36456719 - samples/sec: 199.54\n",
            "2019-12-22 17:50:21,528 epoch 55 - iter 52/138 - loss 0.36032154 - samples/sec: 202.82\n",
            "2019-12-22 17:50:23,702 epoch 55 - iter 65/138 - loss 0.36455500 - samples/sec: 193.17\n",
            "2019-12-22 17:50:25,822 epoch 55 - iter 78/138 - loss 0.37098193 - samples/sec: 198.01\n",
            "2019-12-22 17:50:27,992 epoch 55 - iter 91/138 - loss 0.36662666 - samples/sec: 193.48\n",
            "2019-12-22 17:50:30,231 epoch 55 - iter 104/138 - loss 0.36084564 - samples/sec: 187.50\n",
            "2019-12-22 17:50:32,337 epoch 55 - iter 117/138 - loss 0.35989011 - samples/sec: 199.38\n",
            "2019-12-22 17:50:34,469 epoch 55 - iter 130/138 - loss 0.35451088 - samples/sec: 197.00\n",
            "2019-12-22 17:50:35,618 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:35,619 EPOCH 55 done: loss 0.3553 - lr 0.1000\n",
            "2019-12-22 17:50:35,624 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 17:50:35,628 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:35,816 epoch 56 - iter 0/138 - loss 0.47250533 - samples/sec: 2248.95\n",
            "2019-12-22 17:50:37,935 epoch 56 - iter 13/138 - loss 0.36107856 - samples/sec: 197.92\n",
            "2019-12-22 17:50:40,147 epoch 56 - iter 26/138 - loss 0.31592415 - samples/sec: 189.59\n",
            "2019-12-22 17:50:42,388 epoch 56 - iter 39/138 - loss 0.33600472 - samples/sec: 187.44\n",
            "2019-12-22 17:50:44,563 epoch 56 - iter 52/138 - loss 0.33027142 - samples/sec: 192.98\n",
            "2019-12-22 17:50:46,755 epoch 56 - iter 65/138 - loss 0.32379235 - samples/sec: 191.54\n",
            "2019-12-22 17:50:48,926 epoch 56 - iter 78/138 - loss 0.31887662 - samples/sec: 193.36\n",
            "2019-12-22 17:50:51,094 epoch 56 - iter 91/138 - loss 0.33914757 - samples/sec: 193.50\n",
            "2019-12-22 17:50:53,284 epoch 56 - iter 104/138 - loss 0.33300545 - samples/sec: 191.70\n",
            "2019-12-22 17:50:55,440 epoch 56 - iter 117/138 - loss 0.33549755 - samples/sec: 195.17\n",
            "2019-12-22 17:50:57,704 epoch 56 - iter 130/138 - loss 0.33604648 - samples/sec: 185.32\n",
            "2019-12-22 17:50:58,791 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:58,792 EPOCH 56 done: loss 0.3400 - lr 0.1000\n",
            "2019-12-22 17:50:58,793 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:50:58,794 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:50:58,946 epoch 57 - iter 0/138 - loss 0.30061543 - samples/sec: 2774.31\n",
            "2019-12-22 17:51:01,090 epoch 57 - iter 13/138 - loss 0.31284928 - samples/sec: 195.66\n",
            "2019-12-22 17:51:03,240 epoch 57 - iter 26/138 - loss 0.30885005 - samples/sec: 195.23\n",
            "2019-12-22 17:51:05,547 epoch 57 - iter 39/138 - loss 0.32229852 - samples/sec: 181.90\n",
            "2019-12-22 17:51:07,656 epoch 57 - iter 52/138 - loss 0.33749589 - samples/sec: 199.12\n",
            "2019-12-22 17:51:11,684 epoch 57 - iter 65/138 - loss 0.33244453 - samples/sec: 103.77\n",
            "2019-12-22 17:51:13,874 epoch 57 - iter 78/138 - loss 0.32772329 - samples/sec: 191.60\n",
            "2019-12-22 17:51:16,032 epoch 57 - iter 91/138 - loss 0.33504504 - samples/sec: 194.59\n",
            "2019-12-22 17:51:18,133 epoch 57 - iter 104/138 - loss 0.33260091 - samples/sec: 199.95\n",
            "2019-12-22 17:51:20,346 epoch 57 - iter 117/138 - loss 0.33640284 - samples/sec: 189.60\n",
            "2019-12-22 17:51:22,513 epoch 57 - iter 130/138 - loss 0.33563522 - samples/sec: 193.70\n",
            "2019-12-22 17:51:23,754 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:51:23,755 EPOCH 57 done: loss 0.3309 - lr 0.1000\n",
            "2019-12-22 17:51:23,756 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:51:23,757 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:51:23,913 epoch 58 - iter 0/138 - loss 0.26095927 - samples/sec: 2684.09\n",
            "2019-12-22 17:51:26,045 epoch 58 - iter 13/138 - loss 0.34709827 - samples/sec: 196.59\n",
            "2019-12-22 17:51:28,372 epoch 58 - iter 26/138 - loss 0.34165926 - samples/sec: 180.34\n",
            "2019-12-22 17:51:30,628 epoch 58 - iter 39/138 - loss 0.34882164 - samples/sec: 186.00\n",
            "2019-12-22 17:51:32,838 epoch 58 - iter 52/138 - loss 0.32517179 - samples/sec: 189.92\n",
            "2019-12-22 17:51:35,210 epoch 58 - iter 65/138 - loss 0.32206999 - samples/sec: 176.85\n",
            "2019-12-22 17:51:37,406 epoch 58 - iter 78/138 - loss 0.32299000 - samples/sec: 191.33\n",
            "2019-12-22 17:51:39,565 epoch 58 - iter 91/138 - loss 0.32472708 - samples/sec: 194.15\n",
            "2019-12-22 17:51:41,694 epoch 58 - iter 104/138 - loss 0.31966090 - samples/sec: 197.17\n",
            "2019-12-22 17:51:43,897 epoch 58 - iter 117/138 - loss 0.32290560 - samples/sec: 190.81\n",
            "2019-12-22 17:51:46,006 epoch 58 - iter 130/138 - loss 0.32615296 - samples/sec: 199.12\n",
            "2019-12-22 17:51:47,159 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:51:47,160 EPOCH 58 done: loss 0.3314 - lr 0.1000\n",
            "2019-12-22 17:51:47,161 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:51:47,162 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:51:47,361 epoch 59 - iter 0/138 - loss 0.62810326 - samples/sec: 2098.59\n",
            "2019-12-22 17:51:49,543 epoch 59 - iter 13/138 - loss 0.36827899 - samples/sec: 192.19\n",
            "2019-12-22 17:51:51,663 epoch 59 - iter 26/138 - loss 0.33556519 - samples/sec: 198.14\n",
            "2019-12-22 17:51:53,821 epoch 59 - iter 39/138 - loss 0.34888311 - samples/sec: 194.53\n",
            "2019-12-22 17:51:56,063 epoch 59 - iter 52/138 - loss 0.33374195 - samples/sec: 187.24\n",
            "2019-12-22 17:51:58,265 epoch 59 - iter 65/138 - loss 0.32438309 - samples/sec: 190.57\n",
            "2019-12-22 17:52:00,418 epoch 59 - iter 78/138 - loss 0.33157218 - samples/sec: 195.14\n",
            "2019-12-22 17:52:02,680 epoch 59 - iter 91/138 - loss 0.33594029 - samples/sec: 185.47\n",
            "2019-12-22 17:52:04,823 epoch 59 - iter 104/138 - loss 0.33468817 - samples/sec: 196.03\n",
            "2019-12-22 17:52:07,128 epoch 59 - iter 117/138 - loss 0.33746353 - samples/sec: 182.01\n",
            "2019-12-22 17:52:09,494 epoch 59 - iter 130/138 - loss 0.33637576 - samples/sec: 177.62\n",
            "2019-12-22 17:52:10,591 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:10,592 EPOCH 59 done: loss 0.3359 - lr 0.1000\n",
            "2019-12-22 17:52:10,593 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 17:52:10,594 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:10,752 epoch 60 - iter 0/138 - loss 0.33049923 - samples/sec: 2639.48\n",
            "2019-12-22 17:52:12,905 epoch 60 - iter 13/138 - loss 0.28351117 - samples/sec: 194.80\n",
            "2019-12-22 17:52:15,071 epoch 60 - iter 26/138 - loss 0.27605285 - samples/sec: 193.88\n",
            "2019-12-22 17:52:17,253 epoch 60 - iter 39/138 - loss 0.26691423 - samples/sec: 192.44\n",
            "2019-12-22 17:52:19,510 epoch 60 - iter 52/138 - loss 0.29209144 - samples/sec: 185.99\n",
            "2019-12-22 17:52:21,689 epoch 60 - iter 65/138 - loss 0.28521546 - samples/sec: 192.69\n",
            "2019-12-22 17:52:23,821 epoch 60 - iter 78/138 - loss 0.28927073 - samples/sec: 196.94\n",
            "2019-12-22 17:52:25,981 epoch 60 - iter 91/138 - loss 0.29252964 - samples/sec: 194.43\n",
            "2019-12-22 17:52:28,194 epoch 60 - iter 104/138 - loss 0.30074860 - samples/sec: 189.67\n",
            "2019-12-22 17:52:30,306 epoch 60 - iter 117/138 - loss 0.31013325 - samples/sec: 198.86\n",
            "2019-12-22 17:52:32,362 epoch 60 - iter 130/138 - loss 0.30466644 - samples/sec: 204.34\n",
            "2019-12-22 17:52:33,559 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:33,560 EPOCH 60 done: loss 0.3066 - lr 0.1000\n",
            "2019-12-22 17:52:33,564 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:52:33,566 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:33,739 epoch 61 - iter 0/138 - loss 0.27821958 - samples/sec: 2454.89\n",
            "2019-12-22 17:52:35,935 epoch 61 - iter 13/138 - loss 0.28973447 - samples/sec: 190.95\n",
            "2019-12-22 17:52:37,974 epoch 61 - iter 26/138 - loss 0.30777342 - samples/sec: 205.96\n",
            "2019-12-22 17:52:40,060 epoch 61 - iter 39/138 - loss 0.29803480 - samples/sec: 201.70\n",
            "2019-12-22 17:52:42,190 epoch 61 - iter 52/138 - loss 0.28543452 - samples/sec: 197.13\n",
            "2019-12-22 17:52:44,484 epoch 61 - iter 65/138 - loss 0.29635464 - samples/sec: 182.94\n",
            "2019-12-22 17:52:46,606 epoch 61 - iter 78/138 - loss 0.29625327 - samples/sec: 197.93\n",
            "2019-12-22 17:52:48,758 epoch 61 - iter 91/138 - loss 0.29272940 - samples/sec: 195.18\n",
            "2019-12-22 17:52:51,015 epoch 61 - iter 104/138 - loss 0.30131324 - samples/sec: 186.22\n",
            "2019-12-22 17:52:53,205 epoch 61 - iter 117/138 - loss 0.30050310 - samples/sec: 191.69\n",
            "2019-12-22 17:52:55,333 epoch 61 - iter 130/138 - loss 0.30045579 - samples/sec: 197.45\n",
            "2019-12-22 17:52:56,581 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:56,586 EPOCH 61 done: loss 0.2996 - lr 0.1000\n",
            "2019-12-22 17:52:56,586 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:52:56,587 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:52:56,774 epoch 62 - iter 0/138 - loss 0.31743073 - samples/sec: 2242.57\n",
            "2019-12-22 17:52:58,919 epoch 62 - iter 13/138 - loss 0.33427654 - samples/sec: 195.58\n",
            "2019-12-22 17:53:01,039 epoch 62 - iter 26/138 - loss 0.31124395 - samples/sec: 198.00\n",
            "2019-12-22 17:53:03,117 epoch 62 - iter 39/138 - loss 0.32577648 - samples/sec: 202.24\n",
            "2019-12-22 17:53:05,357 epoch 62 - iter 52/138 - loss 0.32675646 - samples/sec: 187.34\n",
            "2019-12-22 17:53:07,653 epoch 62 - iter 65/138 - loss 0.31176946 - samples/sec: 182.82\n",
            "2019-12-22 17:53:09,710 epoch 62 - iter 78/138 - loss 0.31710796 - samples/sec: 204.33\n",
            "2019-12-22 17:53:11,880 epoch 62 - iter 91/138 - loss 0.33207951 - samples/sec: 193.62\n",
            "2019-12-22 17:53:14,068 epoch 62 - iter 104/138 - loss 0.32817762 - samples/sec: 191.93\n",
            "2019-12-22 17:53:16,283 epoch 62 - iter 117/138 - loss 0.32819217 - samples/sec: 189.44\n",
            "2019-12-22 17:53:18,499 epoch 62 - iter 130/138 - loss 0.32846336 - samples/sec: 189.66\n",
            "2019-12-22 17:53:19,636 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:53:19,637 EPOCH 62 done: loss 0.3317 - lr 0.1000\n",
            "2019-12-22 17:53:19,638 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:53:19,639 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:53:19,809 epoch 63 - iter 0/138 - loss 0.36556697 - samples/sec: 2468.78\n",
            "2019-12-22 17:53:21,987 epoch 63 - iter 13/138 - loss 0.26404895 - samples/sec: 192.50\n",
            "2019-12-22 17:53:24,052 epoch 63 - iter 26/138 - loss 0.29603154 - samples/sec: 203.41\n",
            "2019-12-22 17:53:26,163 epoch 63 - iter 39/138 - loss 0.32952879 - samples/sec: 198.90\n",
            "2019-12-22 17:53:28,353 epoch 63 - iter 52/138 - loss 0.31332216 - samples/sec: 191.53\n",
            "2019-12-22 17:53:30,573 epoch 63 - iter 65/138 - loss 0.31263186 - samples/sec: 189.01\n",
            "2019-12-22 17:53:32,693 epoch 63 - iter 78/138 - loss 0.31244161 - samples/sec: 198.02\n",
            "2019-12-22 17:53:34,882 epoch 63 - iter 91/138 - loss 0.32208561 - samples/sec: 191.85\n",
            "2019-12-22 17:53:37,139 epoch 63 - iter 104/138 - loss 0.32701491 - samples/sec: 185.88\n",
            "2019-12-22 17:53:39,325 epoch 63 - iter 117/138 - loss 0.33069184 - samples/sec: 192.29\n",
            "2019-12-22 17:53:41,447 epoch 63 - iter 130/138 - loss 0.32294421 - samples/sec: 197.72\n",
            "2019-12-22 17:53:42,518 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:53:42,519 EPOCH 63 done: loss 0.3213 - lr 0.1000\n",
            "2019-12-22 17:53:42,521 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 17:53:42,523 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:53:42,688 epoch 64 - iter 0/138 - loss 0.16506124 - samples/sec: 2570.72\n",
            "2019-12-22 17:53:44,851 epoch 64 - iter 13/138 - loss 0.27712863 - samples/sec: 193.84\n",
            "2019-12-22 17:53:47,155 epoch 64 - iter 26/138 - loss 0.28900481 - samples/sec: 182.15\n",
            "2019-12-22 17:53:49,328 epoch 64 - iter 39/138 - loss 0.32796322 - samples/sec: 193.17\n",
            "2019-12-22 17:53:51,531 epoch 64 - iter 52/138 - loss 0.32595659 - samples/sec: 190.57\n",
            "2019-12-22 17:53:53,833 epoch 64 - iter 65/138 - loss 0.32863648 - samples/sec: 182.25\n",
            "2019-12-22 17:53:55,901 epoch 64 - iter 78/138 - loss 0.33609907 - samples/sec: 202.85\n",
            "2019-12-22 17:53:58,041 epoch 64 - iter 91/138 - loss 0.33073423 - samples/sec: 196.24\n",
            "2019-12-22 17:54:00,140 epoch 64 - iter 104/138 - loss 0.32778239 - samples/sec: 200.57\n",
            "2019-12-22 17:54:02,370 epoch 64 - iter 117/138 - loss 0.33062180 - samples/sec: 188.23\n",
            "2019-12-22 17:54:04,655 epoch 64 - iter 130/138 - loss 0.32493904 - samples/sec: 183.60\n",
            "2019-12-22 17:54:05,770 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:05,771 EPOCH 64 done: loss 0.3251 - lr 0.1000\n",
            "2019-12-22 17:54:05,772 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 17:54:05,773 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:05,957 epoch 65 - iter 0/138 - loss 0.29093504 - samples/sec: 2278.92\n",
            "2019-12-22 17:54:08,242 epoch 65 - iter 13/138 - loss 0.29288047 - samples/sec: 183.74\n",
            "2019-12-22 17:54:10,527 epoch 65 - iter 26/138 - loss 0.32996992 - samples/sec: 183.92\n",
            "2019-12-22 17:54:12,725 epoch 65 - iter 39/138 - loss 0.34158942 - samples/sec: 191.01\n",
            "2019-12-22 17:54:14,900 epoch 65 - iter 52/138 - loss 0.32695594 - samples/sec: 193.00\n",
            "2019-12-22 17:54:17,081 epoch 65 - iter 65/138 - loss 0.32681550 - samples/sec: 192.62\n",
            "2019-12-22 17:54:19,373 epoch 65 - iter 78/138 - loss 0.32461551 - samples/sec: 182.94\n",
            "2019-12-22 17:54:21,633 epoch 65 - iter 91/138 - loss 0.31456668 - samples/sec: 185.64\n",
            "2019-12-22 17:54:23,836 epoch 65 - iter 104/138 - loss 0.31885858 - samples/sec: 190.51\n",
            "2019-12-22 17:54:25,974 epoch 65 - iter 117/138 - loss 0.31326500 - samples/sec: 196.35\n",
            "2019-12-22 17:54:28,288 epoch 65 - iter 130/138 - loss 0.31277994 - samples/sec: 181.61\n",
            "2019-12-22 17:54:29,456 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:29,457 EPOCH 65 done: loss 0.3108 - lr 0.1000\n",
            "Epoch    64: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2019-12-22 17:54:29,458 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 17:54:29,459 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:29,624 epoch 66 - iter 0/138 - loss 0.16099703 - samples/sec: 2537.61\n",
            "2019-12-22 17:54:31,817 epoch 66 - iter 13/138 - loss 0.29960444 - samples/sec: 191.27\n",
            "2019-12-22 17:54:34,094 epoch 66 - iter 26/138 - loss 0.31127340 - samples/sec: 184.28\n",
            "2019-12-22 17:54:36,204 epoch 66 - iter 39/138 - loss 0.32329362 - samples/sec: 199.61\n",
            "2019-12-22 17:54:38,445 epoch 66 - iter 52/138 - loss 0.31311607 - samples/sec: 187.37\n",
            "2019-12-22 17:54:40,595 epoch 66 - iter 65/138 - loss 0.31582406 - samples/sec: 195.69\n",
            "2019-12-22 17:54:42,832 epoch 66 - iter 78/138 - loss 0.30216271 - samples/sec: 187.75\n",
            "2019-12-22 17:54:45,004 epoch 66 - iter 91/138 - loss 0.29074900 - samples/sec: 193.31\n",
            "2019-12-22 17:54:47,257 epoch 66 - iter 104/138 - loss 0.28012104 - samples/sec: 186.13\n",
            "2019-12-22 17:54:49,556 epoch 66 - iter 117/138 - loss 0.28037764 - samples/sec: 183.02\n",
            "2019-12-22 17:54:51,845 epoch 66 - iter 130/138 - loss 0.28472067 - samples/sec: 183.40\n",
            "2019-12-22 17:54:53,086 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:53,087 EPOCH 66 done: loss 0.2913 - lr 0.0500\n",
            "2019-12-22 17:54:53,088 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:54:53,089 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:54:53,257 epoch 67 - iter 0/138 - loss 0.38154125 - samples/sec: 2501.20\n",
            "2019-12-22 17:54:55,491 epoch 67 - iter 13/138 - loss 0.28862474 - samples/sec: 187.92\n",
            "2019-12-22 17:54:57,756 epoch 67 - iter 26/138 - loss 0.28362887 - samples/sec: 185.30\n",
            "2019-12-22 17:54:59,936 epoch 67 - iter 39/138 - loss 0.25514822 - samples/sec: 192.44\n",
            "2019-12-22 17:55:02,215 epoch 67 - iter 52/138 - loss 0.25074415 - samples/sec: 184.12\n",
            "2019-12-22 17:55:04,485 epoch 67 - iter 65/138 - loss 0.25278099 - samples/sec: 184.81\n",
            "2019-12-22 17:55:06,720 epoch 67 - iter 78/138 - loss 0.25993181 - samples/sec: 187.72\n",
            "2019-12-22 17:55:08,943 epoch 67 - iter 91/138 - loss 0.25504156 - samples/sec: 188.87\n",
            "2019-12-22 17:55:11,151 epoch 67 - iter 104/138 - loss 0.25126889 - samples/sec: 190.04\n",
            "2019-12-22 17:55:13,298 epoch 67 - iter 117/138 - loss 0.26365299 - samples/sec: 195.63\n",
            "2019-12-22 17:55:15,485 epoch 67 - iter 130/138 - loss 0.26493955 - samples/sec: 191.96\n",
            "2019-12-22 17:55:16,743 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:55:16,744 EPOCH 67 done: loss 0.2649 - lr 0.0500\n",
            "2019-12-22 17:55:16,745 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:55:16,746 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:55:16,926 epoch 68 - iter 0/138 - loss 0.05286813 - samples/sec: 2327.45\n",
            "2019-12-22 17:55:19,120 epoch 68 - iter 13/138 - loss 0.20333594 - samples/sec: 191.59\n",
            "2019-12-22 17:55:21,375 epoch 68 - iter 26/138 - loss 0.23679231 - samples/sec: 186.48\n",
            "2019-12-22 17:55:23,820 epoch 68 - iter 39/138 - loss 0.24979095 - samples/sec: 171.61\n",
            "2019-12-22 17:55:26,160 epoch 68 - iter 52/138 - loss 0.25725414 - samples/sec: 179.90\n",
            "2019-12-22 17:55:28,471 epoch 68 - iter 65/138 - loss 0.25265560 - samples/sec: 181.56\n",
            "2019-12-22 17:55:30,726 epoch 68 - iter 78/138 - loss 0.25183392 - samples/sec: 186.77\n",
            "2019-12-22 17:55:32,931 epoch 68 - iter 91/138 - loss 0.25968591 - samples/sec: 190.30\n",
            "2019-12-22 17:55:35,345 epoch 68 - iter 104/138 - loss 0.26242579 - samples/sec: 173.84\n",
            "2019-12-22 17:55:37,826 epoch 68 - iter 117/138 - loss 0.26250464 - samples/sec: 169.94\n",
            "2019-12-22 17:55:40,092 epoch 68 - iter 130/138 - loss 0.26102462 - samples/sec: 185.30\n",
            "2019-12-22 17:55:41,273 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:55:41,274 EPOCH 68 done: loss 0.2639 - lr 0.0500\n",
            "2019-12-22 17:55:41,275 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:55:41,276 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:55:41,452 epoch 69 - iter 0/138 - loss 0.25267959 - samples/sec: 2388.44\n",
            "2019-12-22 17:55:43,766 epoch 69 - iter 13/138 - loss 0.29772219 - samples/sec: 181.21\n",
            "2019-12-22 17:55:46,097 epoch 69 - iter 26/138 - loss 0.25333689 - samples/sec: 180.09\n",
            "2019-12-22 17:55:48,386 epoch 69 - iter 39/138 - loss 0.25033235 - samples/sec: 183.21\n",
            "2019-12-22 17:55:50,761 epoch 69 - iter 52/138 - loss 0.24524894 - samples/sec: 176.45\n",
            "2019-12-22 17:55:52,914 epoch 69 - iter 65/138 - loss 0.24810325 - samples/sec: 195.05\n",
            "2019-12-22 17:55:55,169 epoch 69 - iter 78/138 - loss 0.24634933 - samples/sec: 186.82\n",
            "2019-12-22 17:55:57,438 epoch 69 - iter 91/138 - loss 0.24645782 - samples/sec: 185.02\n",
            "2019-12-22 17:55:59,863 epoch 69 - iter 104/138 - loss 0.24410767 - samples/sec: 173.11\n",
            "2019-12-22 17:56:02,273 epoch 69 - iter 117/138 - loss 0.24531699 - samples/sec: 174.11\n",
            "2019-12-22 17:56:04,445 epoch 69 - iter 130/138 - loss 0.25316424 - samples/sec: 193.24\n",
            "2019-12-22 17:56:05,674 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:05,675 EPOCH 69 done: loss 0.2538 - lr 0.0500\n",
            "2019-12-22 17:56:05,676 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:56:05,677 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:05,839 epoch 70 - iter 0/138 - loss 0.46354675 - samples/sec: 2587.00\n",
            "2019-12-22 17:56:08,286 epoch 70 - iter 13/138 - loss 0.17544679 - samples/sec: 171.28\n",
            "2019-12-22 17:56:10,476 epoch 70 - iter 26/138 - loss 0.21835877 - samples/sec: 192.06\n",
            "2019-12-22 17:56:12,618 epoch 70 - iter 39/138 - loss 0.23035653 - samples/sec: 196.04\n",
            "2019-12-22 17:56:14,806 epoch 70 - iter 52/138 - loss 0.23307820 - samples/sec: 192.10\n",
            "2019-12-22 17:56:17,123 epoch 70 - iter 65/138 - loss 0.22759640 - samples/sec: 181.02\n",
            "2019-12-22 17:56:19,288 epoch 70 - iter 78/138 - loss 0.23186382 - samples/sec: 194.05\n",
            "2019-12-22 17:56:21,612 epoch 70 - iter 91/138 - loss 0.23526928 - samples/sec: 180.65\n",
            "2019-12-22 17:56:23,919 epoch 70 - iter 104/138 - loss 0.23434591 - samples/sec: 181.80\n",
            "2019-12-22 17:56:26,034 epoch 70 - iter 117/138 - loss 0.23549942 - samples/sec: 198.55\n",
            "2019-12-22 17:56:28,364 epoch 70 - iter 130/138 - loss 0.24096615 - samples/sec: 180.61\n",
            "2019-12-22 17:56:29,531 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:29,532 EPOCH 70 done: loss 0.2372 - lr 0.0500\n",
            "2019-12-22 17:56:29,533 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:56:29,534 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:29,694 epoch 71 - iter 0/138 - loss 0.51062220 - samples/sec: 2635.77\n",
            "2019-12-22 17:56:32,072 epoch 71 - iter 13/138 - loss 0.25494713 - samples/sec: 176.18\n",
            "2019-12-22 17:56:34,317 epoch 71 - iter 26/138 - loss 0.29738955 - samples/sec: 187.00\n",
            "2019-12-22 17:56:36,642 epoch 71 - iter 39/138 - loss 0.27076496 - samples/sec: 180.47\n",
            "2019-12-22 17:56:39,015 epoch 71 - iter 52/138 - loss 0.25656118 - samples/sec: 176.71\n",
            "2019-12-22 17:56:41,318 epoch 71 - iter 65/138 - loss 0.24128121 - samples/sec: 182.36\n",
            "2019-12-22 17:56:43,455 epoch 71 - iter 78/138 - loss 0.24217417 - samples/sec: 196.55\n",
            "2019-12-22 17:56:45,763 epoch 71 - iter 91/138 - loss 0.23743246 - samples/sec: 181.92\n",
            "2019-12-22 17:56:48,141 epoch 71 - iter 104/138 - loss 0.24042725 - samples/sec: 176.52\n",
            "2019-12-22 17:56:50,434 epoch 71 - iter 117/138 - loss 0.23613375 - samples/sec: 183.00\n",
            "2019-12-22 17:56:52,540 epoch 71 - iter 130/138 - loss 0.23543674 - samples/sec: 199.47\n",
            "2019-12-22 17:56:53,752 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:53,754 EPOCH 71 done: loss 0.2378 - lr 0.0500\n",
            "2019-12-22 17:56:53,755 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:56:53,756 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:56:53,942 epoch 72 - iter 0/138 - loss 0.32239389 - samples/sec: 2254.08\n",
            "2019-12-22 17:56:56,163 epoch 72 - iter 13/138 - loss 0.25105875 - samples/sec: 188.96\n",
            "2019-12-22 17:56:58,493 epoch 72 - iter 26/138 - loss 0.25851887 - samples/sec: 180.00\n",
            "2019-12-22 17:57:00,655 epoch 72 - iter 39/138 - loss 0.24994707 - samples/sec: 193.98\n",
            "2019-12-22 17:57:02,858 epoch 72 - iter 52/138 - loss 0.23285741 - samples/sec: 190.39\n",
            "2019-12-22 17:57:05,060 epoch 72 - iter 65/138 - loss 0.23883437 - samples/sec: 190.65\n",
            "2019-12-22 17:57:07,298 epoch 72 - iter 78/138 - loss 0.22782585 - samples/sec: 187.64\n",
            "2019-12-22 17:57:09,784 epoch 72 - iter 91/138 - loss 0.23013447 - samples/sec: 168.98\n",
            "2019-12-22 17:57:12,102 epoch 72 - iter 104/138 - loss 0.22721662 - samples/sec: 181.05\n",
            "2019-12-22 17:57:14,375 epoch 72 - iter 117/138 - loss 0.24110778 - samples/sec: 184.51\n",
            "2019-12-22 17:57:16,701 epoch 72 - iter 130/138 - loss 0.23532102 - samples/sec: 180.37\n",
            "2019-12-22 17:57:17,861 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:57:17,863 EPOCH 72 done: loss 0.2322 - lr 0.0500\n",
            "2019-12-22 17:57:17,864 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:57:17,865 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:57:18,023 epoch 73 - iter 0/138 - loss 0.08834887 - samples/sec: 2671.54\n",
            "2019-12-22 17:57:20,312 epoch 73 - iter 13/138 - loss 0.26051170 - samples/sec: 183.03\n",
            "2019-12-22 17:57:22,649 epoch 73 - iter 26/138 - loss 0.25893416 - samples/sec: 179.62\n",
            "2019-12-22 17:57:24,946 epoch 73 - iter 39/138 - loss 0.25616895 - samples/sec: 183.10\n",
            "2019-12-22 17:57:27,176 epoch 73 - iter 52/138 - loss 0.24042914 - samples/sec: 188.24\n",
            "2019-12-22 17:57:29,678 epoch 73 - iter 65/138 - loss 0.23203888 - samples/sec: 167.63\n",
            "2019-12-22 17:57:32,005 epoch 73 - iter 78/138 - loss 0.23763281 - samples/sec: 180.34\n",
            "2019-12-22 17:57:34,212 epoch 73 - iter 91/138 - loss 0.23675164 - samples/sec: 190.30\n",
            "2019-12-22 17:57:36,366 epoch 73 - iter 104/138 - loss 0.23435802 - samples/sec: 195.36\n",
            "2019-12-22 17:57:38,509 epoch 73 - iter 117/138 - loss 0.22847294 - samples/sec: 196.00\n",
            "2019-12-22 17:57:40,774 epoch 73 - iter 130/138 - loss 0.22926223 - samples/sec: 185.73\n",
            "2019-12-22 17:57:41,989 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:57:41,990 EPOCH 73 done: loss 0.2285 - lr 0.0500\n",
            "2019-12-22 17:57:41,991 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:57:41,992 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:57:42,176 epoch 74 - iter 0/138 - loss 0.18654346 - samples/sec: 2293.21\n",
            "2019-12-22 17:57:44,474 epoch 74 - iter 13/138 - loss 0.21832205 - samples/sec: 182.94\n",
            "2019-12-22 17:57:46,750 epoch 74 - iter 26/138 - loss 0.22434943 - samples/sec: 184.43\n",
            "2019-12-22 17:57:48,913 epoch 74 - iter 39/138 - loss 0.21761321 - samples/sec: 194.08\n",
            "2019-12-22 17:57:51,063 epoch 74 - iter 52/138 - loss 0.22097995 - samples/sec: 195.17\n",
            "2019-12-22 17:57:53,515 epoch 74 - iter 65/138 - loss 0.21791416 - samples/sec: 171.14\n",
            "2019-12-22 17:57:55,656 epoch 74 - iter 78/138 - loss 0.20959305 - samples/sec: 196.33\n",
            "2019-12-22 17:57:57,970 epoch 74 - iter 91/138 - loss 0.20858945 - samples/sec: 181.40\n",
            "2019-12-22 17:58:00,174 epoch 74 - iter 104/138 - loss 0.21292428 - samples/sec: 190.58\n",
            "2019-12-22 17:58:02,355 epoch 74 - iter 117/138 - loss 0.21020253 - samples/sec: 192.42\n",
            "2019-12-22 17:58:04,482 epoch 74 - iter 130/138 - loss 0.22022311 - samples/sec: 197.61\n",
            "2019-12-22 17:58:05,582 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:05,583 EPOCH 74 done: loss 0.2218 - lr 0.0500\n",
            "2019-12-22 17:58:05,585 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:58:05,588 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:05,734 epoch 75 - iter 0/138 - loss 0.05510402 - samples/sec: 2887.87\n",
            "2019-12-22 17:58:07,925 epoch 75 - iter 13/138 - loss 0.23987081 - samples/sec: 191.38\n",
            "2019-12-22 17:58:10,311 epoch 75 - iter 26/138 - loss 0.22511882 - samples/sec: 176.01\n",
            "2019-12-22 17:58:12,445 epoch 75 - iter 39/138 - loss 0.20095453 - samples/sec: 196.80\n",
            "2019-12-22 17:58:14,594 epoch 75 - iter 52/138 - loss 0.19593572 - samples/sec: 195.23\n",
            "2019-12-22 17:58:16,716 epoch 75 - iter 65/138 - loss 0.19828209 - samples/sec: 197.86\n",
            "2019-12-22 17:58:18,849 epoch 75 - iter 78/138 - loss 0.19876324 - samples/sec: 196.90\n",
            "2019-12-22 17:58:21,132 epoch 75 - iter 91/138 - loss 0.21741114 - samples/sec: 183.86\n",
            "2019-12-22 17:58:23,371 epoch 75 - iter 104/138 - loss 0.20860045 - samples/sec: 187.41\n",
            "2019-12-22 17:58:25,609 epoch 75 - iter 117/138 - loss 0.21168333 - samples/sec: 187.57\n",
            "2019-12-22 17:58:27,981 epoch 75 - iter 130/138 - loss 0.20939093 - samples/sec: 176.91\n",
            "2019-12-22 17:58:29,066 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:29,068 EPOCH 75 done: loss 0.2105 - lr 0.0500\n",
            "2019-12-22 17:58:29,069 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:58:29,070 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:29,233 epoch 76 - iter 0/138 - loss 0.17429209 - samples/sec: 2585.97\n",
            "2019-12-22 17:58:31,378 epoch 76 - iter 13/138 - loss 0.17268686 - samples/sec: 195.76\n",
            "2019-12-22 17:58:33,787 epoch 76 - iter 26/138 - loss 0.18809042 - samples/sec: 174.45\n",
            "2019-12-22 17:58:36,071 epoch 76 - iter 39/138 - loss 0.18380112 - samples/sec: 184.18\n",
            "2019-12-22 17:58:38,340 epoch 76 - iter 52/138 - loss 0.19958537 - samples/sec: 185.15\n",
            "2019-12-22 17:58:40,541 epoch 76 - iter 65/138 - loss 0.20041351 - samples/sec: 190.68\n",
            "2019-12-22 17:58:42,669 epoch 76 - iter 78/138 - loss 0.20563173 - samples/sec: 197.36\n",
            "2019-12-22 17:58:44,836 epoch 76 - iter 91/138 - loss 0.20540134 - samples/sec: 193.61\n",
            "2019-12-22 17:58:47,045 epoch 76 - iter 104/138 - loss 0.21000512 - samples/sec: 189.80\n",
            "2019-12-22 17:58:49,292 epoch 76 - iter 117/138 - loss 0.21327794 - samples/sec: 186.83\n",
            "2019-12-22 17:58:51,531 epoch 76 - iter 130/138 - loss 0.21641928 - samples/sec: 187.66\n",
            "2019-12-22 17:58:52,762 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:52,763 EPOCH 76 done: loss 0.2194 - lr 0.0500\n",
            "2019-12-22 17:58:52,764 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:58:52,765 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:58:52,985 epoch 77 - iter 0/138 - loss 0.34918821 - samples/sec: 1905.26\n",
            "2019-12-22 17:58:55,132 epoch 77 - iter 13/138 - loss 0.20719505 - samples/sec: 196.05\n",
            "2019-12-22 17:58:57,429 epoch 77 - iter 26/138 - loss 0.21370764 - samples/sec: 182.71\n",
            "2019-12-22 17:58:59,606 epoch 77 - iter 39/138 - loss 0.20767368 - samples/sec: 192.87\n",
            "2019-12-22 17:59:01,872 epoch 77 - iter 52/138 - loss 0.20817299 - samples/sec: 185.21\n",
            "2019-12-22 17:59:04,047 epoch 77 - iter 65/138 - loss 0.20381725 - samples/sec: 193.05\n",
            "2019-12-22 17:59:06,400 epoch 77 - iter 78/138 - loss 0.19633626 - samples/sec: 178.44\n",
            "2019-12-22 17:59:08,623 epoch 77 - iter 91/138 - loss 0.20482040 - samples/sec: 188.80\n",
            "2019-12-22 17:59:10,948 epoch 77 - iter 104/138 - loss 0.20703860 - samples/sec: 180.51\n",
            "2019-12-22 17:59:13,037 epoch 77 - iter 117/138 - loss 0.20406342 - samples/sec: 201.20\n",
            "2019-12-22 17:59:15,258 epoch 77 - iter 130/138 - loss 0.20456690 - samples/sec: 188.89\n",
            "2019-12-22 17:59:16,406 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:59:16,407 EPOCH 77 done: loss 0.2028 - lr 0.0500\n",
            "2019-12-22 17:59:16,408 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 17:59:16,409 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:59:16,584 epoch 78 - iter 0/138 - loss 0.29225928 - samples/sec: 2399.35\n",
            "2019-12-22 17:59:18,691 epoch 78 - iter 13/138 - loss 0.24124293 - samples/sec: 198.99\n",
            "2019-12-22 17:59:20,927 epoch 78 - iter 26/138 - loss 0.23570724 - samples/sec: 187.89\n",
            "2019-12-22 17:59:23,175 epoch 78 - iter 39/138 - loss 0.21752106 - samples/sec: 186.95\n",
            "2019-12-22 17:59:25,445 epoch 78 - iter 52/138 - loss 0.21270001 - samples/sec: 185.00\n",
            "2019-12-22 17:59:27,751 epoch 78 - iter 65/138 - loss 0.20902205 - samples/sec: 181.81\n",
            "2019-12-22 17:59:29,938 epoch 78 - iter 78/138 - loss 0.20315455 - samples/sec: 191.95\n",
            "2019-12-22 17:59:32,120 epoch 78 - iter 91/138 - loss 0.20927513 - samples/sec: 192.53\n",
            "2019-12-22 17:59:34,264 epoch 78 - iter 104/138 - loss 0.21036949 - samples/sec: 195.64\n",
            "2019-12-22 17:59:36,617 epoch 78 - iter 117/138 - loss 0.20884406 - samples/sec: 178.29\n",
            "2019-12-22 17:59:38,878 epoch 78 - iter 130/138 - loss 0.20949986 - samples/sec: 185.55\n",
            "2019-12-22 17:59:39,927 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:59:39,928 EPOCH 78 done: loss 0.2092 - lr 0.0500\n",
            "2019-12-22 17:59:39,929 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 17:59:39,930 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 17:59:40,096 epoch 79 - iter 0/138 - loss 0.20238125 - samples/sec: 2524.46\n",
            "2019-12-22 17:59:42,308 epoch 79 - iter 13/138 - loss 0.27406135 - samples/sec: 189.51\n",
            "2019-12-22 17:59:44,515 epoch 79 - iter 26/138 - loss 0.26914656 - samples/sec: 190.28\n",
            "2019-12-22 17:59:46,631 epoch 79 - iter 39/138 - loss 0.26559389 - samples/sec: 198.58\n",
            "2019-12-22 17:59:48,782 epoch 79 - iter 52/138 - loss 0.25244351 - samples/sec: 195.19\n",
            "2019-12-22 17:59:51,122 epoch 79 - iter 65/138 - loss 0.23982001 - samples/sec: 179.41\n",
            "2019-12-22 17:59:53,411 epoch 79 - iter 78/138 - loss 0.23847689 - samples/sec: 183.42\n",
            "2019-12-22 17:59:55,645 epoch 79 - iter 91/138 - loss 0.23360672 - samples/sec: 187.98\n",
            "2019-12-22 17:59:57,865 epoch 79 - iter 104/138 - loss 0.23103992 - samples/sec: 189.11\n",
            "2019-12-22 18:00:00,140 epoch 79 - iter 117/138 - loss 0.22514715 - samples/sec: 184.55\n",
            "2019-12-22 18:00:02,487 epoch 79 - iter 130/138 - loss 0.23043631 - samples/sec: 179.59\n",
            "2019-12-22 18:00:03,675 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:03,677 EPOCH 79 done: loss 0.2267 - lr 0.0500\n",
            "2019-12-22 18:00:03,678 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:00:03,679 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:03,852 epoch 80 - iter 0/138 - loss 0.04857755 - samples/sec: 2424.51\n",
            "2019-12-22 18:00:06,055 epoch 80 - iter 13/138 - loss 0.15799596 - samples/sec: 190.32\n",
            "2019-12-22 18:00:08,349 epoch 80 - iter 26/138 - loss 0.17069360 - samples/sec: 183.03\n",
            "2019-12-22 18:00:10,509 epoch 80 - iter 39/138 - loss 0.19210064 - samples/sec: 194.73\n",
            "2019-12-22 18:00:12,654 epoch 80 - iter 52/138 - loss 0.19316198 - samples/sec: 195.77\n",
            "2019-12-22 18:00:14,841 epoch 80 - iter 65/138 - loss 0.19497760 - samples/sec: 191.78\n",
            "2019-12-22 18:00:17,204 epoch 80 - iter 78/138 - loss 0.19667508 - samples/sec: 177.64\n",
            "2019-12-22 18:00:19,404 epoch 80 - iter 91/138 - loss 0.21181850 - samples/sec: 190.87\n",
            "2019-12-22 18:00:21,672 epoch 80 - iter 104/138 - loss 0.21590712 - samples/sec: 185.26\n",
            "2019-12-22 18:00:24,026 epoch 80 - iter 117/138 - loss 0.21042293 - samples/sec: 178.84\n",
            "2019-12-22 18:00:26,245 epoch 80 - iter 130/138 - loss 0.20815889 - samples/sec: 189.13\n",
            "2019-12-22 18:00:27,445 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:27,446 EPOCH 80 done: loss 0.2125 - lr 0.0500\n",
            "2019-12-22 18:00:27,447 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:00:27,448 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:27,609 epoch 81 - iter 0/138 - loss 0.35350156 - samples/sec: 2608.88\n",
            "2019-12-22 18:00:29,816 epoch 81 - iter 13/138 - loss 0.23685425 - samples/sec: 190.02\n",
            "2019-12-22 18:00:32,080 epoch 81 - iter 26/138 - loss 0.23699932 - samples/sec: 185.39\n",
            "2019-12-22 18:00:34,283 epoch 81 - iter 39/138 - loss 0.23692062 - samples/sec: 190.58\n",
            "2019-12-22 18:00:36,534 epoch 81 - iter 52/138 - loss 0.23964617 - samples/sec: 186.77\n",
            "2019-12-22 18:00:38,725 epoch 81 - iter 65/138 - loss 0.23987213 - samples/sec: 191.90\n",
            "2019-12-22 18:00:40,932 epoch 81 - iter 78/138 - loss 0.23920165 - samples/sec: 190.17\n",
            "2019-12-22 18:00:43,178 epoch 81 - iter 91/138 - loss 0.23714345 - samples/sec: 187.11\n",
            "2019-12-22 18:00:45,336 epoch 81 - iter 104/138 - loss 0.23090340 - samples/sec: 194.49\n",
            "2019-12-22 18:00:47,592 epoch 81 - iter 117/138 - loss 0.23097315 - samples/sec: 186.06\n",
            "2019-12-22 18:00:49,819 epoch 81 - iter 130/138 - loss 0.23341148 - samples/sec: 188.48\n",
            "2019-12-22 18:00:51,035 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:51,036 EPOCH 81 done: loss 0.2310 - lr 0.0500\n",
            "Epoch    80: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2019-12-22 18:00:51,037 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:00:51,039 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:00:51,237 epoch 82 - iter 0/138 - loss 0.13078696 - samples/sec: 2122.75\n",
            "2019-12-22 18:00:53,439 epoch 82 - iter 13/138 - loss 0.15764253 - samples/sec: 190.37\n",
            "2019-12-22 18:00:55,733 epoch 82 - iter 26/138 - loss 0.18948636 - samples/sec: 183.00\n",
            "2019-12-22 18:00:57,901 epoch 82 - iter 39/138 - loss 0.19261779 - samples/sec: 193.71\n",
            "2019-12-22 18:01:00,135 epoch 82 - iter 52/138 - loss 0.19534386 - samples/sec: 187.90\n",
            "2019-12-22 18:01:02,345 epoch 82 - iter 65/138 - loss 0.19963313 - samples/sec: 190.17\n",
            "2019-12-22 18:01:04,534 epoch 82 - iter 78/138 - loss 0.20155576 - samples/sec: 192.08\n",
            "2019-12-22 18:01:06,842 epoch 82 - iter 91/138 - loss 0.19650403 - samples/sec: 181.94\n",
            "2019-12-22 18:01:09,290 epoch 82 - iter 104/138 - loss 0.20592939 - samples/sec: 171.41\n",
            "2019-12-22 18:01:11,590 epoch 82 - iter 117/138 - loss 0.19747276 - samples/sec: 182.58\n",
            "2019-12-22 18:01:13,908 epoch 82 - iter 130/138 - loss 0.18975244 - samples/sec: 181.06\n",
            "2019-12-22 18:01:15,103 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:01:15,104 EPOCH 82 done: loss 0.1892 - lr 0.0250\n",
            "2019-12-22 18:01:15,109 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:01:15,114 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:01:15,307 epoch 83 - iter 0/138 - loss 0.38235426 - samples/sec: 2179.45\n",
            "2019-12-22 18:01:17,688 epoch 83 - iter 13/138 - loss 0.17922873 - samples/sec: 176.01\n",
            "2019-12-22 18:01:19,843 epoch 83 - iter 26/138 - loss 0.21107595 - samples/sec: 194.78\n",
            "2019-12-22 18:01:22,147 epoch 83 - iter 39/138 - loss 0.19128864 - samples/sec: 182.20\n",
            "2019-12-22 18:01:24,429 epoch 83 - iter 52/138 - loss 0.19962017 - samples/sec: 183.94\n",
            "2019-12-22 18:01:26,925 epoch 83 - iter 65/138 - loss 0.19555033 - samples/sec: 168.14\n",
            "2019-12-22 18:01:29,248 epoch 83 - iter 78/138 - loss 0.19592303 - samples/sec: 180.87\n",
            "2019-12-22 18:01:31,515 epoch 83 - iter 91/138 - loss 0.20707460 - samples/sec: 185.32\n",
            "2019-12-22 18:01:33,755 epoch 83 - iter 104/138 - loss 0.19979267 - samples/sec: 187.99\n",
            "2019-12-22 18:01:36,059 epoch 83 - iter 117/138 - loss 0.20055635 - samples/sec: 182.22\n",
            "2019-12-22 18:01:38,357 epoch 83 - iter 130/138 - loss 0.20212119 - samples/sec: 182.79\n",
            "2019-12-22 18:01:39,552 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:01:39,553 EPOCH 83 done: loss 0.2007 - lr 0.0250\n",
            "2019-12-22 18:01:39,555 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:01:39,555 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:01:39,770 epoch 84 - iter 0/138 - loss 0.35078967 - samples/sec: 1948.41\n",
            "2019-12-22 18:01:42,064 epoch 84 - iter 13/138 - loss 0.17055173 - samples/sec: 182.84\n",
            "2019-12-22 18:01:44,222 epoch 84 - iter 26/138 - loss 0.15356826 - samples/sec: 194.44\n",
            "2019-12-22 18:01:46,592 epoch 84 - iter 39/138 - loss 0.15848785 - samples/sec: 176.96\n",
            "2019-12-22 18:01:48,929 epoch 84 - iter 52/138 - loss 0.19030988 - samples/sec: 179.59\n",
            "2019-12-22 18:01:51,200 epoch 84 - iter 65/138 - loss 0.18995683 - samples/sec: 184.87\n",
            "2019-12-22 18:01:53,607 epoch 84 - iter 78/138 - loss 0.18536502 - samples/sec: 174.37\n",
            "2019-12-22 18:01:55,911 epoch 84 - iter 91/138 - loss 0.19788868 - samples/sec: 182.25\n",
            "2019-12-22 18:01:58,398 epoch 84 - iter 104/138 - loss 0.19867905 - samples/sec: 168.76\n",
            "2019-12-22 18:02:00,569 epoch 84 - iter 117/138 - loss 0.19579651 - samples/sec: 193.53\n",
            "2019-12-22 18:02:02,736 epoch 84 - iter 130/138 - loss 0.19319613 - samples/sec: 193.54\n",
            "2019-12-22 18:02:03,921 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:03,922 EPOCH 84 done: loss 0.1941 - lr 0.0250\n",
            "2019-12-22 18:02:03,923 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:02:03,924 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:04,094 epoch 85 - iter 0/138 - loss 0.36915684 - samples/sec: 2469.41\n",
            "2019-12-22 18:02:06,273 epoch 85 - iter 13/138 - loss 0.22409660 - samples/sec: 192.48\n",
            "2019-12-22 18:02:08,447 epoch 85 - iter 26/138 - loss 0.21513735 - samples/sec: 192.94\n",
            "2019-12-22 18:02:10,587 epoch 85 - iter 39/138 - loss 0.21923435 - samples/sec: 196.15\n",
            "2019-12-22 18:02:12,754 epoch 85 - iter 52/138 - loss 0.21151964 - samples/sec: 193.76\n",
            "2019-12-22 18:02:15,217 epoch 85 - iter 65/138 - loss 0.21946505 - samples/sec: 170.33\n",
            "2019-12-22 18:02:17,615 epoch 85 - iter 78/138 - loss 0.20820544 - samples/sec: 175.06\n",
            "2019-12-22 18:02:19,906 epoch 85 - iter 91/138 - loss 0.21335499 - samples/sec: 183.42\n",
            "2019-12-22 18:02:22,149 epoch 85 - iter 104/138 - loss 0.21003676 - samples/sec: 187.51\n",
            "2019-12-22 18:02:24,546 epoch 85 - iter 117/138 - loss 0.20590161 - samples/sec: 175.13\n",
            "2019-12-22 18:02:26,801 epoch 85 - iter 130/138 - loss 0.20527914 - samples/sec: 186.27\n",
            "2019-12-22 18:02:28,054 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:28,055 EPOCH 85 done: loss 0.2049 - lr 0.0250\n",
            "2019-12-22 18:02:28,056 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:02:28,057 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:28,262 epoch 86 - iter 0/138 - loss 0.12104481 - samples/sec: 2038.35\n",
            "2019-12-22 18:02:30,643 epoch 86 - iter 13/138 - loss 0.15864444 - samples/sec: 176.09\n",
            "2019-12-22 18:02:33,014 epoch 86 - iter 26/138 - loss 0.19374955 - samples/sec: 177.00\n",
            "2019-12-22 18:02:35,272 epoch 86 - iter 39/138 - loss 0.20315346 - samples/sec: 186.10\n",
            "2019-12-22 18:02:37,657 epoch 86 - iter 52/138 - loss 0.19530330 - samples/sec: 175.88\n",
            "2019-12-22 18:02:39,877 epoch 86 - iter 65/138 - loss 0.19630252 - samples/sec: 189.14\n",
            "2019-12-22 18:02:42,029 epoch 86 - iter 78/138 - loss 0.19575379 - samples/sec: 195.45\n",
            "2019-12-22 18:02:44,291 epoch 86 - iter 91/138 - loss 0.20538788 - samples/sec: 185.81\n",
            "2019-12-22 18:02:46,501 epoch 86 - iter 104/138 - loss 0.20359623 - samples/sec: 189.82\n",
            "2019-12-22 18:02:48,840 epoch 86 - iter 117/138 - loss 0.20094743 - samples/sec: 179.44\n",
            "2019-12-22 18:02:51,031 epoch 86 - iter 130/138 - loss 0.19839861 - samples/sec: 191.84\n",
            "2019-12-22 18:02:52,248 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:52,249 EPOCH 86 done: loss 0.1966 - lr 0.0250\n",
            "Epoch    85: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2019-12-22 18:02:52,250 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:02:52,251 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:02:52,450 epoch 87 - iter 0/138 - loss 0.20907176 - samples/sec: 2102.45\n",
            "2019-12-22 18:02:54,699 epoch 87 - iter 13/138 - loss 0.19430711 - samples/sec: 186.56\n",
            "2019-12-22 18:02:56,952 epoch 87 - iter 26/138 - loss 0.18445858 - samples/sec: 186.55\n",
            "2019-12-22 18:02:59,136 epoch 87 - iter 39/138 - loss 0.17727164 - samples/sec: 192.32\n",
            "2019-12-22 18:03:01,383 epoch 87 - iter 52/138 - loss 0.17697902 - samples/sec: 186.86\n",
            "2019-12-22 18:03:03,731 epoch 87 - iter 65/138 - loss 0.18020034 - samples/sec: 178.58\n",
            "2019-12-22 18:03:05,970 epoch 87 - iter 78/138 - loss 0.18206288 - samples/sec: 187.66\n",
            "2019-12-22 18:03:08,255 epoch 87 - iter 91/138 - loss 0.17816042 - samples/sec: 183.70\n",
            "2019-12-22 18:03:10,417 epoch 87 - iter 104/138 - loss 0.18409321 - samples/sec: 194.92\n",
            "2019-12-22 18:03:12,652 epoch 87 - iter 117/138 - loss 0.18438104 - samples/sec: 188.06\n",
            "2019-12-22 18:03:14,904 epoch 87 - iter 130/138 - loss 0.18616327 - samples/sec: 186.66\n",
            "2019-12-22 18:03:16,085 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:03:16,088 EPOCH 87 done: loss 0.1856 - lr 0.0125\n",
            "2019-12-22 18:03:16,090 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:03:16,092 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:03:16,254 epoch 88 - iter 0/138 - loss 0.20141768 - samples/sec: 2609.90\n",
            "2019-12-22 18:03:18,527 epoch 88 - iter 13/138 - loss 0.15316754 - samples/sec: 184.69\n",
            "2019-12-22 18:03:20,757 epoch 88 - iter 26/138 - loss 0.14593391 - samples/sec: 189.16\n",
            "2019-12-22 18:03:23,080 epoch 88 - iter 39/138 - loss 0.14851305 - samples/sec: 180.63\n",
            "2019-12-22 18:03:25,260 epoch 88 - iter 52/138 - loss 0.15937572 - samples/sec: 192.93\n",
            "2019-12-22 18:03:27,614 epoch 88 - iter 65/138 - loss 0.17593933 - samples/sec: 178.58\n",
            "2019-12-22 18:03:29,781 epoch 88 - iter 78/138 - loss 0.18107892 - samples/sec: 193.56\n",
            "2019-12-22 18:03:31,993 epoch 88 - iter 91/138 - loss 0.19151220 - samples/sec: 189.87\n",
            "2019-12-22 18:03:34,216 epoch 88 - iter 104/138 - loss 0.19532949 - samples/sec: 188.84\n",
            "2019-12-22 18:03:36,474 epoch 88 - iter 117/138 - loss 0.19016727 - samples/sec: 185.88\n",
            "2019-12-22 18:03:38,727 epoch 88 - iter 130/138 - loss 0.18630481 - samples/sec: 186.28\n",
            "2019-12-22 18:03:39,857 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:03:39,858 EPOCH 88 done: loss 0.1843 - lr 0.0125\n",
            "2019-12-22 18:03:39,859 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:03:39,860 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:03:40,050 epoch 89 - iter 0/138 - loss 0.20987558 - samples/sec: 2213.66\n",
            "2019-12-22 18:03:42,301 epoch 89 - iter 13/138 - loss 0.17135133 - samples/sec: 186.49\n",
            "2019-12-22 18:03:44,563 epoch 89 - iter 26/138 - loss 0.16279010 - samples/sec: 185.52\n",
            "2019-12-22 18:03:46,782 epoch 89 - iter 39/138 - loss 0.19319631 - samples/sec: 189.10\n",
            "2019-12-22 18:03:48,996 epoch 89 - iter 52/138 - loss 0.18116852 - samples/sec: 189.55\n",
            "2019-12-22 18:03:51,240 epoch 89 - iter 65/138 - loss 0.18246654 - samples/sec: 186.96\n",
            "2019-12-22 18:03:53,481 epoch 89 - iter 78/138 - loss 0.18314777 - samples/sec: 187.41\n",
            "2019-12-22 18:03:55,821 epoch 89 - iter 91/138 - loss 0.18264067 - samples/sec: 179.47\n",
            "2019-12-22 18:03:58,044 epoch 89 - iter 104/138 - loss 0.18523837 - samples/sec: 188.85\n",
            "2019-12-22 18:04:00,172 epoch 89 - iter 117/138 - loss 0.18402989 - samples/sec: 197.36\n",
            "2019-12-22 18:04:02,509 epoch 89 - iter 130/138 - loss 0.18541874 - samples/sec: 179.69\n",
            "2019-12-22 18:04:03,655 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:03,656 EPOCH 89 done: loss 0.1855 - lr 0.0125\n",
            "2019-12-22 18:04:03,657 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:04:03,658 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:03,825 epoch 90 - iter 0/138 - loss 0.10735929 - samples/sec: 2537.92\n",
            "2019-12-22 18:04:05,979 epoch 90 - iter 13/138 - loss 0.21455076 - samples/sec: 194.69\n",
            "2019-12-22 18:04:08,185 epoch 90 - iter 26/138 - loss 0.21195897 - samples/sec: 190.37\n",
            "2019-12-22 18:04:10,351 epoch 90 - iter 39/138 - loss 0.19942052 - samples/sec: 193.89\n",
            "2019-12-22 18:04:12,591 epoch 90 - iter 52/138 - loss 0.20787973 - samples/sec: 187.24\n",
            "2019-12-22 18:04:14,686 epoch 90 - iter 65/138 - loss 0.19675495 - samples/sec: 200.40\n",
            "2019-12-22 18:04:16,926 epoch 90 - iter 78/138 - loss 0.18964919 - samples/sec: 187.27\n",
            "2019-12-22 18:04:19,137 epoch 90 - iter 91/138 - loss 0.19168678 - samples/sec: 190.52\n",
            "2019-12-22 18:04:21,194 epoch 90 - iter 104/138 - loss 0.19452263 - samples/sec: 203.98\n",
            "2019-12-22 18:04:23,386 epoch 90 - iter 117/138 - loss 0.19614453 - samples/sec: 191.47\n",
            "2019-12-22 18:04:25,542 epoch 90 - iter 130/138 - loss 0.19867085 - samples/sec: 194.77\n",
            "2019-12-22 18:04:26,673 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:26,674 EPOCH 90 done: loss 0.2023 - lr 0.0125\n",
            "2019-12-22 18:04:26,675 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:04:26,676 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:26,844 epoch 91 - iter 0/138 - loss 0.11342418 - samples/sec: 2498.85\n",
            "2019-12-22 18:04:29,040 epoch 91 - iter 13/138 - loss 0.15559993 - samples/sec: 190.85\n",
            "2019-12-22 18:04:31,252 epoch 91 - iter 26/138 - loss 0.16980483 - samples/sec: 189.99\n",
            "2019-12-22 18:04:33,490 epoch 91 - iter 39/138 - loss 0.18996052 - samples/sec: 187.70\n",
            "2019-12-22 18:04:35,661 epoch 91 - iter 52/138 - loss 0.18741877 - samples/sec: 193.52\n",
            "2019-12-22 18:04:37,852 epoch 91 - iter 65/138 - loss 0.17682135 - samples/sec: 192.11\n",
            "2019-12-22 18:04:40,043 epoch 91 - iter 78/138 - loss 0.17437188 - samples/sec: 191.67\n",
            "2019-12-22 18:04:42,186 epoch 91 - iter 91/138 - loss 0.17603338 - samples/sec: 195.97\n",
            "2019-12-22 18:04:44,377 epoch 91 - iter 104/138 - loss 0.18044795 - samples/sec: 191.86\n",
            "2019-12-22 18:04:46,594 epoch 91 - iter 117/138 - loss 0.17711985 - samples/sec: 189.59\n",
            "2019-12-22 18:04:50,787 epoch 91 - iter 130/138 - loss 0.18031474 - samples/sec: 99.67\n",
            "2019-12-22 18:04:51,875 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:51,876 EPOCH 91 done: loss 0.1799 - lr 0.0125\n",
            "2019-12-22 18:04:51,877 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:04:51,878 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:04:52,020 epoch 92 - iter 0/138 - loss 0.12102771 - samples/sec: 2943.46\n",
            "2019-12-22 18:04:54,148 epoch 92 - iter 13/138 - loss 0.18185897 - samples/sec: 197.12\n",
            "2019-12-22 18:04:56,258 epoch 92 - iter 26/138 - loss 0.17684363 - samples/sec: 199.25\n",
            "2019-12-22 18:04:58,403 epoch 92 - iter 39/138 - loss 0.16638151 - samples/sec: 196.19\n",
            "2019-12-22 18:05:00,677 epoch 92 - iter 52/138 - loss 0.17353525 - samples/sec: 184.53\n",
            "2019-12-22 18:05:02,787 epoch 92 - iter 65/138 - loss 0.17921838 - samples/sec: 198.92\n",
            "2019-12-22 18:05:05,035 epoch 92 - iter 78/138 - loss 0.19143097 - samples/sec: 186.69\n",
            "2019-12-22 18:05:07,226 epoch 92 - iter 91/138 - loss 0.18481225 - samples/sec: 191.58\n",
            "2019-12-22 18:05:09,543 epoch 92 - iter 104/138 - loss 0.19245938 - samples/sec: 181.27\n",
            "2019-12-22 18:05:11,783 epoch 92 - iter 117/138 - loss 0.19079614 - samples/sec: 187.41\n",
            "2019-12-22 18:05:14,051 epoch 92 - iter 130/138 - loss 0.19141183 - samples/sec: 184.87\n",
            "2019-12-22 18:05:15,267 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:05:15,268 EPOCH 92 done: loss 0.1906 - lr 0.0125\n",
            "2019-12-22 18:05:15,269 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:05:15,270 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:05:15,432 epoch 93 - iter 0/138 - loss 0.30095363 - samples/sec: 2591.13\n",
            "2019-12-22 18:05:17,570 epoch 93 - iter 13/138 - loss 0.16317114 - samples/sec: 196.18\n",
            "2019-12-22 18:05:19,779 epoch 93 - iter 26/138 - loss 0.15359850 - samples/sec: 190.00\n",
            "2019-12-22 18:05:22,003 epoch 93 - iter 39/138 - loss 0.15831786 - samples/sec: 188.72\n",
            "2019-12-22 18:05:24,220 epoch 93 - iter 52/138 - loss 0.16954182 - samples/sec: 189.32\n",
            "2019-12-22 18:05:26,323 epoch 93 - iter 65/138 - loss 0.17365750 - samples/sec: 199.73\n",
            "2019-12-22 18:05:28,504 epoch 93 - iter 78/138 - loss 0.17663771 - samples/sec: 192.47\n",
            "2019-12-22 18:05:30,771 epoch 93 - iter 91/138 - loss 0.18454919 - samples/sec: 185.08\n",
            "2019-12-22 18:05:32,991 epoch 93 - iter 104/138 - loss 0.18954961 - samples/sec: 189.04\n",
            "2019-12-22 18:05:35,113 epoch 93 - iter 117/138 - loss 0.18871439 - samples/sec: 197.99\n",
            "2019-12-22 18:05:37,276 epoch 93 - iter 130/138 - loss 0.18611208 - samples/sec: 194.49\n",
            "2019-12-22 18:05:38,441 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:05:38,442 EPOCH 93 done: loss 0.1807 - lr 0.0125\n",
            "2019-12-22 18:05:38,443 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:05:38,444 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:05:38,619 epoch 94 - iter 0/138 - loss 0.07420921 - samples/sec: 2415.59\n",
            "2019-12-22 18:05:40,798 epoch 94 - iter 13/138 - loss 0.15825565 - samples/sec: 192.43\n",
            "2019-12-22 18:05:42,875 epoch 94 - iter 26/138 - loss 0.14555393 - samples/sec: 202.29\n",
            "2019-12-22 18:05:45,115 epoch 94 - iter 39/138 - loss 0.15520985 - samples/sec: 187.40\n",
            "2019-12-22 18:05:47,344 epoch 94 - iter 52/138 - loss 0.15495053 - samples/sec: 188.25\n",
            "2019-12-22 18:05:49,665 epoch 94 - iter 65/138 - loss 0.16382580 - samples/sec: 180.74\n",
            "2019-12-22 18:05:51,877 epoch 94 - iter 78/138 - loss 0.16384768 - samples/sec: 190.16\n",
            "2019-12-22 18:05:54,092 epoch 94 - iter 91/138 - loss 0.17367403 - samples/sec: 189.55\n",
            "2019-12-22 18:05:56,479 epoch 94 - iter 104/138 - loss 0.17241305 - samples/sec: 175.85\n",
            "2019-12-22 18:05:58,729 epoch 94 - iter 117/138 - loss 0.16961794 - samples/sec: 186.79\n",
            "2019-12-22 18:06:00,983 epoch 94 - iter 130/138 - loss 0.17428192 - samples/sec: 186.60\n",
            "2019-12-22 18:06:02,128 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:02,129 EPOCH 94 done: loss 0.1738 - lr 0.0125\n",
            "2019-12-22 18:06:02,130 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:06:02,131 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:02,304 epoch 95 - iter 0/138 - loss 0.11916155 - samples/sec: 2433.31\n",
            "2019-12-22 18:06:04,505 epoch 95 - iter 13/138 - loss 0.19248999 - samples/sec: 190.57\n",
            "2019-12-22 18:06:06,784 epoch 95 - iter 26/138 - loss 0.19740456 - samples/sec: 183.96\n",
            "2019-12-22 18:06:09,009 epoch 95 - iter 39/138 - loss 0.18251035 - samples/sec: 188.55\n",
            "2019-12-22 18:06:11,220 epoch 95 - iter 52/138 - loss 0.18359172 - samples/sec: 189.73\n",
            "2019-12-22 18:06:13,439 epoch 95 - iter 65/138 - loss 0.19469167 - samples/sec: 189.23\n",
            "2019-12-22 18:06:15,663 epoch 95 - iter 78/138 - loss 0.19443769 - samples/sec: 188.76\n",
            "2019-12-22 18:06:17,922 epoch 95 - iter 91/138 - loss 0.19682233 - samples/sec: 185.88\n",
            "2019-12-22 18:06:20,071 epoch 95 - iter 104/138 - loss 0.19727810 - samples/sec: 195.43\n",
            "2019-12-22 18:06:22,240 epoch 95 - iter 117/138 - loss 0.19684536 - samples/sec: 193.58\n",
            "2019-12-22 18:06:24,485 epoch 95 - iter 130/138 - loss 0.19816935 - samples/sec: 187.03\n",
            "2019-12-22 18:06:25,655 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:25,656 EPOCH 95 done: loss 0.1961 - lr 0.0125\n",
            "2019-12-22 18:06:25,656 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:06:25,657 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:25,830 epoch 96 - iter 0/138 - loss 0.63260150 - samples/sec: 2430.15\n",
            "2019-12-22 18:06:27,980 epoch 96 - iter 13/138 - loss 0.21664113 - samples/sec: 195.06\n",
            "2019-12-22 18:06:30,150 epoch 96 - iter 26/138 - loss 0.17442555 - samples/sec: 193.79\n",
            "2019-12-22 18:06:32,444 epoch 96 - iter 39/138 - loss 0.18796735 - samples/sec: 182.94\n",
            "2019-12-22 18:06:34,644 epoch 96 - iter 52/138 - loss 0.19055819 - samples/sec: 190.63\n",
            "2019-12-22 18:06:36,932 epoch 96 - iter 65/138 - loss 0.18190957 - samples/sec: 183.77\n",
            "2019-12-22 18:06:39,221 epoch 96 - iter 78/138 - loss 0.18273437 - samples/sec: 183.34\n",
            "2019-12-22 18:06:41,383 epoch 96 - iter 91/138 - loss 0.18252902 - samples/sec: 194.14\n",
            "2019-12-22 18:06:43,723 epoch 96 - iter 104/138 - loss 0.17897609 - samples/sec: 179.23\n",
            "2019-12-22 18:06:45,795 epoch 96 - iter 117/138 - loss 0.18252167 - samples/sec: 202.96\n",
            "2019-12-22 18:06:48,082 epoch 96 - iter 130/138 - loss 0.18747733 - samples/sec: 183.35\n",
            "2019-12-22 18:06:49,258 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:49,259 EPOCH 96 done: loss 0.1877 - lr 0.0125\n",
            "2019-12-22 18:06:49,260 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:06:49,260 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:06:49,427 epoch 97 - iter 0/138 - loss 0.14333415 - samples/sec: 2516.43\n",
            "2019-12-22 18:06:51,567 epoch 97 - iter 13/138 - loss 0.16651177 - samples/sec: 196.17\n",
            "2019-12-22 18:06:53,804 epoch 97 - iter 26/138 - loss 0.19189312 - samples/sec: 188.07\n",
            "2019-12-22 18:06:56,059 epoch 97 - iter 39/138 - loss 0.19005041 - samples/sec: 186.30\n",
            "2019-12-22 18:06:58,344 epoch 97 - iter 52/138 - loss 0.17850826 - samples/sec: 184.05\n",
            "2019-12-22 18:07:00,499 epoch 97 - iter 65/138 - loss 0.18100975 - samples/sec: 194.98\n",
            "2019-12-22 18:07:02,899 epoch 97 - iter 78/138 - loss 0.19042451 - samples/sec: 175.23\n",
            "2019-12-22 18:07:05,143 epoch 97 - iter 91/138 - loss 0.18728102 - samples/sec: 187.07\n",
            "2019-12-22 18:07:07,365 epoch 97 - iter 104/138 - loss 0.18286910 - samples/sec: 188.89\n",
            "2019-12-22 18:07:09,723 epoch 97 - iter 117/138 - loss 0.18270950 - samples/sec: 178.06\n",
            "2019-12-22 18:07:11,907 epoch 97 - iter 130/138 - loss 0.17700630 - samples/sec: 193.01\n",
            "2019-12-22 18:07:13,013 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:07:13,014 EPOCH 97 done: loss 0.1796 - lr 0.0125\n",
            "2019-12-22 18:07:13,015 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:07:13,016 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:07:13,176 epoch 98 - iter 0/138 - loss 0.07876909 - samples/sec: 2625.78\n",
            "2019-12-22 18:07:15,309 epoch 98 - iter 13/138 - loss 0.18669791 - samples/sec: 196.71\n",
            "2019-12-22 18:07:17,562 epoch 98 - iter 26/138 - loss 0.17215399 - samples/sec: 186.50\n",
            "2019-12-22 18:07:19,912 epoch 98 - iter 39/138 - loss 0.16612144 - samples/sec: 178.45\n",
            "2019-12-22 18:07:22,053 epoch 98 - iter 52/138 - loss 0.17156775 - samples/sec: 196.08\n",
            "2019-12-22 18:07:24,323 epoch 98 - iter 65/138 - loss 0.16292203 - samples/sec: 185.25\n",
            "2019-12-22 18:07:26,510 epoch 98 - iter 78/138 - loss 0.17587785 - samples/sec: 192.01\n",
            "2019-12-22 18:07:28,624 epoch 98 - iter 91/138 - loss 0.18530190 - samples/sec: 198.75\n",
            "2019-12-22 18:07:30,850 epoch 98 - iter 104/138 - loss 0.18822312 - samples/sec: 188.82\n",
            "2019-12-22 18:07:33,052 epoch 98 - iter 117/138 - loss 0.18934448 - samples/sec: 190.83\n",
            "2019-12-22 18:07:35,374 epoch 98 - iter 130/138 - loss 0.19072925 - samples/sec: 180.62\n",
            "2019-12-22 18:07:36,509 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:07:36,510 EPOCH 98 done: loss 0.1916 - lr 0.0125\n",
            "Epoch    97: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2019-12-22 18:07:36,511 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:07:36,512 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:07:36,681 epoch 99 - iter 0/138 - loss 0.19245458 - samples/sec: 2484.94\n",
            "2019-12-22 18:07:38,862 epoch 99 - iter 13/138 - loss 0.14532703 - samples/sec: 192.28\n",
            "2019-12-22 18:07:41,047 epoch 99 - iter 26/138 - loss 0.16955021 - samples/sec: 192.10\n",
            "2019-12-22 18:07:43,382 epoch 99 - iter 39/138 - loss 0.16930576 - samples/sec: 179.92\n",
            "2019-12-22 18:07:45,765 epoch 99 - iter 52/138 - loss 0.16220292 - samples/sec: 176.18\n",
            "2019-12-22 18:07:48,003 epoch 99 - iter 65/138 - loss 0.17273545 - samples/sec: 187.47\n",
            "2019-12-22 18:07:50,213 epoch 99 - iter 78/138 - loss 0.16860928 - samples/sec: 189.96\n",
            "2019-12-22 18:07:52,427 epoch 99 - iter 91/138 - loss 0.16439773 - samples/sec: 189.62\n",
            "2019-12-22 18:07:54,659 epoch 99 - iter 104/138 - loss 0.15529779 - samples/sec: 188.27\n",
            "2019-12-22 18:07:56,776 epoch 99 - iter 117/138 - loss 0.15875938 - samples/sec: 198.33\n",
            "2019-12-22 18:07:58,952 epoch 99 - iter 130/138 - loss 0.16398009 - samples/sec: 192.91\n",
            "2019-12-22 18:08:00,126 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:00,127 EPOCH 99 done: loss 0.1621 - lr 0.0063\n",
            "2019-12-22 18:08:00,129 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:08:00,130 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:00,326 epoch 100 - iter 0/138 - loss 0.68372726 - samples/sec: 2135.54\n",
            "2019-12-22 18:08:02,700 epoch 100 - iter 13/138 - loss 0.19945984 - samples/sec: 176.82\n",
            "2019-12-22 18:08:04,936 epoch 100 - iter 26/138 - loss 0.19465550 - samples/sec: 187.73\n",
            "2019-12-22 18:08:07,100 epoch 100 - iter 39/138 - loss 0.20626100 - samples/sec: 194.07\n",
            "2019-12-22 18:08:09,382 epoch 100 - iter 52/138 - loss 0.20200477 - samples/sec: 183.88\n",
            "2019-12-22 18:08:11,642 epoch 100 - iter 65/138 - loss 0.18957519 - samples/sec: 185.72\n",
            "2019-12-22 18:08:13,841 epoch 100 - iter 78/138 - loss 0.19512152 - samples/sec: 190.86\n",
            "2019-12-22 18:08:15,994 epoch 100 - iter 91/138 - loss 0.19563667 - samples/sec: 195.07\n",
            "2019-12-22 18:08:18,146 epoch 100 - iter 104/138 - loss 0.19068551 - samples/sec: 194.95\n",
            "2019-12-22 18:08:20,272 epoch 100 - iter 117/138 - loss 0.18195570 - samples/sec: 197.53\n",
            "2019-12-22 18:08:22,505 epoch 100 - iter 130/138 - loss 0.18485871 - samples/sec: 187.77\n",
            "2019-12-22 18:08:23,720 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:23,721 EPOCH 100 done: loss 0.1829 - lr 0.0063\n",
            "2019-12-22 18:08:23,722 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:08:23,723 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:23,971 epoch 101 - iter 0/138 - loss 0.29997253 - samples/sec: 1689.77\n",
            "2019-12-22 18:08:26,107 epoch 101 - iter 13/138 - loss 0.22086229 - samples/sec: 196.33\n",
            "2019-12-22 18:08:28,399 epoch 101 - iter 26/138 - loss 0.17943752 - samples/sec: 183.21\n",
            "2019-12-22 18:08:30,492 epoch 101 - iter 39/138 - loss 0.15877590 - samples/sec: 200.77\n",
            "2019-12-22 18:08:32,760 epoch 101 - iter 52/138 - loss 0.16391110 - samples/sec: 184.82\n",
            "2019-12-22 18:08:34,900 epoch 101 - iter 65/138 - loss 0.16941730 - samples/sec: 196.07\n",
            "2019-12-22 18:08:37,199 epoch 101 - iter 78/138 - loss 0.16847072 - samples/sec: 182.47\n",
            "2019-12-22 18:08:39,523 epoch 101 - iter 91/138 - loss 0.16020290 - samples/sec: 180.58\n",
            "2019-12-22 18:08:41,753 epoch 101 - iter 104/138 - loss 0.15613974 - samples/sec: 188.43\n",
            "2019-12-22 18:08:44,036 epoch 101 - iter 117/138 - loss 0.15311147 - samples/sec: 184.00\n",
            "2019-12-22 18:08:46,112 epoch 101 - iter 130/138 - loss 0.15678540 - samples/sec: 202.66\n",
            "2019-12-22 18:08:47,194 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:47,195 EPOCH 101 done: loss 0.1567 - lr 0.0063\n",
            "2019-12-22 18:08:47,200 BAD EPOCHS (no improvement): 0\n",
            "2019-12-22 18:08:47,203 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:08:47,374 epoch 102 - iter 0/138 - loss 0.27970576 - samples/sec: 2498.57\n",
            "2019-12-22 18:08:49,538 epoch 102 - iter 13/138 - loss 0.23424957 - samples/sec: 193.79\n",
            "2019-12-22 18:08:51,629 epoch 102 - iter 26/138 - loss 0.21469110 - samples/sec: 200.88\n",
            "2019-12-22 18:08:53,885 epoch 102 - iter 39/138 - loss 0.18187619 - samples/sec: 186.11\n",
            "2019-12-22 18:08:56,000 epoch 102 - iter 52/138 - loss 0.17688097 - samples/sec: 198.53\n",
            "2019-12-22 18:08:58,211 epoch 102 - iter 65/138 - loss 0.18234806 - samples/sec: 189.84\n",
            "2019-12-22 18:09:00,470 epoch 102 - iter 78/138 - loss 0.18124513 - samples/sec: 185.96\n",
            "2019-12-22 18:09:02,573 epoch 102 - iter 91/138 - loss 0.18412051 - samples/sec: 199.68\n",
            "2019-12-22 18:09:04,841 epoch 102 - iter 104/138 - loss 0.18085605 - samples/sec: 185.11\n",
            "2019-12-22 18:09:06,992 epoch 102 - iter 117/138 - loss 0.17878896 - samples/sec: 195.79\n",
            "2019-12-22 18:09:09,297 epoch 102 - iter 130/138 - loss 0.17322952 - samples/sec: 182.16\n",
            "2019-12-22 18:09:10,445 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:10,448 EPOCH 102 done: loss 0.1714 - lr 0.0063\n",
            "2019-12-22 18:09:10,450 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:09:10,452 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:10,640 epoch 103 - iter 0/138 - loss 0.32857931 - samples/sec: 2254.09\n",
            "2019-12-22 18:09:12,920 epoch 103 - iter 13/138 - loss 0.15730415 - samples/sec: 183.96\n",
            "2019-12-22 18:09:15,034 epoch 103 - iter 26/138 - loss 0.16471836 - samples/sec: 198.52\n",
            "2019-12-22 18:09:17,413 epoch 103 - iter 39/138 - loss 0.17199911 - samples/sec: 176.50\n",
            "2019-12-22 18:09:19,491 epoch 103 - iter 52/138 - loss 0.19387432 - samples/sec: 201.95\n",
            "2019-12-22 18:09:21,751 epoch 103 - iter 65/138 - loss 0.18790000 - samples/sec: 185.73\n",
            "2019-12-22 18:09:24,049 epoch 103 - iter 78/138 - loss 0.18885009 - samples/sec: 182.88\n",
            "2019-12-22 18:09:26,169 epoch 103 - iter 91/138 - loss 0.19693566 - samples/sec: 198.47\n",
            "2019-12-22 18:09:28,372 epoch 103 - iter 104/138 - loss 0.19287671 - samples/sec: 190.74\n",
            "2019-12-22 18:09:30,589 epoch 103 - iter 117/138 - loss 0.19139302 - samples/sec: 189.37\n",
            "2019-12-22 18:09:32,852 epoch 103 - iter 130/138 - loss 0.19417415 - samples/sec: 185.34\n",
            "2019-12-22 18:09:33,980 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:33,981 EPOCH 103 done: loss 0.1940 - lr 0.0063\n",
            "2019-12-22 18:09:33,982 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:09:33,983 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:34,141 epoch 104 - iter 0/138 - loss 0.08247840 - samples/sec: 2654.40\n",
            "2019-12-22 18:09:36,372 epoch 104 - iter 13/138 - loss 0.13109142 - samples/sec: 188.00\n",
            "2019-12-22 18:09:38,591 epoch 104 - iter 26/138 - loss 0.14723972 - samples/sec: 189.10\n",
            "2019-12-22 18:09:40,877 epoch 104 - iter 39/138 - loss 0.15018701 - samples/sec: 183.83\n",
            "2019-12-22 18:09:42,967 epoch 104 - iter 52/138 - loss 0.15722414 - samples/sec: 200.95\n",
            "2019-12-22 18:09:45,229 epoch 104 - iter 65/138 - loss 0.16975144 - samples/sec: 185.69\n",
            "2019-12-22 18:09:47,490 epoch 104 - iter 78/138 - loss 0.16366334 - samples/sec: 186.11\n",
            "2019-12-22 18:09:49,679 epoch 104 - iter 91/138 - loss 0.16882161 - samples/sec: 191.73\n",
            "2019-12-22 18:09:51,728 epoch 104 - iter 104/138 - loss 0.17430222 - samples/sec: 205.00\n",
            "2019-12-22 18:09:53,870 epoch 104 - iter 117/138 - loss 0.17092432 - samples/sec: 196.09\n",
            "2019-12-22 18:09:56,048 epoch 104 - iter 130/138 - loss 0.17175438 - samples/sec: 192.84\n",
            "2019-12-22 18:09:57,317 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:57,318 EPOCH 104 done: loss 0.1744 - lr 0.0063\n",
            "2019-12-22 18:09:57,318 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:09:57,319 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:09:57,506 epoch 105 - iter 0/138 - loss 0.11391580 - samples/sec: 2260.07\n",
            "2019-12-22 18:09:59,696 epoch 105 - iter 13/138 - loss 0.17056095 - samples/sec: 191.44\n",
            "2019-12-22 18:10:01,869 epoch 105 - iter 26/138 - loss 0.19109454 - samples/sec: 193.35\n",
            "2019-12-22 18:10:03,994 epoch 105 - iter 39/138 - loss 0.17072407 - samples/sec: 197.61\n",
            "2019-12-22 18:10:06,161 epoch 105 - iter 52/138 - loss 0.18261580 - samples/sec: 193.93\n",
            "2019-12-22 18:10:08,443 epoch 105 - iter 65/138 - loss 0.18367113 - samples/sec: 183.97\n",
            "2019-12-22 18:10:10,693 epoch 105 - iter 78/138 - loss 0.17417175 - samples/sec: 186.76\n",
            "2019-12-22 18:10:12,830 epoch 105 - iter 91/138 - loss 0.17300596 - samples/sec: 196.45\n",
            "2019-12-22 18:10:15,007 epoch 105 - iter 104/138 - loss 0.16757126 - samples/sec: 192.91\n",
            "2019-12-22 18:10:17,262 epoch 105 - iter 117/138 - loss 0.16375204 - samples/sec: 186.04\n",
            "2019-12-22 18:10:19,481 epoch 105 - iter 130/138 - loss 0.16535461 - samples/sec: 189.56\n",
            "2019-12-22 18:10:20,649 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:10:20,650 EPOCH 105 done: loss 0.1664 - lr 0.0063\n",
            "Epoch   104: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2019-12-22 18:10:20,651 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:10:20,652 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:10:20,845 epoch 106 - iter 0/138 - loss 0.22055888 - samples/sec: 2171.07\n",
            "2019-12-22 18:10:23,041 epoch 106 - iter 13/138 - loss 0.19358668 - samples/sec: 191.15\n",
            "2019-12-22 18:10:25,252 epoch 106 - iter 26/138 - loss 0.19351745 - samples/sec: 189.98\n",
            "2019-12-22 18:10:27,598 epoch 106 - iter 39/138 - loss 0.19175835 - samples/sec: 179.02\n",
            "2019-12-22 18:10:29,676 epoch 106 - iter 52/138 - loss 0.18924049 - samples/sec: 202.06\n",
            "2019-12-22 18:10:31,885 epoch 106 - iter 65/138 - loss 0.18004821 - samples/sec: 190.06\n",
            "2019-12-22 18:10:34,081 epoch 106 - iter 78/138 - loss 0.17302291 - samples/sec: 191.31\n",
            "2019-12-22 18:10:36,413 epoch 106 - iter 91/138 - loss 0.17037542 - samples/sec: 179.95\n",
            "2019-12-22 18:10:38,637 epoch 106 - iter 104/138 - loss 0.17185524 - samples/sec: 188.63\n",
            "2019-12-22 18:10:40,931 epoch 106 - iter 117/138 - loss 0.17537051 - samples/sec: 182.91\n",
            "2019-12-22 18:10:43,064 epoch 106 - iter 130/138 - loss 0.17809267 - samples/sec: 196.98\n",
            "2019-12-22 18:10:44,192 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:10:44,197 EPOCH 106 done: loss 0.1787 - lr 0.0031\n",
            "2019-12-22 18:10:44,200 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:10:44,204 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:10:44,394 epoch 107 - iter 0/138 - loss 0.13076866 - samples/sec: 2198.89\n",
            "2019-12-22 18:10:46,500 epoch 107 - iter 13/138 - loss 0.17330436 - samples/sec: 199.25\n",
            "2019-12-22 18:10:48,641 epoch 107 - iter 26/138 - loss 0.17937601 - samples/sec: 196.07\n",
            "2019-12-22 18:10:50,929 epoch 107 - iter 39/138 - loss 0.16625640 - samples/sec: 183.56\n",
            "2019-12-22 18:10:53,323 epoch 107 - iter 52/138 - loss 0.17711756 - samples/sec: 175.29\n",
            "2019-12-22 18:10:55,562 epoch 107 - iter 65/138 - loss 0.18233237 - samples/sec: 187.52\n",
            "2019-12-22 18:10:57,876 epoch 107 - iter 78/138 - loss 0.17792438 - samples/sec: 181.26\n",
            "2019-12-22 18:11:00,032 epoch 107 - iter 91/138 - loss 0.17948949 - samples/sec: 194.48\n",
            "2019-12-22 18:11:02,191 epoch 107 - iter 104/138 - loss 0.18346650 - samples/sec: 194.45\n",
            "2019-12-22 18:11:04,352 epoch 107 - iter 117/138 - loss 0.18333179 - samples/sec: 194.64\n",
            "2019-12-22 18:11:06,528 epoch 107 - iter 130/138 - loss 0.18157185 - samples/sec: 193.22\n",
            "2019-12-22 18:11:07,730 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:07,732 EPOCH 107 done: loss 0.1807 - lr 0.0031\n",
            "2019-12-22 18:11:07,732 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:11:07,733 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:07,908 epoch 108 - iter 0/138 - loss 0.14329529 - samples/sec: 2407.14\n",
            "2019-12-22 18:11:10,066 epoch 108 - iter 13/138 - loss 0.15174914 - samples/sec: 194.34\n",
            "2019-12-22 18:11:12,227 epoch 108 - iter 26/138 - loss 0.15439328 - samples/sec: 194.33\n",
            "2019-12-22 18:11:14,555 epoch 108 - iter 39/138 - loss 0.15960645 - samples/sec: 180.40\n",
            "2019-12-22 18:11:16,730 epoch 108 - iter 52/138 - loss 0.19241614 - samples/sec: 193.01\n",
            "2019-12-22 18:11:18,852 epoch 108 - iter 65/138 - loss 0.19045564 - samples/sec: 197.98\n",
            "2019-12-22 18:11:21,037 epoch 108 - iter 78/138 - loss 0.19141706 - samples/sec: 192.15\n",
            "2019-12-22 18:11:23,322 epoch 108 - iter 91/138 - loss 0.19408504 - samples/sec: 183.69\n",
            "2019-12-22 18:11:25,577 epoch 108 - iter 104/138 - loss 0.19843757 - samples/sec: 185.96\n",
            "2019-12-22 18:11:27,889 epoch 108 - iter 117/138 - loss 0.19824436 - samples/sec: 181.51\n",
            "2019-12-22 18:11:30,001 epoch 108 - iter 130/138 - loss 0.19390441 - samples/sec: 198.84\n",
            "2019-12-22 18:11:31,179 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:31,181 EPOCH 108 done: loss 0.1913 - lr 0.0031\n",
            "2019-12-22 18:11:31,183 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:11:31,186 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:31,334 epoch 109 - iter 0/138 - loss 0.05150652 - samples/sec: 2922.17\n",
            "2019-12-22 18:11:33,597 epoch 109 - iter 13/138 - loss 0.16940929 - samples/sec: 185.27\n",
            "2019-12-22 18:11:35,875 epoch 109 - iter 26/138 - loss 0.17707674 - samples/sec: 184.22\n",
            "2019-12-22 18:11:38,063 epoch 109 - iter 39/138 - loss 0.19058085 - samples/sec: 191.91\n",
            "2019-12-22 18:11:40,308 epoch 109 - iter 52/138 - loss 0.18290376 - samples/sec: 187.03\n",
            "2019-12-22 18:11:42,407 epoch 109 - iter 65/138 - loss 0.17985318 - samples/sec: 200.09\n",
            "2019-12-22 18:11:44,636 epoch 109 - iter 78/138 - loss 0.17621504 - samples/sec: 188.41\n",
            "2019-12-22 18:11:46,796 epoch 109 - iter 91/138 - loss 0.18701554 - samples/sec: 194.57\n",
            "2019-12-22 18:11:49,130 epoch 109 - iter 104/138 - loss 0.18094426 - samples/sec: 179.71\n",
            "2019-12-22 18:11:51,295 epoch 109 - iter 117/138 - loss 0.17719555 - samples/sec: 193.81\n",
            "2019-12-22 18:11:53,523 epoch 109 - iter 130/138 - loss 0.17158667 - samples/sec: 188.39\n",
            "2019-12-22 18:11:54,666 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:54,667 EPOCH 109 done: loss 0.1705 - lr 0.0031\n",
            "Epoch   108: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2019-12-22 18:11:54,668 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:11:54,670 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:11:54,853 epoch 110 - iter 0/138 - loss 0.10430241 - samples/sec: 2292.71\n",
            "2019-12-22 18:11:57,104 epoch 110 - iter 13/138 - loss 0.20607695 - samples/sec: 186.26\n",
            "2019-12-22 18:11:59,238 epoch 110 - iter 26/138 - loss 0.19334793 - samples/sec: 196.94\n",
            "2019-12-22 18:12:01,435 epoch 110 - iter 39/138 - loss 0.20700117 - samples/sec: 191.07\n",
            "2019-12-22 18:12:03,718 epoch 110 - iter 52/138 - loss 0.19725516 - samples/sec: 184.02\n",
            "2019-12-22 18:12:05,898 epoch 110 - iter 65/138 - loss 0.18189150 - samples/sec: 192.56\n",
            "2019-12-22 18:12:08,218 epoch 110 - iter 78/138 - loss 0.17116663 - samples/sec: 180.72\n",
            "2019-12-22 18:12:10,577 epoch 110 - iter 91/138 - loss 0.16892099 - samples/sec: 178.08\n",
            "2019-12-22 18:12:12,832 epoch 110 - iter 104/138 - loss 0.17169781 - samples/sec: 186.17\n",
            "2019-12-22 18:12:14,925 epoch 110 - iter 117/138 - loss 0.17316862 - samples/sec: 200.84\n",
            "2019-12-22 18:12:17,160 epoch 110 - iter 130/138 - loss 0.17804623 - samples/sec: 187.84\n",
            "2019-12-22 18:12:18,322 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:12:18,323 EPOCH 110 done: loss 0.1797 - lr 0.0016\n",
            "2019-12-22 18:12:18,331 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:12:18,332 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:12:18,505 epoch 111 - iter 0/138 - loss 0.02478242 - samples/sec: 2468.40\n",
            "2019-12-22 18:12:20,619 epoch 111 - iter 13/138 - loss 0.23316059 - samples/sec: 198.41\n",
            "2019-12-22 18:12:22,880 epoch 111 - iter 26/138 - loss 0.20205001 - samples/sec: 185.68\n",
            "2019-12-22 18:12:25,055 epoch 111 - iter 39/138 - loss 0.20724221 - samples/sec: 193.00\n",
            "2019-12-22 18:12:27,373 epoch 111 - iter 52/138 - loss 0.20476369 - samples/sec: 181.66\n",
            "2019-12-22 18:12:29,622 epoch 111 - iter 65/138 - loss 0.19157210 - samples/sec: 186.65\n",
            "2019-12-22 18:12:31,766 epoch 111 - iter 78/138 - loss 0.19171852 - samples/sec: 195.86\n",
            "2019-12-22 18:12:33,972 epoch 111 - iter 91/138 - loss 0.19143385 - samples/sec: 190.26\n",
            "2019-12-22 18:12:36,172 epoch 111 - iter 104/138 - loss 0.18671221 - samples/sec: 190.82\n",
            "2019-12-22 18:12:38,412 epoch 111 - iter 117/138 - loss 0.18288211 - samples/sec: 187.90\n",
            "2019-12-22 18:12:40,769 epoch 111 - iter 130/138 - loss 0.18534431 - samples/sec: 177.94\n",
            "2019-12-22 18:12:41,967 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:12:41,968 EPOCH 111 done: loss 0.1814 - lr 0.0016\n",
            "2019-12-22 18:12:41,970 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:12:41,971 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:12:42,142 epoch 112 - iter 0/138 - loss 0.07617784 - samples/sec: 2487.49\n",
            "2019-12-22 18:12:44,391 epoch 112 - iter 13/138 - loss 0.15528275 - samples/sec: 186.57\n",
            "2019-12-22 18:12:46,583 epoch 112 - iter 26/138 - loss 0.15888939 - samples/sec: 191.85\n",
            "2019-12-22 18:12:48,716 epoch 112 - iter 39/138 - loss 0.16096521 - samples/sec: 196.78\n",
            "2019-12-22 18:12:50,942 epoch 112 - iter 52/138 - loss 0.16799010 - samples/sec: 188.74\n",
            "2019-12-22 18:12:53,114 epoch 112 - iter 65/138 - loss 0.18280277 - samples/sec: 193.16\n",
            "2019-12-22 18:12:55,340 epoch 112 - iter 78/138 - loss 0.18284764 - samples/sec: 188.60\n",
            "2019-12-22 18:12:57,593 epoch 112 - iter 91/138 - loss 0.17821438 - samples/sec: 186.24\n",
            "2019-12-22 18:12:59,810 epoch 112 - iter 104/138 - loss 0.17427057 - samples/sec: 189.31\n",
            "2019-12-22 18:13:02,232 epoch 112 - iter 117/138 - loss 0.18058154 - samples/sec: 173.55\n",
            "2019-12-22 18:13:04,367 epoch 112 - iter 130/138 - loss 0.17902826 - samples/sec: 196.81\n",
            "2019-12-22 18:13:05,526 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:05,527 EPOCH 112 done: loss 0.1786 - lr 0.0016\n",
            "2019-12-22 18:13:05,528 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:13:05,529 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:05,697 epoch 113 - iter 0/138 - loss 0.15400136 - samples/sec: 2493.51\n",
            "2019-12-22 18:13:07,902 epoch 113 - iter 13/138 - loss 0.23368741 - samples/sec: 190.17\n",
            "2019-12-22 18:13:10,054 epoch 113 - iter 26/138 - loss 0.21805797 - samples/sec: 195.05\n",
            "2019-12-22 18:13:12,373 epoch 113 - iter 39/138 - loss 0.20417978 - samples/sec: 181.29\n",
            "2019-12-22 18:13:14,609 epoch 113 - iter 52/138 - loss 0.18686249 - samples/sec: 187.70\n",
            "2019-12-22 18:13:16,776 epoch 113 - iter 65/138 - loss 0.18489333 - samples/sec: 193.74\n",
            "2019-12-22 18:13:19,156 epoch 113 - iter 78/138 - loss 0.18344406 - samples/sec: 176.37\n",
            "2019-12-22 18:13:21,320 epoch 113 - iter 91/138 - loss 0.17879978 - samples/sec: 193.90\n",
            "2019-12-22 18:13:23,596 epoch 113 - iter 104/138 - loss 0.17780665 - samples/sec: 184.51\n",
            "2019-12-22 18:13:25,799 epoch 113 - iter 117/138 - loss 0.17913974 - samples/sec: 190.58\n",
            "2019-12-22 18:13:28,060 epoch 113 - iter 130/138 - loss 0.18137981 - samples/sec: 185.40\n",
            "2019-12-22 18:13:29,194 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:29,195 EPOCH 113 done: loss 0.1847 - lr 0.0016\n",
            "Epoch   112: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2019-12-22 18:13:29,196 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:13:29,197 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:29,395 epoch 114 - iter 0/138 - loss 0.19230437 - samples/sec: 2115.02\n",
            "2019-12-22 18:13:31,605 epoch 114 - iter 13/138 - loss 0.16304431 - samples/sec: 190.15\n",
            "2019-12-22 18:13:33,808 epoch 114 - iter 26/138 - loss 0.16270571 - samples/sec: 191.20\n",
            "2019-12-22 18:13:36,130 epoch 114 - iter 39/138 - loss 0.17229042 - samples/sec: 181.06\n",
            "2019-12-22 18:13:38,384 epoch 114 - iter 52/138 - loss 0.17352153 - samples/sec: 186.03\n",
            "2019-12-22 18:13:40,546 epoch 114 - iter 65/138 - loss 0.16021098 - samples/sec: 193.94\n",
            "2019-12-22 18:13:42,713 epoch 114 - iter 78/138 - loss 0.15800945 - samples/sec: 193.77\n",
            "2019-12-22 18:13:44,891 epoch 114 - iter 91/138 - loss 0.16442713 - samples/sec: 192.50\n",
            "2019-12-22 18:13:47,040 epoch 114 - iter 104/138 - loss 0.16746136 - samples/sec: 195.36\n",
            "2019-12-22 18:13:49,317 epoch 114 - iter 117/138 - loss 0.17079170 - samples/sec: 184.62\n",
            "2019-12-22 18:13:51,615 epoch 114 - iter 130/138 - loss 0.17715110 - samples/sec: 182.61\n",
            "2019-12-22 18:13:52,765 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:52,766 EPOCH 114 done: loss 0.1791 - lr 0.0008\n",
            "2019-12-22 18:13:52,767 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:13:52,768 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:13:52,932 epoch 115 - iter 0/138 - loss 0.27568853 - samples/sec: 2556.58\n",
            "2019-12-22 18:13:55,200 epoch 115 - iter 13/138 - loss 0.14049484 - samples/sec: 185.46\n",
            "2019-12-22 18:13:57,376 epoch 115 - iter 26/138 - loss 0.16682415 - samples/sec: 193.60\n",
            "2019-12-22 18:13:59,590 epoch 115 - iter 39/138 - loss 0.16699926 - samples/sec: 189.52\n",
            "2019-12-22 18:14:01,786 epoch 115 - iter 52/138 - loss 0.17438118 - samples/sec: 191.24\n",
            "2019-12-22 18:14:04,096 epoch 115 - iter 65/138 - loss 0.18548005 - samples/sec: 181.65\n",
            "2019-12-22 18:14:06,330 epoch 115 - iter 78/138 - loss 0.17595642 - samples/sec: 188.00\n",
            "2019-12-22 18:14:08,621 epoch 115 - iter 91/138 - loss 0.17469354 - samples/sec: 183.44\n",
            "2019-12-22 18:14:10,768 epoch 115 - iter 104/138 - loss 0.17198317 - samples/sec: 195.58\n",
            "2019-12-22 18:14:13,095 epoch 115 - iter 117/138 - loss 0.17580351 - samples/sec: 180.19\n",
            "2019-12-22 18:14:15,193 epoch 115 - iter 130/138 - loss 0.17569701 - samples/sec: 200.16\n",
            "2019-12-22 18:14:16,334 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:14:16,335 EPOCH 115 done: loss 0.1757 - lr 0.0008\n",
            "2019-12-22 18:14:16,336 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:14:16,337 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:14:16,516 epoch 116 - iter 0/138 - loss 0.24101657 - samples/sec: 2340.30\n",
            "2019-12-22 18:14:18,653 epoch 116 - iter 13/138 - loss 0.17834310 - samples/sec: 196.17\n",
            "2019-12-22 18:14:20,920 epoch 116 - iter 26/138 - loss 0.19345837 - samples/sec: 185.14\n",
            "2019-12-22 18:14:23,101 epoch 116 - iter 39/138 - loss 0.18403568 - samples/sec: 192.39\n",
            "2019-12-22 18:14:25,306 epoch 116 - iter 52/138 - loss 0.18801737 - samples/sec: 190.41\n",
            "2019-12-22 18:14:27,502 epoch 116 - iter 65/138 - loss 0.17389318 - samples/sec: 191.08\n",
            "2019-12-22 18:14:29,709 epoch 116 - iter 78/138 - loss 0.17090008 - samples/sec: 190.24\n",
            "2019-12-22 18:14:31,873 epoch 116 - iter 91/138 - loss 0.17565580 - samples/sec: 193.80\n",
            "2019-12-22 18:14:34,378 epoch 116 - iter 104/138 - loss 0.16876370 - samples/sec: 167.38\n",
            "2019-12-22 18:14:36,591 epoch 116 - iter 117/138 - loss 0.16825203 - samples/sec: 189.64\n",
            "2019-12-22 18:14:38,951 epoch 116 - iter 130/138 - loss 0.17094583 - samples/sec: 177.75\n",
            "2019-12-22 18:14:40,132 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:14:40,133 EPOCH 116 done: loss 0.1688 - lr 0.0008\n",
            "2019-12-22 18:14:40,134 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:14:40,135 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:14:40,288 epoch 117 - iter 0/138 - loss 0.22606313 - samples/sec: 2741.06\n",
            "2019-12-22 18:14:42,586 epoch 117 - iter 13/138 - loss 0.17713872 - samples/sec: 182.36\n",
            "2019-12-22 18:14:44,738 epoch 117 - iter 26/138 - loss 0.16460071 - samples/sec: 195.41\n",
            "2019-12-22 18:14:46,967 epoch 117 - iter 39/138 - loss 0.15761672 - samples/sec: 188.62\n",
            "2019-12-22 18:14:49,251 epoch 117 - iter 52/138 - loss 0.14627035 - samples/sec: 183.88\n",
            "2019-12-22 18:14:51,500 epoch 117 - iter 65/138 - loss 0.16431669 - samples/sec: 186.47\n",
            "2019-12-22 18:14:53,725 epoch 117 - iter 78/138 - loss 0.16613126 - samples/sec: 188.87\n",
            "2019-12-22 18:14:55,838 epoch 117 - iter 91/138 - loss 0.17619182 - samples/sec: 199.11\n",
            "2019-12-22 18:14:58,011 epoch 117 - iter 104/138 - loss 0.17345463 - samples/sec: 193.10\n",
            "2019-12-22 18:15:00,203 epoch 117 - iter 117/138 - loss 0.17093887 - samples/sec: 191.42\n",
            "2019-12-22 18:15:02,435 epoch 117 - iter 130/138 - loss 0.16942096 - samples/sec: 188.14\n",
            "2019-12-22 18:15:03,589 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:03,591 EPOCH 117 done: loss 0.1675 - lr 0.0008\n",
            "Epoch   116: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2019-12-22 18:15:03,592 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:15:03,593 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:03,761 epoch 118 - iter 0/138 - loss 0.22273707 - samples/sec: 2495.09\n",
            "2019-12-22 18:15:05,966 epoch 118 - iter 13/138 - loss 0.18891672 - samples/sec: 190.35\n",
            "2019-12-22 18:15:08,257 epoch 118 - iter 26/138 - loss 0.19087165 - samples/sec: 183.27\n",
            "2019-12-22 18:15:10,396 epoch 118 - iter 39/138 - loss 0.19745531 - samples/sec: 196.49\n",
            "2019-12-22 18:15:12,658 epoch 118 - iter 52/138 - loss 0.17487978 - samples/sec: 185.48\n",
            "2019-12-22 18:15:14,895 epoch 118 - iter 65/138 - loss 0.17243859 - samples/sec: 187.73\n",
            "2019-12-22 18:15:17,039 epoch 118 - iter 78/138 - loss 0.16534565 - samples/sec: 195.76\n",
            "2019-12-22 18:15:19,276 epoch 118 - iter 91/138 - loss 0.16633522 - samples/sec: 187.67\n",
            "2019-12-22 18:15:21,422 epoch 118 - iter 104/138 - loss 0.16337856 - samples/sec: 195.92\n",
            "2019-12-22 18:15:23,672 epoch 118 - iter 117/138 - loss 0.16741407 - samples/sec: 186.60\n",
            "2019-12-22 18:15:25,920 epoch 118 - iter 130/138 - loss 0.17361455 - samples/sec: 186.75\n",
            "2019-12-22 18:15:27,121 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:27,122 EPOCH 118 done: loss 0.1722 - lr 0.0004\n",
            "2019-12-22 18:15:27,123 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:15:27,124 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:27,284 epoch 119 - iter 0/138 - loss 0.03905082 - samples/sec: 2627.50\n",
            "2019-12-22 18:15:29,628 epoch 119 - iter 13/138 - loss 0.21712859 - samples/sec: 179.17\n",
            "2019-12-22 18:15:31,830 epoch 119 - iter 26/138 - loss 0.18244897 - samples/sec: 191.19\n",
            "2019-12-22 18:15:34,033 epoch 119 - iter 39/138 - loss 0.17368345 - samples/sec: 190.51\n",
            "2019-12-22 18:15:36,256 epoch 119 - iter 52/138 - loss 0.17886366 - samples/sec: 188.90\n",
            "2019-12-22 18:15:38,385 epoch 119 - iter 65/138 - loss 0.17071752 - samples/sec: 197.18\n",
            "2019-12-22 18:15:40,612 epoch 119 - iter 78/138 - loss 0.16572640 - samples/sec: 188.40\n",
            "2019-12-22 18:15:42,813 epoch 119 - iter 91/138 - loss 0.16200904 - samples/sec: 190.74\n",
            "2019-12-22 18:15:44,887 epoch 119 - iter 104/138 - loss 0.16105595 - samples/sec: 202.29\n",
            "2019-12-22 18:15:47,057 epoch 119 - iter 117/138 - loss 0.16529408 - samples/sec: 193.30\n",
            "2019-12-22 18:15:49,373 epoch 119 - iter 130/138 - loss 0.16737284 - samples/sec: 180.97\n",
            "2019-12-22 18:15:50,548 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:50,549 EPOCH 119 done: loss 0.1685 - lr 0.0004\n",
            "2019-12-22 18:15:50,550 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:15:50,552 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:15:50,714 epoch 120 - iter 0/138 - loss 0.27126729 - samples/sec: 2590.71\n",
            "2019-12-22 18:15:52,823 epoch 120 - iter 13/138 - loss 0.18629601 - samples/sec: 198.92\n",
            "2019-12-22 18:15:54,951 epoch 120 - iter 26/138 - loss 0.17010710 - samples/sec: 197.29\n",
            "2019-12-22 18:15:57,197 epoch 120 - iter 39/138 - loss 0.16897278 - samples/sec: 186.91\n",
            "2019-12-22 18:15:59,456 epoch 120 - iter 52/138 - loss 0.15926657 - samples/sec: 186.10\n",
            "2019-12-22 18:16:01,793 epoch 120 - iter 65/138 - loss 0.15896718 - samples/sec: 180.07\n",
            "2019-12-22 18:16:04,030 epoch 120 - iter 78/138 - loss 0.16912478 - samples/sec: 187.60\n",
            "2019-12-22 18:16:06,360 epoch 120 - iter 91/138 - loss 0.17107465 - samples/sec: 180.31\n",
            "2019-12-22 18:16:08,622 epoch 120 - iter 104/138 - loss 0.17073086 - samples/sec: 185.55\n",
            "2019-12-22 18:16:10,902 epoch 120 - iter 117/138 - loss 0.17269721 - samples/sec: 184.01\n",
            "2019-12-22 18:16:13,217 epoch 120 - iter 130/138 - loss 0.16660342 - samples/sec: 181.27\n",
            "2019-12-22 18:16:14,402 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:16:14,403 EPOCH 120 done: loss 0.1695 - lr 0.0004\n",
            "2019-12-22 18:16:14,404 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:16:14,406 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:16:14,559 epoch 121 - iter 0/138 - loss 0.22292709 - samples/sec: 2743.55\n",
            "2019-12-22 18:16:16,754 epoch 121 - iter 13/138 - loss 0.17087390 - samples/sec: 190.98\n",
            "2019-12-22 18:16:19,028 epoch 121 - iter 26/138 - loss 0.21177467 - samples/sec: 185.06\n",
            "2019-12-22 18:16:21,179 epoch 121 - iter 39/138 - loss 0.21858141 - samples/sec: 195.21\n",
            "2019-12-22 18:16:23,415 epoch 121 - iter 52/138 - loss 0.20486208 - samples/sec: 187.82\n",
            "2019-12-22 18:16:25,619 epoch 121 - iter 65/138 - loss 0.19672727 - samples/sec: 190.43\n",
            "2019-12-22 18:16:27,876 epoch 121 - iter 78/138 - loss 0.19310186 - samples/sec: 186.15\n",
            "2019-12-22 18:16:30,014 epoch 121 - iter 91/138 - loss 0.18875421 - samples/sec: 196.35\n",
            "2019-12-22 18:16:32,162 epoch 121 - iter 104/138 - loss 0.18242863 - samples/sec: 195.46\n",
            "2019-12-22 18:16:34,436 epoch 121 - iter 117/138 - loss 0.17703397 - samples/sec: 184.66\n",
            "2019-12-22 18:16:36,686 epoch 121 - iter 130/138 - loss 0.18083026 - samples/sec: 186.70\n",
            "2019-12-22 18:16:37,947 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:16:37,951 EPOCH 121 done: loss 0.1812 - lr 0.0004\n",
            "Epoch   120: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2019-12-22 18:16:37,955 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:16:37,958 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:16:38,115 epoch 122 - iter 0/138 - loss 0.12557143 - samples/sec: 2725.16\n",
            "2019-12-22 18:16:40,255 epoch 122 - iter 13/138 - loss 0.16505958 - samples/sec: 195.97\n",
            "2019-12-22 18:16:42,506 epoch 122 - iter 26/138 - loss 0.18992908 - samples/sec: 186.55\n",
            "2019-12-22 18:16:44,627 epoch 122 - iter 39/138 - loss 0.19471510 - samples/sec: 197.89\n",
            "2019-12-22 18:16:46,845 epoch 122 - iter 52/138 - loss 0.18411493 - samples/sec: 189.64\n",
            "2019-12-22 18:16:49,113 epoch 122 - iter 65/138 - loss 0.18328785 - samples/sec: 185.30\n",
            "2019-12-22 18:16:51,268 epoch 122 - iter 78/138 - loss 0.17405963 - samples/sec: 194.67\n",
            "2019-12-22 18:16:53,463 epoch 122 - iter 91/138 - loss 0.16908056 - samples/sec: 192.00\n",
            "2019-12-22 18:16:55,578 epoch 122 - iter 104/138 - loss 0.17942368 - samples/sec: 198.52\n",
            "2019-12-22 18:16:57,716 epoch 122 - iter 117/138 - loss 0.17599670 - samples/sec: 196.47\n",
            "2019-12-22 18:16:59,925 epoch 122 - iter 130/138 - loss 0.17515685 - samples/sec: 190.12\n",
            "2019-12-22 18:17:01,191 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:01,193 EPOCH 122 done: loss 0.1761 - lr 0.0002\n",
            "2019-12-22 18:17:01,194 BAD EPOCHS (no improvement): 1\n",
            "2019-12-22 18:17:01,196 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:01,366 epoch 123 - iter 0/138 - loss 0.04882175 - samples/sec: 2472.22\n",
            "2019-12-22 18:17:03,588 epoch 123 - iter 13/138 - loss 0.22614208 - samples/sec: 188.71\n",
            "2019-12-22 18:17:05,771 epoch 123 - iter 26/138 - loss 0.21981710 - samples/sec: 192.29\n",
            "2019-12-22 18:17:07,924 epoch 123 - iter 39/138 - loss 0.19719984 - samples/sec: 195.08\n",
            "2019-12-22 18:17:10,136 epoch 123 - iter 52/138 - loss 0.18505255 - samples/sec: 189.58\n",
            "2019-12-22 18:17:12,378 epoch 123 - iter 65/138 - loss 0.17330984 - samples/sec: 187.28\n",
            "2019-12-22 18:17:14,564 epoch 123 - iter 78/138 - loss 0.17184896 - samples/sec: 191.95\n",
            "2019-12-22 18:17:16,669 epoch 123 - iter 91/138 - loss 0.17472831 - samples/sec: 199.84\n",
            "2019-12-22 18:17:19,057 epoch 123 - iter 104/138 - loss 0.17210861 - samples/sec: 175.65\n",
            "2019-12-22 18:17:21,390 epoch 123 - iter 117/138 - loss 0.16824714 - samples/sec: 180.00\n",
            "2019-12-22 18:17:23,694 epoch 123 - iter 130/138 - loss 0.16644676 - samples/sec: 182.17\n",
            "2019-12-22 18:17:24,901 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:24,902 EPOCH 123 done: loss 0.1654 - lr 0.0002\n",
            "2019-12-22 18:17:24,903 BAD EPOCHS (no improvement): 2\n",
            "2019-12-22 18:17:24,904 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:25,074 epoch 124 - iter 0/138 - loss 0.29655421 - samples/sec: 2484.61\n",
            "2019-12-22 18:17:27,321 epoch 124 - iter 13/138 - loss 0.16149424 - samples/sec: 186.57\n",
            "2019-12-22 18:17:29,654 epoch 124 - iter 26/138 - loss 0.15243482 - samples/sec: 179.96\n",
            "2019-12-22 18:17:32,027 epoch 124 - iter 39/138 - loss 0.16037010 - samples/sec: 177.12\n",
            "2019-12-22 18:17:34,262 epoch 124 - iter 52/138 - loss 0.17555780 - samples/sec: 187.86\n",
            "2019-12-22 18:17:36,566 epoch 124 - iter 65/138 - loss 0.17015712 - samples/sec: 181.89\n",
            "2019-12-22 18:17:38,734 epoch 124 - iter 78/138 - loss 0.17062218 - samples/sec: 193.68\n",
            "2019-12-22 18:17:40,901 epoch 124 - iter 91/138 - loss 0.17464599 - samples/sec: 193.87\n",
            "2019-12-22 18:17:43,132 epoch 124 - iter 104/138 - loss 0.17501577 - samples/sec: 188.16\n",
            "2019-12-22 18:17:45,229 epoch 124 - iter 117/138 - loss 0.17298330 - samples/sec: 200.22\n",
            "2019-12-22 18:17:47,500 epoch 124 - iter 130/138 - loss 0.17402913 - samples/sec: 184.78\n",
            "2019-12-22 18:17:48,735 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:48,741 EPOCH 124 done: loss 0.1703 - lr 0.0002\n",
            "2019-12-22 18:17:48,742 BAD EPOCHS (no improvement): 3\n",
            "2019-12-22 18:17:48,752 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:17:49,037 epoch 125 - iter 0/138 - loss 0.05304050 - samples/sec: 1470.09\n",
            "2019-12-22 18:17:51,149 epoch 125 - iter 13/138 - loss 0.17504086 - samples/sec: 198.93\n",
            "2019-12-22 18:17:53,524 epoch 125 - iter 26/138 - loss 0.18822162 - samples/sec: 176.60\n",
            "2019-12-22 18:17:55,615 epoch 125 - iter 39/138 - loss 0.18933367 - samples/sec: 200.83\n",
            "2019-12-22 18:17:57,879 epoch 125 - iter 52/138 - loss 0.18586397 - samples/sec: 185.42\n",
            "2019-12-22 18:18:00,050 epoch 125 - iter 65/138 - loss 0.18957203 - samples/sec: 193.36\n",
            "2019-12-22 18:18:02,159 epoch 125 - iter 78/138 - loss 0.18509537 - samples/sec: 198.92\n",
            "2019-12-22 18:18:04,265 epoch 125 - iter 91/138 - loss 0.18307976 - samples/sec: 199.43\n",
            "2019-12-22 18:18:06,455 epoch 125 - iter 104/138 - loss 0.17885127 - samples/sec: 191.98\n",
            "2019-12-22 18:18:08,652 epoch 125 - iter 117/138 - loss 0.17516105 - samples/sec: 191.43\n",
            "2019-12-22 18:18:10,933 epoch 125 - iter 130/138 - loss 0.16805077 - samples/sec: 184.03\n",
            "2019-12-22 18:18:12,080 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:18:12,081 EPOCH 125 done: loss 0.1669 - lr 0.0002\n",
            "Epoch   124: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2019-12-22 18:18:12,082 BAD EPOCHS (no improvement): 4\n",
            "2019-12-22 18:18:12,085 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:18:12,086 ----------------------------------------------------------------------------------------------------\n",
            "2019-12-22 18:18:12,088 learning rate too small - quitting training!\n",
            "2019-12-22 18:18:12,089 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRlQ1p-p3Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}